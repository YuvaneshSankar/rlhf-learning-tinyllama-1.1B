{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.9230414214837891,
  "eval_steps": 500,
  "global_step": 6000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0015384023691396484,
      "grad_norm": 2.584967613220215,
      "learning_rate": 2.7649769585253458e-06,
      "loss": 3.3921,
      "step": 10
    },
    {
      "epoch": 0.003076804738279297,
      "grad_norm": 2.1197686195373535,
      "learning_rate": 5.837173579109064e-06,
      "loss": 3.4355,
      "step": 20
    },
    {
      "epoch": 0.004615207107418946,
      "grad_norm": 1.4546103477478027,
      "learning_rate": 8.90937019969278e-06,
      "loss": 3.2477,
      "step": 30
    },
    {
      "epoch": 0.006153609476558594,
      "grad_norm": 1.344617486000061,
      "learning_rate": 1.1981566820276497e-05,
      "loss": 3.1501,
      "step": 40
    },
    {
      "epoch": 0.007692011845698243,
      "grad_norm": 2.066725730895996,
      "learning_rate": 1.5053763440860215e-05,
      "loss": 3.1164,
      "step": 50
    },
    {
      "epoch": 0.009230414214837892,
      "grad_norm": 1.6152411699295044,
      "learning_rate": 1.8125960061443932e-05,
      "loss": 2.7623,
      "step": 60
    },
    {
      "epoch": 0.010768816583977539,
      "grad_norm": 1.8350930213928223,
      "learning_rate": 2.1198156682027652e-05,
      "loss": 2.5937,
      "step": 70
    },
    {
      "epoch": 0.012307218953117188,
      "grad_norm": 2.344829797744751,
      "learning_rate": 2.427035330261137e-05,
      "loss": 2.4205,
      "step": 80
    },
    {
      "epoch": 0.013845621322256836,
      "grad_norm": 4.050056457519531,
      "learning_rate": 2.734254992319509e-05,
      "loss": 2.2338,
      "step": 90
    },
    {
      "epoch": 0.015384023691396485,
      "grad_norm": 2.1490421295166016,
      "learning_rate": 3.0414746543778806e-05,
      "loss": 2.0444,
      "step": 100
    },
    {
      "epoch": 0.016922426060536132,
      "grad_norm": 1.358504056930542,
      "learning_rate": 3.348694316436252e-05,
      "loss": 1.8553,
      "step": 110
    },
    {
      "epoch": 0.018460828429675783,
      "grad_norm": 1.3946998119354248,
      "learning_rate": 3.655913978494624e-05,
      "loss": 1.8513,
      "step": 120
    },
    {
      "epoch": 0.01999923079881543,
      "grad_norm": 2.3798844814300537,
      "learning_rate": 3.963133640552996e-05,
      "loss": 1.7238,
      "step": 130
    },
    {
      "epoch": 0.021537633167955077,
      "grad_norm": 1.6720370054244995,
      "learning_rate": 4.270353302611367e-05,
      "loss": 1.6326,
      "step": 140
    },
    {
      "epoch": 0.023076035537094728,
      "grad_norm": 1.9096741676330566,
      "learning_rate": 4.577572964669739e-05,
      "loss": 1.499,
      "step": 150
    },
    {
      "epoch": 0.024614437906234375,
      "grad_norm": 1.5747382640838623,
      "learning_rate": 4.8847926267281106e-05,
      "loss": 1.3412,
      "step": 160
    },
    {
      "epoch": 0.026152840275374026,
      "grad_norm": 1.592766523361206,
      "learning_rate": 5.192012288786483e-05,
      "loss": 1.3982,
      "step": 170
    },
    {
      "epoch": 0.027691242644513673,
      "grad_norm": 1.6842422485351562,
      "learning_rate": 5.499231950844854e-05,
      "loss": 1.3376,
      "step": 180
    },
    {
      "epoch": 0.02922964501365332,
      "grad_norm": 1.5417200326919556,
      "learning_rate": 5.8064516129032266e-05,
      "loss": 1.2252,
      "step": 190
    },
    {
      "epoch": 0.03076804738279297,
      "grad_norm": 3.287029504776001,
      "learning_rate": 6.113671274961598e-05,
      "loss": 1.315,
      "step": 200
    },
    {
      "epoch": 0.03230644975193262,
      "grad_norm": 1.9193211793899536,
      "learning_rate": 6.42089093701997e-05,
      "loss": 1.2919,
      "step": 210
    },
    {
      "epoch": 0.033844852121072265,
      "grad_norm": 1.1237131357192993,
      "learning_rate": 6.72811059907834e-05,
      "loss": 1.2607,
      "step": 220
    },
    {
      "epoch": 0.035383254490211916,
      "grad_norm": 5.890222072601318,
      "learning_rate": 7.035330261136714e-05,
      "loss": 1.1936,
      "step": 230
    },
    {
      "epoch": 0.036921656859351566,
      "grad_norm": 1.1118741035461426,
      "learning_rate": 7.342549923195085e-05,
      "loss": 1.3291,
      "step": 240
    },
    {
      "epoch": 0.03846005922849121,
      "grad_norm": 1.3041982650756836,
      "learning_rate": 7.649769585253457e-05,
      "loss": 1.1517,
      "step": 250
    },
    {
      "epoch": 0.03999846159763086,
      "grad_norm": 0.8721215128898621,
      "learning_rate": 7.956989247311829e-05,
      "loss": 1.1674,
      "step": 260
    },
    {
      "epoch": 0.04153686396677051,
      "grad_norm": 1.0562869310379028,
      "learning_rate": 8.2642089093702e-05,
      "loss": 1.2735,
      "step": 270
    },
    {
      "epoch": 0.043075266335910155,
      "grad_norm": 1.0866798162460327,
      "learning_rate": 8.571428571428571e-05,
      "loss": 1.2501,
      "step": 280
    },
    {
      "epoch": 0.044613668705049805,
      "grad_norm": 1.4433777332305908,
      "learning_rate": 8.878648233486943e-05,
      "loss": 1.1162,
      "step": 290
    },
    {
      "epoch": 0.046152071074189456,
      "grad_norm": 1.0068490505218506,
      "learning_rate": 9.185867895545315e-05,
      "loss": 1.1211,
      "step": 300
    },
    {
      "epoch": 0.0476904734433291,
      "grad_norm": 1.2410961389541626,
      "learning_rate": 9.493087557603687e-05,
      "loss": 1.1446,
      "step": 310
    },
    {
      "epoch": 0.04922887581246875,
      "grad_norm": 0.9184087514877319,
      "learning_rate": 9.800307219662059e-05,
      "loss": 1.1429,
      "step": 320
    },
    {
      "epoch": 0.0507672781816084,
      "grad_norm": 0.8276410698890686,
      "learning_rate": 0.0001010752688172043,
      "loss": 1.2176,
      "step": 330
    },
    {
      "epoch": 0.05230568055074805,
      "grad_norm": 0.9394607543945312,
      "learning_rate": 0.00010414746543778802,
      "loss": 1.1135,
      "step": 340
    },
    {
      "epoch": 0.053844082919887695,
      "grad_norm": 0.7618582844734192,
      "learning_rate": 0.00010721966205837175,
      "loss": 1.1033,
      "step": 350
    },
    {
      "epoch": 0.055382485289027346,
      "grad_norm": 0.8039534091949463,
      "learning_rate": 0.00011029185867895546,
      "loss": 1.1847,
      "step": 360
    },
    {
      "epoch": 0.056920887658166996,
      "grad_norm": 1.115044355392456,
      "learning_rate": 0.00011336405529953918,
      "loss": 1.1742,
      "step": 370
    },
    {
      "epoch": 0.05845929002730664,
      "grad_norm": 0.9048172235488892,
      "learning_rate": 0.00011643625192012289,
      "loss": 1.214,
      "step": 380
    },
    {
      "epoch": 0.05999769239644629,
      "grad_norm": 0.8437467217445374,
      "learning_rate": 0.00011950844854070662,
      "loss": 1.1377,
      "step": 390
    },
    {
      "epoch": 0.06153609476558594,
      "grad_norm": 0.7477501630783081,
      "learning_rate": 0.00012258064516129034,
      "loss": 1.1254,
      "step": 400
    },
    {
      "epoch": 0.06307449713472559,
      "grad_norm": 0.7767701148986816,
      "learning_rate": 0.00012565284178187405,
      "loss": 1.107,
      "step": 410
    },
    {
      "epoch": 0.06461289950386524,
      "grad_norm": 1.1635019779205322,
      "learning_rate": 0.00012872503840245775,
      "loss": 1.1033,
      "step": 420
    },
    {
      "epoch": 0.06615130187300489,
      "grad_norm": 0.8846574425697327,
      "learning_rate": 0.0001317972350230415,
      "loss": 1.1624,
      "step": 430
    },
    {
      "epoch": 0.06768970424214453,
      "grad_norm": 0.9813984036445618,
      "learning_rate": 0.0001348694316436252,
      "loss": 1.2409,
      "step": 440
    },
    {
      "epoch": 0.06922810661128419,
      "grad_norm": 0.9507609009742737,
      "learning_rate": 0.0001379416282642089,
      "loss": 1.2503,
      "step": 450
    },
    {
      "epoch": 0.07076650898042383,
      "grad_norm": 0.7813093066215515,
      "learning_rate": 0.00014101382488479263,
      "loss": 1.0684,
      "step": 460
    },
    {
      "epoch": 0.07230491134956347,
      "grad_norm": 0.7689453363418579,
      "learning_rate": 0.00014408602150537637,
      "loss": 1.1374,
      "step": 470
    },
    {
      "epoch": 0.07384331371870313,
      "grad_norm": 0.7696683406829834,
      "learning_rate": 0.00014715821812596007,
      "loss": 1.0953,
      "step": 480
    },
    {
      "epoch": 0.07538171608784278,
      "grad_norm": 0.6287794709205627,
      "learning_rate": 0.00015023041474654378,
      "loss": 1.107,
      "step": 490
    },
    {
      "epoch": 0.07692011845698242,
      "grad_norm": 0.8339664340019226,
      "learning_rate": 0.0001533026113671275,
      "loss": 1.1726,
      "step": 500
    },
    {
      "epoch": 0.07845852082612208,
      "grad_norm": 0.7385021448135376,
      "learning_rate": 0.00015637480798771122,
      "loss": 1.1105,
      "step": 510
    },
    {
      "epoch": 0.07999692319526172,
      "grad_norm": 1.2106229066848755,
      "learning_rate": 0.00015944700460829493,
      "loss": 1.0924,
      "step": 520
    },
    {
      "epoch": 0.08153532556440136,
      "grad_norm": 0.7910128235816956,
      "learning_rate": 0.00016251920122887866,
      "loss": 1.0595,
      "step": 530
    },
    {
      "epoch": 0.08307372793354102,
      "grad_norm": 0.6995137333869934,
      "learning_rate": 0.0001655913978494624,
      "loss": 1.1278,
      "step": 540
    },
    {
      "epoch": 0.08461213030268067,
      "grad_norm": 0.6679721474647522,
      "learning_rate": 0.0001686635944700461,
      "loss": 1.1902,
      "step": 550
    },
    {
      "epoch": 0.08615053267182031,
      "grad_norm": 0.7743341326713562,
      "learning_rate": 0.0001717357910906298,
      "loss": 1.1896,
      "step": 560
    },
    {
      "epoch": 0.08768893504095997,
      "grad_norm": 0.537539005279541,
      "learning_rate": 0.0001748079877112135,
      "loss": 1.1159,
      "step": 570
    },
    {
      "epoch": 0.08922733741009961,
      "grad_norm": 0.7204731702804565,
      "learning_rate": 0.00017788018433179725,
      "loss": 1.0316,
      "step": 580
    },
    {
      "epoch": 0.09076573977923925,
      "grad_norm": 0.7807141542434692,
      "learning_rate": 0.00018095238095238095,
      "loss": 1.1666,
      "step": 590
    },
    {
      "epoch": 0.09230414214837891,
      "grad_norm": 0.6946710348129272,
      "learning_rate": 0.0001840245775729647,
      "loss": 1.0602,
      "step": 600
    },
    {
      "epoch": 0.09384254451751856,
      "grad_norm": 0.6253068447113037,
      "learning_rate": 0.0001870967741935484,
      "loss": 1.1165,
      "step": 610
    },
    {
      "epoch": 0.0953809468866582,
      "grad_norm": 0.6554427146911621,
      "learning_rate": 0.00019016897081413213,
      "loss": 1.1488,
      "step": 620
    },
    {
      "epoch": 0.09691934925579786,
      "grad_norm": 0.7545556426048279,
      "learning_rate": 0.00019324116743471583,
      "loss": 1.0227,
      "step": 630
    },
    {
      "epoch": 0.0984577516249375,
      "grad_norm": 0.654740035533905,
      "learning_rate": 0.00019631336405529954,
      "loss": 1.0965,
      "step": 640
    },
    {
      "epoch": 0.09999615399407714,
      "grad_norm": 0.5748230218887329,
      "learning_rate": 0.00019938556067588327,
      "loss": 1.1018,
      "step": 650
    },
    {
      "epoch": 0.1015345563632168,
      "grad_norm": 0.49472448229789734,
      "learning_rate": 0.00019972649572649572,
      "loss": 1.1408,
      "step": 660
    },
    {
      "epoch": 0.10307295873235645,
      "grad_norm": 0.5874394178390503,
      "learning_rate": 0.0001993846153846154,
      "loss": 1.0593,
      "step": 670
    },
    {
      "epoch": 0.1046113611014961,
      "grad_norm": 0.7135041952133179,
      "learning_rate": 0.00019904273504273506,
      "loss": 1.177,
      "step": 680
    },
    {
      "epoch": 0.10614976347063575,
      "grad_norm": 0.659172773361206,
      "learning_rate": 0.00019870085470085472,
      "loss": 1.0972,
      "step": 690
    },
    {
      "epoch": 0.10768816583977539,
      "grad_norm": 0.5658200979232788,
      "learning_rate": 0.00019835897435897438,
      "loss": 0.998,
      "step": 700
    },
    {
      "epoch": 0.10922656820891505,
      "grad_norm": 0.6410398483276367,
      "learning_rate": 0.00019801709401709404,
      "loss": 1.0226,
      "step": 710
    },
    {
      "epoch": 0.11076497057805469,
      "grad_norm": 0.6680729389190674,
      "learning_rate": 0.0001976752136752137,
      "loss": 1.0757,
      "step": 720
    },
    {
      "epoch": 0.11230337294719434,
      "grad_norm": 0.9527504444122314,
      "learning_rate": 0.00019733333333333335,
      "loss": 1.0509,
      "step": 730
    },
    {
      "epoch": 0.11384177531633399,
      "grad_norm": 0.5344208478927612,
      "learning_rate": 0.000196991452991453,
      "loss": 1.137,
      "step": 740
    },
    {
      "epoch": 0.11538017768547364,
      "grad_norm": 0.5921201705932617,
      "learning_rate": 0.00019664957264957267,
      "loss": 1.0559,
      "step": 750
    },
    {
      "epoch": 0.11691858005461328,
      "grad_norm": 0.711140513420105,
      "learning_rate": 0.00019630769230769232,
      "loss": 1.0359,
      "step": 760
    },
    {
      "epoch": 0.11845698242375294,
      "grad_norm": 0.4643453061580658,
      "learning_rate": 0.00019596581196581198,
      "loss": 1.051,
      "step": 770
    },
    {
      "epoch": 0.11999538479289258,
      "grad_norm": 0.6035484671592712,
      "learning_rate": 0.0001956239316239316,
      "loss": 1.088,
      "step": 780
    },
    {
      "epoch": 0.12153378716203223,
      "grad_norm": 0.7892462015151978,
      "learning_rate": 0.00019528205128205127,
      "loss": 1.0369,
      "step": 790
    },
    {
      "epoch": 0.12307218953117188,
      "grad_norm": 0.4591948986053467,
      "learning_rate": 0.00019494017094017095,
      "loss": 1.0998,
      "step": 800
    },
    {
      "epoch": 0.12461059190031153,
      "grad_norm": 0.5988150835037231,
      "learning_rate": 0.0001945982905982906,
      "loss": 1.0529,
      "step": 810
    },
    {
      "epoch": 0.12614899426945117,
      "grad_norm": 0.5166341662406921,
      "learning_rate": 0.00019425641025641027,
      "loss": 1.0788,
      "step": 820
    },
    {
      "epoch": 0.12768739663859083,
      "grad_norm": 0.5803174376487732,
      "learning_rate": 0.00019391452991452993,
      "loss": 0.9944,
      "step": 830
    },
    {
      "epoch": 0.12922579900773049,
      "grad_norm": 0.617644727230072,
      "learning_rate": 0.00019357264957264958,
      "loss": 1.0888,
      "step": 840
    },
    {
      "epoch": 0.13076420137687012,
      "grad_norm": 0.61956787109375,
      "learning_rate": 0.00019323076923076924,
      "loss": 1.083,
      "step": 850
    },
    {
      "epoch": 0.13230260374600977,
      "grad_norm": 0.6188405156135559,
      "learning_rate": 0.0001928888888888889,
      "loss": 1.1391,
      "step": 860
    },
    {
      "epoch": 0.13384100611514943,
      "grad_norm": 0.6159250736236572,
      "learning_rate": 0.00019254700854700856,
      "loss": 1.0782,
      "step": 870
    },
    {
      "epoch": 0.13537940848428906,
      "grad_norm": 0.4905155301094055,
      "learning_rate": 0.00019220512820512822,
      "loss": 1.0785,
      "step": 880
    },
    {
      "epoch": 0.13691781085342872,
      "grad_norm": 0.476400226354599,
      "learning_rate": 0.00019186324786324787,
      "loss": 1.0371,
      "step": 890
    },
    {
      "epoch": 0.13845621322256838,
      "grad_norm": 0.5361171960830688,
      "learning_rate": 0.00019152136752136753,
      "loss": 1.1167,
      "step": 900
    },
    {
      "epoch": 0.139994615591708,
      "grad_norm": 0.45032036304473877,
      "learning_rate": 0.0001911794871794872,
      "loss": 1.0379,
      "step": 910
    },
    {
      "epoch": 0.14153301796084766,
      "grad_norm": 0.5094605684280396,
      "learning_rate": 0.00019083760683760685,
      "loss": 1.0336,
      "step": 920
    },
    {
      "epoch": 0.14307142032998732,
      "grad_norm": 0.6864129900932312,
      "learning_rate": 0.0001904957264957265,
      "loss": 1.0494,
      "step": 930
    },
    {
      "epoch": 0.14460982269912695,
      "grad_norm": 0.5435264706611633,
      "learning_rate": 0.00019015384615384616,
      "loss": 1.0812,
      "step": 940
    },
    {
      "epoch": 0.1461482250682666,
      "grad_norm": 0.5616250038146973,
      "learning_rate": 0.00018981196581196582,
      "loss": 1.0181,
      "step": 950
    },
    {
      "epoch": 0.14768662743740626,
      "grad_norm": 0.5459400415420532,
      "learning_rate": 0.00018947008547008548,
      "loss": 1.0613,
      "step": 960
    },
    {
      "epoch": 0.1492250298065459,
      "grad_norm": 0.5072353482246399,
      "learning_rate": 0.00018912820512820513,
      "loss": 1.0952,
      "step": 970
    },
    {
      "epoch": 0.15076343217568555,
      "grad_norm": 0.5611528158187866,
      "learning_rate": 0.0001887863247863248,
      "loss": 1.034,
      "step": 980
    },
    {
      "epoch": 0.1523018345448252,
      "grad_norm": 0.5034070014953613,
      "learning_rate": 0.00018844444444444445,
      "loss": 1.1059,
      "step": 990
    },
    {
      "epoch": 0.15384023691396484,
      "grad_norm": 0.4489002227783203,
      "learning_rate": 0.0001881025641025641,
      "loss": 1.1294,
      "step": 1000
    },
    {
      "epoch": 0.1553786392831045,
      "grad_norm": 0.5371571779251099,
      "learning_rate": 0.00018776068376068377,
      "loss": 1.0166,
      "step": 1010
    },
    {
      "epoch": 0.15691704165224415,
      "grad_norm": 0.5085307955741882,
      "learning_rate": 0.00018741880341880342,
      "loss": 1.0421,
      "step": 1020
    },
    {
      "epoch": 0.15845544402138378,
      "grad_norm": 0.49393656849861145,
      "learning_rate": 0.00018707692307692308,
      "loss": 1.1003,
      "step": 1030
    },
    {
      "epoch": 0.15999384639052344,
      "grad_norm": 0.47883597016334534,
      "learning_rate": 0.00018673504273504274,
      "loss": 1.015,
      "step": 1040
    },
    {
      "epoch": 0.1615322487596631,
      "grad_norm": 0.4961189329624176,
      "learning_rate": 0.0001863931623931624,
      "loss": 1.0595,
      "step": 1050
    },
    {
      "epoch": 0.16307065112880273,
      "grad_norm": 0.5869439840316772,
      "learning_rate": 0.00018605128205128205,
      "loss": 1.0432,
      "step": 1060
    },
    {
      "epoch": 0.1646090534979424,
      "grad_norm": 0.4931619167327881,
      "learning_rate": 0.0001857094017094017,
      "loss": 1.0956,
      "step": 1070
    },
    {
      "epoch": 0.16614745586708204,
      "grad_norm": 0.5093882083892822,
      "learning_rate": 0.00018536752136752137,
      "loss": 1.0178,
      "step": 1080
    },
    {
      "epoch": 0.16768585823622167,
      "grad_norm": 0.5186927318572998,
      "learning_rate": 0.00018502564102564103,
      "loss": 1.0256,
      "step": 1090
    },
    {
      "epoch": 0.16922426060536133,
      "grad_norm": 0.443472683429718,
      "learning_rate": 0.00018468376068376068,
      "loss": 1.0748,
      "step": 1100
    },
    {
      "epoch": 0.170762662974501,
      "grad_norm": 0.4937427043914795,
      "learning_rate": 0.00018434188034188037,
      "loss": 1.1118,
      "step": 1110
    },
    {
      "epoch": 0.17230106534364062,
      "grad_norm": 0.48177462816238403,
      "learning_rate": 0.00018400000000000003,
      "loss": 1.0347,
      "step": 1120
    },
    {
      "epoch": 0.17383946771278028,
      "grad_norm": 0.48172661662101746,
      "learning_rate": 0.00018365811965811968,
      "loss": 1.0622,
      "step": 1130
    },
    {
      "epoch": 0.17537787008191993,
      "grad_norm": 0.5767273306846619,
      "learning_rate": 0.00018331623931623934,
      "loss": 1.0836,
      "step": 1140
    },
    {
      "epoch": 0.17691627245105956,
      "grad_norm": 0.5713164806365967,
      "learning_rate": 0.00018297435897435897,
      "loss": 1.0415,
      "step": 1150
    },
    {
      "epoch": 0.17845467482019922,
      "grad_norm": 0.7022517919540405,
      "learning_rate": 0.00018263247863247863,
      "loss": 1.0547,
      "step": 1160
    },
    {
      "epoch": 0.17999307718933888,
      "grad_norm": 0.5203202962875366,
      "learning_rate": 0.0001822905982905983,
      "loss": 1.0455,
      "step": 1170
    },
    {
      "epoch": 0.1815314795584785,
      "grad_norm": 0.6131396293640137,
      "learning_rate": 0.00018194871794871795,
      "loss": 1.1274,
      "step": 1180
    },
    {
      "epoch": 0.18306988192761817,
      "grad_norm": 0.4761025011539459,
      "learning_rate": 0.0001816068376068376,
      "loss": 1.0878,
      "step": 1190
    },
    {
      "epoch": 0.18460828429675782,
      "grad_norm": 0.5496890544891357,
      "learning_rate": 0.00018126495726495726,
      "loss": 1.0284,
      "step": 1200
    },
    {
      "epoch": 0.18614668666589745,
      "grad_norm": 0.5592815279960632,
      "learning_rate": 0.00018092307692307692,
      "loss": 1.0159,
      "step": 1210
    },
    {
      "epoch": 0.1876850890350371,
      "grad_norm": 0.4768582284450531,
      "learning_rate": 0.00018058119658119658,
      "loss": 1.005,
      "step": 1220
    },
    {
      "epoch": 0.18922349140417677,
      "grad_norm": 0.46654272079467773,
      "learning_rate": 0.00018023931623931623,
      "loss": 1.0125,
      "step": 1230
    },
    {
      "epoch": 0.1907618937733164,
      "grad_norm": 0.46942490339279175,
      "learning_rate": 0.00017989743589743592,
      "loss": 1.0969,
      "step": 1240
    },
    {
      "epoch": 0.19230029614245606,
      "grad_norm": 0.4688929319381714,
      "learning_rate": 0.00017955555555555558,
      "loss": 0.9848,
      "step": 1250
    },
    {
      "epoch": 0.19383869851159571,
      "grad_norm": 0.4767244756221771,
      "learning_rate": 0.00017921367521367523,
      "loss": 1.0352,
      "step": 1260
    },
    {
      "epoch": 0.19537710088073534,
      "grad_norm": 0.51897794008255,
      "learning_rate": 0.0001788717948717949,
      "loss": 1.0773,
      "step": 1270
    },
    {
      "epoch": 0.196915503249875,
      "grad_norm": 0.5375524759292603,
      "learning_rate": 0.00017852991452991455,
      "loss": 1.0726,
      "step": 1280
    },
    {
      "epoch": 0.19845390561901466,
      "grad_norm": 0.4755353033542633,
      "learning_rate": 0.0001781880341880342,
      "loss": 1.0804,
      "step": 1290
    },
    {
      "epoch": 0.1999923079881543,
      "grad_norm": 0.534662127494812,
      "learning_rate": 0.00017784615384615387,
      "loss": 1.0072,
      "step": 1300
    },
    {
      "epoch": 0.20153071035729395,
      "grad_norm": 0.4839552044868469,
      "learning_rate": 0.00017750427350427352,
      "loss": 1.003,
      "step": 1310
    },
    {
      "epoch": 0.2030691127264336,
      "grad_norm": 0.5820987820625305,
      "learning_rate": 0.00017716239316239318,
      "loss": 1.0435,
      "step": 1320
    },
    {
      "epoch": 0.20460751509557323,
      "grad_norm": 0.6065599322319031,
      "learning_rate": 0.0001768205128205128,
      "loss": 1.0177,
      "step": 1330
    },
    {
      "epoch": 0.2061459174647129,
      "grad_norm": 0.5341425538063049,
      "learning_rate": 0.00017647863247863247,
      "loss": 1.0608,
      "step": 1340
    },
    {
      "epoch": 0.20768431983385255,
      "grad_norm": 0.7015015482902527,
      "learning_rate": 0.00017613675213675213,
      "loss": 1.0008,
      "step": 1350
    },
    {
      "epoch": 0.2092227222029922,
      "grad_norm": 0.7774748802185059,
      "learning_rate": 0.00017579487179487178,
      "loss": 1.0223,
      "step": 1360
    },
    {
      "epoch": 0.21076112457213184,
      "grad_norm": 0.4953027069568634,
      "learning_rate": 0.00017545299145299144,
      "loss": 1.0264,
      "step": 1370
    },
    {
      "epoch": 0.2122995269412715,
      "grad_norm": 0.3876659870147705,
      "learning_rate": 0.00017511111111111113,
      "loss": 1.0861,
      "step": 1380
    },
    {
      "epoch": 0.21383792931041115,
      "grad_norm": 0.4554290771484375,
      "learning_rate": 0.00017476923076923078,
      "loss": 1.0197,
      "step": 1390
    },
    {
      "epoch": 0.21537633167955078,
      "grad_norm": 0.47685301303863525,
      "learning_rate": 0.00017442735042735044,
      "loss": 1.0574,
      "step": 1400
    },
    {
      "epoch": 0.21691473404869044,
      "grad_norm": 0.5016463994979858,
      "learning_rate": 0.0001740854700854701,
      "loss": 1.0582,
      "step": 1410
    },
    {
      "epoch": 0.2184531364178301,
      "grad_norm": 0.48101872205734253,
      "learning_rate": 0.00017374358974358976,
      "loss": 1.0172,
      "step": 1420
    },
    {
      "epoch": 0.21999153878696973,
      "grad_norm": 0.47797712683677673,
      "learning_rate": 0.00017340170940170942,
      "loss": 1.0505,
      "step": 1430
    },
    {
      "epoch": 0.22152994115610938,
      "grad_norm": 0.5501773357391357,
      "learning_rate": 0.00017305982905982907,
      "loss": 1.019,
      "step": 1440
    },
    {
      "epoch": 0.22306834352524904,
      "grad_norm": 0.4892399311065674,
      "learning_rate": 0.00017271794871794873,
      "loss": 1.0413,
      "step": 1450
    },
    {
      "epoch": 0.22460674589438867,
      "grad_norm": 0.49763554334640503,
      "learning_rate": 0.0001723760683760684,
      "loss": 1.0176,
      "step": 1460
    },
    {
      "epoch": 0.22614514826352833,
      "grad_norm": 0.4826153516769409,
      "learning_rate": 0.00017203418803418805,
      "loss": 0.9775,
      "step": 1470
    },
    {
      "epoch": 0.22768355063266799,
      "grad_norm": 0.4612135589122772,
      "learning_rate": 0.0001716923076923077,
      "loss": 1.0594,
      "step": 1480
    },
    {
      "epoch": 0.22922195300180762,
      "grad_norm": 0.4901314973831177,
      "learning_rate": 0.00017135042735042736,
      "loss": 1.0009,
      "step": 1490
    },
    {
      "epoch": 0.23076035537094727,
      "grad_norm": 0.589230477809906,
      "learning_rate": 0.00017100854700854702,
      "loss": 0.9804,
      "step": 1500
    },
    {
      "epoch": 0.23229875774008693,
      "grad_norm": 0.5508666634559631,
      "learning_rate": 0.00017066666666666668,
      "loss": 0.9797,
      "step": 1510
    },
    {
      "epoch": 0.23383716010922656,
      "grad_norm": 0.4979475736618042,
      "learning_rate": 0.00017032478632478633,
      "loss": 0.9843,
      "step": 1520
    },
    {
      "epoch": 0.23537556247836622,
      "grad_norm": 0.6265875101089478,
      "learning_rate": 0.000169982905982906,
      "loss": 0.9785,
      "step": 1530
    },
    {
      "epoch": 0.23691396484750588,
      "grad_norm": 0.593346118927002,
      "learning_rate": 0.00016964102564102565,
      "loss": 1.0834,
      "step": 1540
    },
    {
      "epoch": 0.2384523672166455,
      "grad_norm": 0.4974205195903778,
      "learning_rate": 0.0001692991452991453,
      "loss": 1.0233,
      "step": 1550
    },
    {
      "epoch": 0.23999076958578516,
      "grad_norm": 0.4518866539001465,
      "learning_rate": 0.00016895726495726497,
      "loss": 1.0232,
      "step": 1560
    },
    {
      "epoch": 0.24152917195492482,
      "grad_norm": 0.8069553375244141,
      "learning_rate": 0.00016861538461538462,
      "loss": 1.0455,
      "step": 1570
    },
    {
      "epoch": 0.24306757432406445,
      "grad_norm": 0.432888388633728,
      "learning_rate": 0.00016827350427350428,
      "loss": 1.0586,
      "step": 1580
    },
    {
      "epoch": 0.2446059766932041,
      "grad_norm": 0.44304749369621277,
      "learning_rate": 0.00016793162393162394,
      "loss": 1.0687,
      "step": 1590
    },
    {
      "epoch": 0.24614437906234377,
      "grad_norm": 0.5872960686683655,
      "learning_rate": 0.0001675897435897436,
      "loss": 0.9978,
      "step": 1600
    },
    {
      "epoch": 0.2476827814314834,
      "grad_norm": 1.649830937385559,
      "learning_rate": 0.00016724786324786325,
      "loss": 1.091,
      "step": 1610
    },
    {
      "epoch": 0.24922118380062305,
      "grad_norm": 0.566842257976532,
      "learning_rate": 0.0001669059829059829,
      "loss": 0.9668,
      "step": 1620
    },
    {
      "epoch": 0.2507595861697627,
      "grad_norm": 0.5304808616638184,
      "learning_rate": 0.00016656410256410257,
      "loss": 1.0779,
      "step": 1630
    },
    {
      "epoch": 0.25229798853890234,
      "grad_norm": 0.5211077332496643,
      "learning_rate": 0.00016622222222222223,
      "loss": 1.0482,
      "step": 1640
    },
    {
      "epoch": 0.253836390908042,
      "grad_norm": 0.5898643732070923,
      "learning_rate": 0.00016588034188034188,
      "loss": 0.9736,
      "step": 1650
    },
    {
      "epoch": 0.25537479327718166,
      "grad_norm": 0.438373327255249,
      "learning_rate": 0.00016553846153846154,
      "loss": 0.978,
      "step": 1660
    },
    {
      "epoch": 0.2569131956463213,
      "grad_norm": 0.5471181273460388,
      "learning_rate": 0.0001651965811965812,
      "loss": 1.079,
      "step": 1670
    },
    {
      "epoch": 0.25845159801546097,
      "grad_norm": 0.4805585741996765,
      "learning_rate": 0.00016485470085470088,
      "loss": 1.0538,
      "step": 1680
    },
    {
      "epoch": 0.2599900003846006,
      "grad_norm": 0.5803124904632568,
      "learning_rate": 0.00016451282051282054,
      "loss": 1.096,
      "step": 1690
    },
    {
      "epoch": 0.26152840275374023,
      "grad_norm": 0.5438754558563232,
      "learning_rate": 0.00016417094017094017,
      "loss": 0.9875,
      "step": 1700
    },
    {
      "epoch": 0.2630668051228799,
      "grad_norm": 0.4743230938911438,
      "learning_rate": 0.00016382905982905983,
      "loss": 0.9778,
      "step": 1710
    },
    {
      "epoch": 0.26460520749201955,
      "grad_norm": 0.48273923993110657,
      "learning_rate": 0.0001634871794871795,
      "loss": 1.0281,
      "step": 1720
    },
    {
      "epoch": 0.2661436098611592,
      "grad_norm": 0.456667959690094,
      "learning_rate": 0.00016314529914529915,
      "loss": 0.9784,
      "step": 1730
    },
    {
      "epoch": 0.26768201223029886,
      "grad_norm": 0.5305406451225281,
      "learning_rate": 0.0001628034188034188,
      "loss": 0.9641,
      "step": 1740
    },
    {
      "epoch": 0.26922041459943846,
      "grad_norm": 0.521521270275116,
      "learning_rate": 0.00016246153846153846,
      "loss": 0.9471,
      "step": 1750
    },
    {
      "epoch": 0.2707588169685781,
      "grad_norm": 0.5061790943145752,
      "learning_rate": 0.00016211965811965812,
      "loss": 1.0047,
      "step": 1760
    },
    {
      "epoch": 0.2722972193377178,
      "grad_norm": 0.45628097653388977,
      "learning_rate": 0.00016177777777777778,
      "loss": 0.9981,
      "step": 1770
    },
    {
      "epoch": 0.27383562170685743,
      "grad_norm": 0.5362868905067444,
      "learning_rate": 0.00016143589743589743,
      "loss": 1.0533,
      "step": 1780
    },
    {
      "epoch": 0.2753740240759971,
      "grad_norm": 0.475908488035202,
      "learning_rate": 0.0001610940170940171,
      "loss": 1.0608,
      "step": 1790
    },
    {
      "epoch": 0.27691242644513675,
      "grad_norm": 0.517157256603241,
      "learning_rate": 0.00016075213675213675,
      "loss": 1.0611,
      "step": 1800
    },
    {
      "epoch": 0.27845082881427635,
      "grad_norm": 0.4431826174259186,
      "learning_rate": 0.0001604102564102564,
      "loss": 1.0208,
      "step": 1810
    },
    {
      "epoch": 0.279989231183416,
      "grad_norm": 0.4075940251350403,
      "learning_rate": 0.0001600683760683761,
      "loss": 1.0293,
      "step": 1820
    },
    {
      "epoch": 0.28152763355255567,
      "grad_norm": 0.4613097608089447,
      "learning_rate": 0.00015972649572649575,
      "loss": 1.0069,
      "step": 1830
    },
    {
      "epoch": 0.2830660359216953,
      "grad_norm": 0.4482089877128601,
      "learning_rate": 0.0001593846153846154,
      "loss": 0.9918,
      "step": 1840
    },
    {
      "epoch": 0.284604438290835,
      "grad_norm": 0.4703086018562317,
      "learning_rate": 0.00015904273504273507,
      "loss": 1.0441,
      "step": 1850
    },
    {
      "epoch": 0.28614284065997464,
      "grad_norm": 0.45097023248672485,
      "learning_rate": 0.00015870085470085472,
      "loss": 1.0661,
      "step": 1860
    },
    {
      "epoch": 0.28768124302911424,
      "grad_norm": 0.515642523765564,
      "learning_rate": 0.00015835897435897438,
      "loss": 1.0211,
      "step": 1870
    },
    {
      "epoch": 0.2892196453982539,
      "grad_norm": 0.4621179699897766,
      "learning_rate": 0.000158017094017094,
      "loss": 1.0384,
      "step": 1880
    },
    {
      "epoch": 0.29075804776739356,
      "grad_norm": 0.5528222322463989,
      "learning_rate": 0.00015767521367521367,
      "loss": 1.0289,
      "step": 1890
    },
    {
      "epoch": 0.2922964501365332,
      "grad_norm": 0.4765111804008484,
      "learning_rate": 0.00015733333333333333,
      "loss": 1.0358,
      "step": 1900
    },
    {
      "epoch": 0.29383485250567287,
      "grad_norm": 0.44668295979499817,
      "learning_rate": 0.00015699145299145298,
      "loss": 1.0387,
      "step": 1910
    },
    {
      "epoch": 0.29537325487481253,
      "grad_norm": 0.5040528178215027,
      "learning_rate": 0.00015664957264957264,
      "loss": 1.0139,
      "step": 1920
    },
    {
      "epoch": 0.29691165724395213,
      "grad_norm": 0.4558364450931549,
      "learning_rate": 0.0001563076923076923,
      "loss": 1.0437,
      "step": 1930
    },
    {
      "epoch": 0.2984500596130918,
      "grad_norm": 0.45144742727279663,
      "learning_rate": 0.00015596581196581196,
      "loss": 1.0264,
      "step": 1940
    },
    {
      "epoch": 0.29998846198223145,
      "grad_norm": 0.5601553320884705,
      "learning_rate": 0.00015562393162393164,
      "loss": 0.9812,
      "step": 1950
    },
    {
      "epoch": 0.3015268643513711,
      "grad_norm": 0.4512892961502075,
      "learning_rate": 0.0001552820512820513,
      "loss": 1.0399,
      "step": 1960
    },
    {
      "epoch": 0.30306526672051076,
      "grad_norm": 0.4634018838405609,
      "learning_rate": 0.00015494017094017096,
      "loss": 0.9926,
      "step": 1970
    },
    {
      "epoch": 0.3046036690896504,
      "grad_norm": 0.46779969334602356,
      "learning_rate": 0.00015459829059829062,
      "loss": 1.0136,
      "step": 1980
    },
    {
      "epoch": 0.30614207145879,
      "grad_norm": 0.46477067470550537,
      "learning_rate": 0.00015425641025641027,
      "loss": 1.1196,
      "step": 1990
    },
    {
      "epoch": 0.3076804738279297,
      "grad_norm": 0.4696638286113739,
      "learning_rate": 0.00015391452991452993,
      "loss": 1.0334,
      "step": 2000
    },
    {
      "epoch": 0.30921887619706934,
      "grad_norm": 0.452246755361557,
      "learning_rate": 0.0001535726495726496,
      "loss": 1.065,
      "step": 2010
    },
    {
      "epoch": 0.310757278566209,
      "grad_norm": 0.45645472407341003,
      "learning_rate": 0.00015323076923076925,
      "loss": 1.0647,
      "step": 2020
    },
    {
      "epoch": 0.31229568093534865,
      "grad_norm": 0.43664100766181946,
      "learning_rate": 0.0001528888888888889,
      "loss": 1.0037,
      "step": 2030
    },
    {
      "epoch": 0.3138340833044883,
      "grad_norm": 0.44121554493904114,
      "learning_rate": 0.00015254700854700856,
      "loss": 1.0471,
      "step": 2040
    },
    {
      "epoch": 0.3153724856736279,
      "grad_norm": 0.4761958122253418,
      "learning_rate": 0.00015220512820512822,
      "loss": 1.0821,
      "step": 2050
    },
    {
      "epoch": 0.31691088804276757,
      "grad_norm": 0.6178991794586182,
      "learning_rate": 0.00015186324786324785,
      "loss": 1.0819,
      "step": 2060
    },
    {
      "epoch": 0.3184492904119072,
      "grad_norm": 0.4778635799884796,
      "learning_rate": 0.0001515213675213675,
      "loss": 0.9992,
      "step": 2070
    },
    {
      "epoch": 0.3199876927810469,
      "grad_norm": 0.5040847063064575,
      "learning_rate": 0.00015117948717948717,
      "loss": 0.9499,
      "step": 2080
    },
    {
      "epoch": 0.32152609515018654,
      "grad_norm": 0.5121925473213196,
      "learning_rate": 0.00015083760683760685,
      "loss": 1.0061,
      "step": 2090
    },
    {
      "epoch": 0.3230644975193262,
      "grad_norm": 0.4523119330406189,
      "learning_rate": 0.0001504957264957265,
      "loss": 1.0362,
      "step": 2100
    },
    {
      "epoch": 0.3246028998884658,
      "grad_norm": 0.42942947149276733,
      "learning_rate": 0.00015015384615384617,
      "loss": 1.0222,
      "step": 2110
    },
    {
      "epoch": 0.32614130225760546,
      "grad_norm": 0.512516975402832,
      "learning_rate": 0.00014981196581196582,
      "loss": 0.9766,
      "step": 2120
    },
    {
      "epoch": 0.3276797046267451,
      "grad_norm": 0.4922696352005005,
      "learning_rate": 0.00014947008547008548,
      "loss": 1.0475,
      "step": 2130
    },
    {
      "epoch": 0.3292181069958848,
      "grad_norm": 0.4694203734397888,
      "learning_rate": 0.00014912820512820514,
      "loss": 1.0292,
      "step": 2140
    },
    {
      "epoch": 0.33075650936502443,
      "grad_norm": 0.5093258023262024,
      "learning_rate": 0.0001487863247863248,
      "loss": 1.0582,
      "step": 2150
    },
    {
      "epoch": 0.3322949117341641,
      "grad_norm": 0.46809127926826477,
      "learning_rate": 0.00014844444444444445,
      "loss": 1.0181,
      "step": 2160
    },
    {
      "epoch": 0.33383331410330375,
      "grad_norm": 0.43712982535362244,
      "learning_rate": 0.0001481025641025641,
      "loss": 1.02,
      "step": 2170
    },
    {
      "epoch": 0.33537171647244335,
      "grad_norm": 0.5161628723144531,
      "learning_rate": 0.00014776068376068377,
      "loss": 1.075,
      "step": 2180
    },
    {
      "epoch": 0.336910118841583,
      "grad_norm": 0.4963924288749695,
      "learning_rate": 0.00014741880341880343,
      "loss": 1.0453,
      "step": 2190
    },
    {
      "epoch": 0.33844852121072266,
      "grad_norm": 0.44669291377067566,
      "learning_rate": 0.00014707692307692308,
      "loss": 0.9485,
      "step": 2200
    },
    {
      "epoch": 0.3399869235798623,
      "grad_norm": 0.4726347029209137,
      "learning_rate": 0.00014673504273504274,
      "loss": 1.0377,
      "step": 2210
    },
    {
      "epoch": 0.341525325949002,
      "grad_norm": 0.38600215315818787,
      "learning_rate": 0.0001463931623931624,
      "loss": 1.0553,
      "step": 2220
    },
    {
      "epoch": 0.34306372831814164,
      "grad_norm": 0.47462794184684753,
      "learning_rate": 0.00014605128205128206,
      "loss": 0.9994,
      "step": 2230
    },
    {
      "epoch": 0.34460213068728124,
      "grad_norm": 0.4465763568878174,
      "learning_rate": 0.00014570940170940172,
      "loss": 0.9951,
      "step": 2240
    },
    {
      "epoch": 0.3461405330564209,
      "grad_norm": 0.4504924714565277,
      "learning_rate": 0.00014536752136752137,
      "loss": 1.0283,
      "step": 2250
    },
    {
      "epoch": 0.34767893542556055,
      "grad_norm": 0.4634293019771576,
      "learning_rate": 0.00014502564102564103,
      "loss": 0.9875,
      "step": 2260
    },
    {
      "epoch": 0.3492173377947002,
      "grad_norm": 0.4401499629020691,
      "learning_rate": 0.0001446837606837607,
      "loss": 1.0025,
      "step": 2270
    },
    {
      "epoch": 0.35075574016383987,
      "grad_norm": 0.5444926619529724,
      "learning_rate": 0.00014434188034188035,
      "loss": 1.0173,
      "step": 2280
    },
    {
      "epoch": 0.3522941425329795,
      "grad_norm": 0.4763002395629883,
      "learning_rate": 0.000144,
      "loss": 0.9704,
      "step": 2290
    },
    {
      "epoch": 0.35383254490211913,
      "grad_norm": 0.5075652599334717,
      "learning_rate": 0.00014365811965811966,
      "loss": 1.008,
      "step": 2300
    },
    {
      "epoch": 0.3553709472712588,
      "grad_norm": 0.489330917596817,
      "learning_rate": 0.00014331623931623932,
      "loss": 0.9758,
      "step": 2310
    },
    {
      "epoch": 0.35690934964039844,
      "grad_norm": 0.4513956308364868,
      "learning_rate": 0.00014297435897435898,
      "loss": 1.0316,
      "step": 2320
    },
    {
      "epoch": 0.3584477520095381,
      "grad_norm": 0.4388315975666046,
      "learning_rate": 0.00014263247863247863,
      "loss": 1.063,
      "step": 2330
    },
    {
      "epoch": 0.35998615437867776,
      "grad_norm": 0.5001503229141235,
      "learning_rate": 0.0001422905982905983,
      "loss": 1.0446,
      "step": 2340
    },
    {
      "epoch": 0.3615245567478174,
      "grad_norm": 0.5730881690979004,
      "learning_rate": 0.00014194871794871795,
      "loss": 0.9892,
      "step": 2350
    },
    {
      "epoch": 0.363062959116957,
      "grad_norm": 0.56324702501297,
      "learning_rate": 0.0001416068376068376,
      "loss": 1.0799,
      "step": 2360
    },
    {
      "epoch": 0.3646013614860967,
      "grad_norm": 0.47002285718917847,
      "learning_rate": 0.00014126495726495727,
      "loss": 1.0665,
      "step": 2370
    },
    {
      "epoch": 0.36613976385523633,
      "grad_norm": 0.551929771900177,
      "learning_rate": 0.00014092307692307692,
      "loss": 1.0239,
      "step": 2380
    },
    {
      "epoch": 0.367678166224376,
      "grad_norm": 0.5911964178085327,
      "learning_rate": 0.0001405811965811966,
      "loss": 1.0258,
      "step": 2390
    },
    {
      "epoch": 0.36921656859351565,
      "grad_norm": 0.5289866924285889,
      "learning_rate": 0.00014023931623931627,
      "loss": 0.9667,
      "step": 2400
    },
    {
      "epoch": 0.3707549709626553,
      "grad_norm": 0.4729161262512207,
      "learning_rate": 0.00013989743589743592,
      "loss": 1.0122,
      "step": 2410
    },
    {
      "epoch": 0.3722933733317949,
      "grad_norm": 0.426726758480072,
      "learning_rate": 0.00013955555555555558,
      "loss": 0.9985,
      "step": 2420
    },
    {
      "epoch": 0.37383177570093457,
      "grad_norm": 0.526434063911438,
      "learning_rate": 0.0001392136752136752,
      "loss": 0.9952,
      "step": 2430
    },
    {
      "epoch": 0.3753701780700742,
      "grad_norm": 0.6388368606567383,
      "learning_rate": 0.00013887179487179487,
      "loss": 1.017,
      "step": 2440
    },
    {
      "epoch": 0.3769085804392139,
      "grad_norm": 0.4309264123439789,
      "learning_rate": 0.00013852991452991453,
      "loss": 0.9927,
      "step": 2450
    },
    {
      "epoch": 0.37844698280835354,
      "grad_norm": 0.48485633730888367,
      "learning_rate": 0.00013818803418803418,
      "loss": 1.0217,
      "step": 2460
    },
    {
      "epoch": 0.3799853851774932,
      "grad_norm": 0.482890784740448,
      "learning_rate": 0.00013784615384615384,
      "loss": 0.9824,
      "step": 2470
    },
    {
      "epoch": 0.3815237875466328,
      "grad_norm": 0.4928055703639984,
      "learning_rate": 0.0001375042735042735,
      "loss": 1.0613,
      "step": 2480
    },
    {
      "epoch": 0.38306218991577246,
      "grad_norm": 0.5213229060173035,
      "learning_rate": 0.00013716239316239316,
      "loss": 1.0067,
      "step": 2490
    },
    {
      "epoch": 0.3846005922849121,
      "grad_norm": 0.46435466408729553,
      "learning_rate": 0.00013682051282051282,
      "loss": 0.993,
      "step": 2500
    },
    {
      "epoch": 0.38613899465405177,
      "grad_norm": 0.5699139833450317,
      "learning_rate": 0.00013647863247863247,
      "loss": 1.0204,
      "step": 2510
    },
    {
      "epoch": 0.38767739702319143,
      "grad_norm": 0.45603734254837036,
      "learning_rate": 0.00013613675213675213,
      "loss": 1.0268,
      "step": 2520
    },
    {
      "epoch": 0.3892157993923311,
      "grad_norm": 0.5721235871315002,
      "learning_rate": 0.00013579487179487182,
      "loss": 0.9732,
      "step": 2530
    },
    {
      "epoch": 0.3907542017614707,
      "grad_norm": 0.4519679844379425,
      "learning_rate": 0.00013545299145299147,
      "loss": 1.0198,
      "step": 2540
    },
    {
      "epoch": 0.39229260413061035,
      "grad_norm": 0.49018147587776184,
      "learning_rate": 0.00013511111111111113,
      "loss": 1.0004,
      "step": 2550
    },
    {
      "epoch": 0.39383100649975,
      "grad_norm": 0.4484868049621582,
      "learning_rate": 0.0001347692307692308,
      "loss": 1.012,
      "step": 2560
    },
    {
      "epoch": 0.39536940886888966,
      "grad_norm": 0.4514232575893402,
      "learning_rate": 0.00013442735042735045,
      "loss": 1.0314,
      "step": 2570
    },
    {
      "epoch": 0.3969078112380293,
      "grad_norm": 0.5341078042984009,
      "learning_rate": 0.0001340854700854701,
      "loss": 1.0429,
      "step": 2580
    },
    {
      "epoch": 0.398446213607169,
      "grad_norm": 0.47826820611953735,
      "learning_rate": 0.00013374358974358976,
      "loss": 0.9353,
      "step": 2590
    },
    {
      "epoch": 0.3999846159763086,
      "grad_norm": 0.44649574160575867,
      "learning_rate": 0.00013340170940170942,
      "loss": 1.0336,
      "step": 2600
    },
    {
      "epoch": 0.40152301834544823,
      "grad_norm": 0.5894924998283386,
      "learning_rate": 0.00013305982905982905,
      "loss": 0.9613,
      "step": 2610
    },
    {
      "epoch": 0.4030614207145879,
      "grad_norm": 0.6241974234580994,
      "learning_rate": 0.0001327179487179487,
      "loss": 0.9576,
      "step": 2620
    },
    {
      "epoch": 0.40459982308372755,
      "grad_norm": 0.5636184811592102,
      "learning_rate": 0.00013237606837606836,
      "loss": 1.0198,
      "step": 2630
    },
    {
      "epoch": 0.4061382254528672,
      "grad_norm": 0.4794470965862274,
      "learning_rate": 0.00013203418803418802,
      "loss": 1.0137,
      "step": 2640
    },
    {
      "epoch": 0.40767662782200687,
      "grad_norm": 0.4346396327018738,
      "learning_rate": 0.00013169230769230768,
      "loss": 1.0509,
      "step": 2650
    },
    {
      "epoch": 0.40921503019114647,
      "grad_norm": 0.6024537086486816,
      "learning_rate": 0.00013135042735042737,
      "loss": 0.971,
      "step": 2660
    },
    {
      "epoch": 0.4107534325602861,
      "grad_norm": 0.48606881499290466,
      "learning_rate": 0.00013100854700854702,
      "loss": 0.986,
      "step": 2670
    },
    {
      "epoch": 0.4122918349294258,
      "grad_norm": 0.44571158289909363,
      "learning_rate": 0.00013066666666666668,
      "loss": 1.0989,
      "step": 2680
    },
    {
      "epoch": 0.41383023729856544,
      "grad_norm": 0.4881031811237335,
      "learning_rate": 0.00013032478632478634,
      "loss": 1.0537,
      "step": 2690
    },
    {
      "epoch": 0.4153686396677051,
      "grad_norm": 0.5769477486610413,
      "learning_rate": 0.000129982905982906,
      "loss": 1.0136,
      "step": 2700
    },
    {
      "epoch": 0.41690704203684475,
      "grad_norm": 0.4960319697856903,
      "learning_rate": 0.00012964102564102565,
      "loss": 1.0478,
      "step": 2710
    },
    {
      "epoch": 0.4184454444059844,
      "grad_norm": 0.47399699687957764,
      "learning_rate": 0.0001292991452991453,
      "loss": 1.011,
      "step": 2720
    },
    {
      "epoch": 0.419983846775124,
      "grad_norm": 1.5989514589309692,
      "learning_rate": 0.00012895726495726497,
      "loss": 0.9706,
      "step": 2730
    },
    {
      "epoch": 0.42152224914426367,
      "grad_norm": 0.46955832839012146,
      "learning_rate": 0.00012861538461538463,
      "loss": 1.0276,
      "step": 2740
    },
    {
      "epoch": 0.42306065151340333,
      "grad_norm": 0.48883306980133057,
      "learning_rate": 0.00012827350427350428,
      "loss": 1.044,
      "step": 2750
    },
    {
      "epoch": 0.424599053882543,
      "grad_norm": 0.5509262681007385,
      "learning_rate": 0.00012793162393162394,
      "loss": 1.0287,
      "step": 2760
    },
    {
      "epoch": 0.42613745625168264,
      "grad_norm": 0.49244505167007446,
      "learning_rate": 0.0001275897435897436,
      "loss": 1.0132,
      "step": 2770
    },
    {
      "epoch": 0.4276758586208223,
      "grad_norm": 0.5097609758377075,
      "learning_rate": 0.00012724786324786326,
      "loss": 1.0386,
      "step": 2780
    },
    {
      "epoch": 0.4292142609899619,
      "grad_norm": 0.4813893437385559,
      "learning_rate": 0.00012690598290598292,
      "loss": 1.0349,
      "step": 2790
    },
    {
      "epoch": 0.43075266335910156,
      "grad_norm": 0.4296726584434509,
      "learning_rate": 0.00012656410256410257,
      "loss": 0.97,
      "step": 2800
    },
    {
      "epoch": 0.4322910657282412,
      "grad_norm": 0.4754703938961029,
      "learning_rate": 0.00012622222222222223,
      "loss": 1.0236,
      "step": 2810
    },
    {
      "epoch": 0.4338294680973809,
      "grad_norm": 0.44898521900177,
      "learning_rate": 0.0001258803418803419,
      "loss": 1.0008,
      "step": 2820
    },
    {
      "epoch": 0.43536787046652053,
      "grad_norm": 0.4491705298423767,
      "learning_rate": 0.00012553846153846155,
      "loss": 0.9552,
      "step": 2830
    },
    {
      "epoch": 0.4369062728356602,
      "grad_norm": 0.4652876853942871,
      "learning_rate": 0.0001251965811965812,
      "loss": 1.0188,
      "step": 2840
    },
    {
      "epoch": 0.4384446752047998,
      "grad_norm": 0.48203080892562866,
      "learning_rate": 0.00012485470085470086,
      "loss": 1.0227,
      "step": 2850
    },
    {
      "epoch": 0.43998307757393945,
      "grad_norm": 0.5408573746681213,
      "learning_rate": 0.00012451282051282052,
      "loss": 1.0383,
      "step": 2860
    },
    {
      "epoch": 0.4415214799430791,
      "grad_norm": 0.7079630494117737,
      "learning_rate": 0.00012417094017094018,
      "loss": 1.0821,
      "step": 2870
    },
    {
      "epoch": 0.44305988231221877,
      "grad_norm": 0.49403104186058044,
      "learning_rate": 0.00012382905982905983,
      "loss": 1.0192,
      "step": 2880
    },
    {
      "epoch": 0.4445982846813584,
      "grad_norm": 0.44623225927352905,
      "learning_rate": 0.0001234871794871795,
      "loss": 1.0238,
      "step": 2890
    },
    {
      "epoch": 0.4461366870504981,
      "grad_norm": 0.4913828670978546,
      "learning_rate": 0.00012314529914529915,
      "loss": 1.0098,
      "step": 2900
    },
    {
      "epoch": 0.4476750894196377,
      "grad_norm": 0.4788702428340912,
      "learning_rate": 0.0001228034188034188,
      "loss": 0.9463,
      "step": 2910
    },
    {
      "epoch": 0.44921349178877734,
      "grad_norm": 0.476877361536026,
      "learning_rate": 0.00012246153846153846,
      "loss": 1.0684,
      "step": 2920
    },
    {
      "epoch": 0.450751894157917,
      "grad_norm": 0.4510871469974518,
      "learning_rate": 0.00012211965811965812,
      "loss": 1.022,
      "step": 2930
    },
    {
      "epoch": 0.45229029652705666,
      "grad_norm": 0.44846615195274353,
      "learning_rate": 0.0001217777777777778,
      "loss": 0.9378,
      "step": 2940
    },
    {
      "epoch": 0.4538286988961963,
      "grad_norm": 0.5603658556938171,
      "learning_rate": 0.00012143589743589745,
      "loss": 1.0696,
      "step": 2950
    },
    {
      "epoch": 0.45536710126533597,
      "grad_norm": 0.4440000355243683,
      "learning_rate": 0.00012109401709401711,
      "loss": 0.9738,
      "step": 2960
    },
    {
      "epoch": 0.4569055036344756,
      "grad_norm": 0.4420330226421356,
      "learning_rate": 0.00012075213675213677,
      "loss": 1.0835,
      "step": 2970
    },
    {
      "epoch": 0.45844390600361523,
      "grad_norm": 0.5237401127815247,
      "learning_rate": 0.00012041025641025641,
      "loss": 1.0565,
      "step": 2980
    },
    {
      "epoch": 0.4599823083727549,
      "grad_norm": 0.47431764006614685,
      "learning_rate": 0.00012006837606837607,
      "loss": 1.0452,
      "step": 2990
    },
    {
      "epoch": 0.46152071074189455,
      "grad_norm": 0.5057913661003113,
      "learning_rate": 0.00011972649572649573,
      "loss": 1.04,
      "step": 3000
    },
    {
      "epoch": 0.4630591131110342,
      "grad_norm": 0.4987821877002716,
      "learning_rate": 0.00011938461538461538,
      "loss": 1.115,
      "step": 3010
    },
    {
      "epoch": 0.46459751548017386,
      "grad_norm": 0.5371751189231873,
      "learning_rate": 0.00011904273504273504,
      "loss": 0.9389,
      "step": 3020
    },
    {
      "epoch": 0.46613591784931346,
      "grad_norm": 0.4830132722854614,
      "learning_rate": 0.0001187008547008547,
      "loss": 0.9976,
      "step": 3030
    },
    {
      "epoch": 0.4676743202184531,
      "grad_norm": 0.45632466673851013,
      "learning_rate": 0.00011835897435897436,
      "loss": 0.9714,
      "step": 3040
    },
    {
      "epoch": 0.4692127225875928,
      "grad_norm": 0.397664874792099,
      "learning_rate": 0.00011801709401709401,
      "loss": 1.0266,
      "step": 3050
    },
    {
      "epoch": 0.47075112495673244,
      "grad_norm": 0.5173747539520264,
      "learning_rate": 0.00011767521367521369,
      "loss": 1.0108,
      "step": 3060
    },
    {
      "epoch": 0.4722895273258721,
      "grad_norm": 0.4755321443080902,
      "learning_rate": 0.00011733333333333334,
      "loss": 1.0571,
      "step": 3070
    },
    {
      "epoch": 0.47382792969501175,
      "grad_norm": 0.489154577255249,
      "learning_rate": 0.000116991452991453,
      "loss": 0.9252,
      "step": 3080
    },
    {
      "epoch": 0.47536633206415135,
      "grad_norm": 0.44085070490837097,
      "learning_rate": 0.00011664957264957266,
      "loss": 1.015,
      "step": 3090
    },
    {
      "epoch": 0.476904734433291,
      "grad_norm": 0.5117154717445374,
      "learning_rate": 0.00011630769230769232,
      "loss": 1.0788,
      "step": 3100
    },
    {
      "epoch": 0.47844313680243067,
      "grad_norm": 0.5192663073539734,
      "learning_rate": 0.00011596581196581197,
      "loss": 1.0173,
      "step": 3110
    },
    {
      "epoch": 0.4799815391715703,
      "grad_norm": 0.4797329604625702,
      "learning_rate": 0.00011562393162393163,
      "loss": 1.0285,
      "step": 3120
    },
    {
      "epoch": 0.48151994154071,
      "grad_norm": 0.4516552686691284,
      "learning_rate": 0.00011528205128205129,
      "loss": 1.0534,
      "step": 3130
    },
    {
      "epoch": 0.48305834390984964,
      "grad_norm": 0.3859146237373352,
      "learning_rate": 0.00011494017094017096,
      "loss": 0.9954,
      "step": 3140
    },
    {
      "epoch": 0.48459674627898924,
      "grad_norm": 0.43039536476135254,
      "learning_rate": 0.00011459829059829062,
      "loss": 1.0256,
      "step": 3150
    },
    {
      "epoch": 0.4861351486481289,
      "grad_norm": 0.4785546362400055,
      "learning_rate": 0.00011425641025641025,
      "loss": 0.9854,
      "step": 3160
    },
    {
      "epoch": 0.48767355101726856,
      "grad_norm": 0.6272962093353271,
      "learning_rate": 0.00011391452991452991,
      "loss": 0.9883,
      "step": 3170
    },
    {
      "epoch": 0.4892119533864082,
      "grad_norm": 0.5059628486633301,
      "learning_rate": 0.00011357264957264956,
      "loss": 0.9654,
      "step": 3180
    },
    {
      "epoch": 0.4907503557555479,
      "grad_norm": 0.4502537250518799,
      "learning_rate": 0.00011323076923076922,
      "loss": 0.9219,
      "step": 3190
    },
    {
      "epoch": 0.49228875812468753,
      "grad_norm": 0.5902127027511597,
      "learning_rate": 0.0001128888888888889,
      "loss": 0.9966,
      "step": 3200
    },
    {
      "epoch": 0.49382716049382713,
      "grad_norm": 0.49637946486473083,
      "learning_rate": 0.00011254700854700855,
      "loss": 0.9747,
      "step": 3210
    },
    {
      "epoch": 0.4953655628629668,
      "grad_norm": 0.4538553059101105,
      "learning_rate": 0.00011220512820512821,
      "loss": 1.0122,
      "step": 3220
    },
    {
      "epoch": 0.49690396523210645,
      "grad_norm": 0.4777303636074066,
      "learning_rate": 0.00011186324786324787,
      "loss": 1.0046,
      "step": 3230
    },
    {
      "epoch": 0.4984423676012461,
      "grad_norm": 0.4581485986709595,
      "learning_rate": 0.00011152136752136752,
      "loss": 1.0116,
      "step": 3240
    },
    {
      "epoch": 0.49998076997038576,
      "grad_norm": 0.45275676250457764,
      "learning_rate": 0.00011117948717948718,
      "loss": 1.0269,
      "step": 3250
    },
    {
      "epoch": 0.5015191723395254,
      "grad_norm": 1.0057971477508545,
      "learning_rate": 0.00011083760683760684,
      "loss": 1.0592,
      "step": 3260
    },
    {
      "epoch": 0.5030575747086651,
      "grad_norm": 0.433308869600296,
      "learning_rate": 0.0001104957264957265,
      "loss": 1.0466,
      "step": 3270
    },
    {
      "epoch": 0.5045959770778047,
      "grad_norm": 0.6959103345870972,
      "learning_rate": 0.00011015384615384617,
      "loss": 1.0653,
      "step": 3280
    },
    {
      "epoch": 0.5061343794469444,
      "grad_norm": 0.4877837300300598,
      "learning_rate": 0.00010981196581196583,
      "loss": 1.0481,
      "step": 3290
    },
    {
      "epoch": 0.507672781816084,
      "grad_norm": 0.4811130464076996,
      "learning_rate": 0.00010947008547008548,
      "loss": 1.0379,
      "step": 3300
    },
    {
      "epoch": 0.5092111841852236,
      "grad_norm": 0.4736965000629425,
      "learning_rate": 0.00010912820512820514,
      "loss": 0.9726,
      "step": 3310
    },
    {
      "epoch": 0.5107495865543633,
      "grad_norm": 0.4696624279022217,
      "learning_rate": 0.0001087863247863248,
      "loss": 1.0257,
      "step": 3320
    },
    {
      "epoch": 0.5122879889235029,
      "grad_norm": 0.42184942960739136,
      "learning_rate": 0.00010844444444444446,
      "loss": 0.97,
      "step": 3330
    },
    {
      "epoch": 0.5138263912926426,
      "grad_norm": 0.4114851951599121,
      "learning_rate": 0.0001081025641025641,
      "loss": 1.0316,
      "step": 3340
    },
    {
      "epoch": 0.5153647936617822,
      "grad_norm": 0.536302387714386,
      "learning_rate": 0.00010776068376068376,
      "loss": 0.9849,
      "step": 3350
    },
    {
      "epoch": 0.5169031960309219,
      "grad_norm": 0.47962480783462524,
      "learning_rate": 0.00010741880341880342,
      "loss": 0.9854,
      "step": 3360
    },
    {
      "epoch": 0.5184415984000615,
      "grad_norm": 0.5005306005477905,
      "learning_rate": 0.00010707692307692307,
      "loss": 1.0238,
      "step": 3370
    },
    {
      "epoch": 0.5199800007692011,
      "grad_norm": 0.46713271737098694,
      "learning_rate": 0.00010673504273504273,
      "loss": 1.0486,
      "step": 3380
    },
    {
      "epoch": 0.5215184031383409,
      "grad_norm": 0.47879284620285034,
      "learning_rate": 0.00010639316239316239,
      "loss": 0.9914,
      "step": 3390
    },
    {
      "epoch": 0.5230568055074805,
      "grad_norm": 0.7694814205169678,
      "learning_rate": 0.00010605128205128205,
      "loss": 1.0241,
      "step": 3400
    },
    {
      "epoch": 0.5245952078766202,
      "grad_norm": 0.48376962542533875,
      "learning_rate": 0.00010570940170940172,
      "loss": 1.0161,
      "step": 3410
    },
    {
      "epoch": 0.5261336102457598,
      "grad_norm": 0.5859001278877258,
      "learning_rate": 0.00010536752136752138,
      "loss": 1.0533,
      "step": 3420
    },
    {
      "epoch": 0.5276720126148994,
      "grad_norm": 0.45253825187683105,
      "learning_rate": 0.00010502564102564103,
      "loss": 1.0521,
      "step": 3430
    },
    {
      "epoch": 0.5292104149840391,
      "grad_norm": 0.46115732192993164,
      "learning_rate": 0.00010468376068376069,
      "loss": 1.0543,
      "step": 3440
    },
    {
      "epoch": 0.5307488173531787,
      "grad_norm": 0.4528706669807434,
      "learning_rate": 0.00010434188034188035,
      "loss": 1.0578,
      "step": 3450
    },
    {
      "epoch": 0.5322872197223184,
      "grad_norm": 0.45014455914497375,
      "learning_rate": 0.00010400000000000001,
      "loss": 1.057,
      "step": 3460
    },
    {
      "epoch": 0.533825622091458,
      "grad_norm": 0.4986632764339447,
      "learning_rate": 0.00010365811965811966,
      "loss": 0.9669,
      "step": 3470
    },
    {
      "epoch": 0.5353640244605977,
      "grad_norm": 0.4341486692428589,
      "learning_rate": 0.00010331623931623932,
      "loss": 0.9694,
      "step": 3480
    },
    {
      "epoch": 0.5369024268297373,
      "grad_norm": 0.5026548504829407,
      "learning_rate": 0.00010297435897435898,
      "loss": 0.9783,
      "step": 3490
    },
    {
      "epoch": 0.5384408291988769,
      "grad_norm": 0.5059952735900879,
      "learning_rate": 0.00010263247863247865,
      "loss": 1.0209,
      "step": 3500
    },
    {
      "epoch": 0.5399792315680166,
      "grad_norm": 0.5065003633499146,
      "learning_rate": 0.00010229059829059831,
      "loss": 1.0432,
      "step": 3510
    },
    {
      "epoch": 0.5415176339371562,
      "grad_norm": 0.47472190856933594,
      "learning_rate": 0.00010194871794871797,
      "loss": 0.986,
      "step": 3520
    },
    {
      "epoch": 0.543056036306296,
      "grad_norm": 0.5457119941711426,
      "learning_rate": 0.0001016068376068376,
      "loss": 1.1381,
      "step": 3530
    },
    {
      "epoch": 0.5445944386754356,
      "grad_norm": 0.5610067844390869,
      "learning_rate": 0.00010126495726495726,
      "loss": 0.9173,
      "step": 3540
    },
    {
      "epoch": 0.5461328410445752,
      "grad_norm": 0.5882203578948975,
      "learning_rate": 0.00010092307692307693,
      "loss": 1.018,
      "step": 3550
    },
    {
      "epoch": 0.5476712434137149,
      "grad_norm": 0.4444960653781891,
      "learning_rate": 0.00010058119658119658,
      "loss": 0.9871,
      "step": 3560
    },
    {
      "epoch": 0.5492096457828545,
      "grad_norm": 0.5150115489959717,
      "learning_rate": 0.00010023931623931624,
      "loss": 0.9607,
      "step": 3570
    },
    {
      "epoch": 0.5507480481519942,
      "grad_norm": 0.5042332410812378,
      "learning_rate": 9.98974358974359e-05,
      "loss": 0.969,
      "step": 3580
    },
    {
      "epoch": 0.5522864505211338,
      "grad_norm": 0.4922487139701843,
      "learning_rate": 9.955555555555556e-05,
      "loss": 1.0209,
      "step": 3590
    },
    {
      "epoch": 0.5538248528902735,
      "grad_norm": 0.4377758204936981,
      "learning_rate": 9.921367521367521e-05,
      "loss": 1.0329,
      "step": 3600
    },
    {
      "epoch": 0.5553632552594131,
      "grad_norm": 0.47488436102867126,
      "learning_rate": 9.887179487179487e-05,
      "loss": 1.0106,
      "step": 3610
    },
    {
      "epoch": 0.5569016576285527,
      "grad_norm": 0.4387638568878174,
      "learning_rate": 9.852991452991453e-05,
      "loss": 1.0284,
      "step": 3620
    },
    {
      "epoch": 0.5584400599976924,
      "grad_norm": 0.49823302030563354,
      "learning_rate": 9.81880341880342e-05,
      "loss": 0.9525,
      "step": 3630
    },
    {
      "epoch": 0.559978462366832,
      "grad_norm": 0.4711962640285492,
      "learning_rate": 9.784615384615386e-05,
      "loss": 0.9833,
      "step": 3640
    },
    {
      "epoch": 0.5615168647359717,
      "grad_norm": 0.5340354442596436,
      "learning_rate": 9.750427350427352e-05,
      "loss": 0.9514,
      "step": 3650
    },
    {
      "epoch": 0.5630552671051113,
      "grad_norm": 0.48217591643333435,
      "learning_rate": 9.716239316239316e-05,
      "loss": 1.0202,
      "step": 3660
    },
    {
      "epoch": 0.5645936694742509,
      "grad_norm": 0.4130433201789856,
      "learning_rate": 9.682051282051282e-05,
      "loss": 1.0399,
      "step": 3670
    },
    {
      "epoch": 0.5661320718433906,
      "grad_norm": 0.4116210341453552,
      "learning_rate": 9.647863247863248e-05,
      "loss": 1.0161,
      "step": 3680
    },
    {
      "epoch": 0.5676704742125303,
      "grad_norm": 0.463244765996933,
      "learning_rate": 9.613675213675213e-05,
      "loss": 0.9964,
      "step": 3690
    },
    {
      "epoch": 0.56920887658167,
      "grad_norm": 0.48553693294525146,
      "learning_rate": 9.57948717948718e-05,
      "loss": 0.9682,
      "step": 3700
    },
    {
      "epoch": 0.5707472789508096,
      "grad_norm": 0.48212671279907227,
      "learning_rate": 9.545299145299146e-05,
      "loss": 0.9729,
      "step": 3710
    },
    {
      "epoch": 0.5722856813199493,
      "grad_norm": 0.46349358558654785,
      "learning_rate": 9.511111111111112e-05,
      "loss": 0.9736,
      "step": 3720
    },
    {
      "epoch": 0.5738240836890889,
      "grad_norm": 0.4420188069343567,
      "learning_rate": 9.476923076923078e-05,
      "loss": 1.0032,
      "step": 3730
    },
    {
      "epoch": 0.5753624860582285,
      "grad_norm": 0.47891440987586975,
      "learning_rate": 9.442735042735044e-05,
      "loss": 0.9971,
      "step": 3740
    },
    {
      "epoch": 0.5769008884273682,
      "grad_norm": 0.45031991600990295,
      "learning_rate": 9.40854700854701e-05,
      "loss": 1.0218,
      "step": 3750
    },
    {
      "epoch": 0.5784392907965078,
      "grad_norm": 0.5158865451812744,
      "learning_rate": 9.374358974358974e-05,
      "loss": 0.9688,
      "step": 3760
    },
    {
      "epoch": 0.5799776931656475,
      "grad_norm": 0.4990372657775879,
      "learning_rate": 9.340170940170941e-05,
      "loss": 0.9991,
      "step": 3770
    },
    {
      "epoch": 0.5815160955347871,
      "grad_norm": 0.4774261713027954,
      "learning_rate": 9.305982905982907e-05,
      "loss": 0.9904,
      "step": 3780
    },
    {
      "epoch": 0.5830544979039268,
      "grad_norm": 0.4630940556526184,
      "learning_rate": 9.271794871794872e-05,
      "loss": 0.9478,
      "step": 3790
    },
    {
      "epoch": 0.5845929002730664,
      "grad_norm": 0.5120342373847961,
      "learning_rate": 9.237606837606838e-05,
      "loss": 1.0655,
      "step": 3800
    },
    {
      "epoch": 0.586131302642206,
      "grad_norm": 0.46336519718170166,
      "learning_rate": 9.203418803418804e-05,
      "loss": 1.0308,
      "step": 3810
    },
    {
      "epoch": 0.5876697050113457,
      "grad_norm": 0.48117130994796753,
      "learning_rate": 9.16923076923077e-05,
      "loss": 1.0034,
      "step": 3820
    },
    {
      "epoch": 0.5892081073804853,
      "grad_norm": 0.4399054944515228,
      "learning_rate": 9.135042735042736e-05,
      "loss": 1.0637,
      "step": 3830
    },
    {
      "epoch": 0.5907465097496251,
      "grad_norm": 0.5473572611808777,
      "learning_rate": 9.100854700854701e-05,
      "loss": 1.0707,
      "step": 3840
    },
    {
      "epoch": 0.5922849121187647,
      "grad_norm": 0.58238685131073,
      "learning_rate": 9.066666666666667e-05,
      "loss": 1.0111,
      "step": 3850
    },
    {
      "epoch": 0.5938233144879043,
      "grad_norm": 0.4877139627933502,
      "learning_rate": 9.032478632478633e-05,
      "loss": 1.048,
      "step": 3860
    },
    {
      "epoch": 0.595361716857044,
      "grad_norm": 0.4608486592769623,
      "learning_rate": 8.998290598290599e-05,
      "loss": 0.9749,
      "step": 3870
    },
    {
      "epoch": 0.5969001192261836,
      "grad_norm": 0.4056682586669922,
      "learning_rate": 8.964102564102564e-05,
      "loss": 1.0087,
      "step": 3880
    },
    {
      "epoch": 0.5984385215953233,
      "grad_norm": 0.4985199570655823,
      "learning_rate": 8.92991452991453e-05,
      "loss": 1.0314,
      "step": 3890
    },
    {
      "epoch": 0.5999769239644629,
      "grad_norm": 0.4461810290813446,
      "learning_rate": 8.895726495726496e-05,
      "loss": 1.009,
      "step": 3900
    },
    {
      "epoch": 0.6015153263336026,
      "grad_norm": 0.46050533652305603,
      "learning_rate": 8.861538461538462e-05,
      "loss": 1.0012,
      "step": 3910
    },
    {
      "epoch": 0.6030537287027422,
      "grad_norm": 0.4470753073692322,
      "learning_rate": 8.827350427350429e-05,
      "loss": 0.999,
      "step": 3920
    },
    {
      "epoch": 0.6045921310718818,
      "grad_norm": 0.49706217646598816,
      "learning_rate": 8.793162393162395e-05,
      "loss": 0.9905,
      "step": 3930
    },
    {
      "epoch": 0.6061305334410215,
      "grad_norm": 0.5830263495445251,
      "learning_rate": 8.758974358974359e-05,
      "loss": 0.9904,
      "step": 3940
    },
    {
      "epoch": 0.6076689358101611,
      "grad_norm": 0.48658502101898193,
      "learning_rate": 8.724786324786325e-05,
      "loss": 1.001,
      "step": 3950
    },
    {
      "epoch": 0.6092073381793008,
      "grad_norm": 0.4904552102088928,
      "learning_rate": 8.69059829059829e-05,
      "loss": 0.9912,
      "step": 3960
    },
    {
      "epoch": 0.6107457405484404,
      "grad_norm": 0.43994224071502686,
      "learning_rate": 8.656410256410256e-05,
      "loss": 1.0496,
      "step": 3970
    },
    {
      "epoch": 0.61228414291758,
      "grad_norm": 0.4462207555770874,
      "learning_rate": 8.622222222222222e-05,
      "loss": 1.0075,
      "step": 3980
    },
    {
      "epoch": 0.6138225452867198,
      "grad_norm": 0.4386101961135864,
      "learning_rate": 8.588034188034189e-05,
      "loss": 1.0376,
      "step": 3990
    },
    {
      "epoch": 0.6153609476558594,
      "grad_norm": 0.41698208451271057,
      "learning_rate": 8.553846153846155e-05,
      "loss": 1.0427,
      "step": 4000
    },
    {
      "epoch": 0.6168993500249991,
      "grad_norm": 0.5510523319244385,
      "learning_rate": 8.519658119658121e-05,
      "loss": 1.0232,
      "step": 4010
    },
    {
      "epoch": 0.6184377523941387,
      "grad_norm": 0.49737316370010376,
      "learning_rate": 8.485470085470086e-05,
      "loss": 1.0287,
      "step": 4020
    },
    {
      "epoch": 0.6199761547632784,
      "grad_norm": 0.4774438142776489,
      "learning_rate": 8.451282051282051e-05,
      "loss": 1.0546,
      "step": 4030
    },
    {
      "epoch": 0.621514557132418,
      "grad_norm": 0.46045878529548645,
      "learning_rate": 8.417094017094017e-05,
      "loss": 0.9766,
      "step": 4040
    },
    {
      "epoch": 0.6230529595015576,
      "grad_norm": 0.5054082870483398,
      "learning_rate": 8.382905982905982e-05,
      "loss": 0.951,
      "step": 4050
    },
    {
      "epoch": 0.6245913618706973,
      "grad_norm": 0.4494772255420685,
      "learning_rate": 8.34871794871795e-05,
      "loss": 0.9472,
      "step": 4060
    },
    {
      "epoch": 0.6261297642398369,
      "grad_norm": 0.46364492177963257,
      "learning_rate": 8.314529914529915e-05,
      "loss": 0.9799,
      "step": 4070
    },
    {
      "epoch": 0.6276681666089766,
      "grad_norm": 0.45751115679740906,
      "learning_rate": 8.280341880341881e-05,
      "loss": 1.0296,
      "step": 4080
    },
    {
      "epoch": 0.6292065689781162,
      "grad_norm": 0.49335426092147827,
      "learning_rate": 8.246153846153847e-05,
      "loss": 1.0391,
      "step": 4090
    },
    {
      "epoch": 0.6307449713472558,
      "grad_norm": 0.44529882073402405,
      "learning_rate": 8.211965811965813e-05,
      "loss": 1.0302,
      "step": 4100
    },
    {
      "epoch": 0.6322833737163955,
      "grad_norm": 0.5169335007667542,
      "learning_rate": 8.177777777777778e-05,
      "loss": 1.0235,
      "step": 4110
    },
    {
      "epoch": 0.6338217760855351,
      "grad_norm": 0.45856523513793945,
      "learning_rate": 8.143589743589743e-05,
      "loss": 0.9441,
      "step": 4120
    },
    {
      "epoch": 0.6353601784546749,
      "grad_norm": 0.5385552644729614,
      "learning_rate": 8.10940170940171e-05,
      "loss": 1.0214,
      "step": 4130
    },
    {
      "epoch": 0.6368985808238145,
      "grad_norm": 0.49311205744743347,
      "learning_rate": 8.075213675213676e-05,
      "loss": 0.9841,
      "step": 4140
    },
    {
      "epoch": 0.6384369831929542,
      "grad_norm": 0.5630927085876465,
      "learning_rate": 8.041025641025641e-05,
      "loss": 1.0627,
      "step": 4150
    },
    {
      "epoch": 0.6399753855620938,
      "grad_norm": 0.43924352526664734,
      "learning_rate": 8.006837606837607e-05,
      "loss": 0.9903,
      "step": 4160
    },
    {
      "epoch": 0.6415137879312334,
      "grad_norm": 0.4483819305896759,
      "learning_rate": 7.972649572649573e-05,
      "loss": 1.0277,
      "step": 4170
    },
    {
      "epoch": 0.6430521903003731,
      "grad_norm": 0.4714765250682831,
      "learning_rate": 7.938461538461539e-05,
      "loss": 1.041,
      "step": 4180
    },
    {
      "epoch": 0.6445905926695127,
      "grad_norm": 0.4829670488834381,
      "learning_rate": 7.904273504273505e-05,
      "loss": 0.9611,
      "step": 4190
    },
    {
      "epoch": 0.6461289950386524,
      "grad_norm": 0.4727685749530792,
      "learning_rate": 7.87008547008547e-05,
      "loss": 0.9711,
      "step": 4200
    },
    {
      "epoch": 0.647667397407792,
      "grad_norm": 0.45242372155189514,
      "learning_rate": 7.835897435897436e-05,
      "loss": 0.999,
      "step": 4210
    },
    {
      "epoch": 0.6492057997769316,
      "grad_norm": 0.4948423206806183,
      "learning_rate": 7.801709401709402e-05,
      "loss": 1.0317,
      "step": 4220
    },
    {
      "epoch": 0.6507442021460713,
      "grad_norm": 0.49889814853668213,
      "learning_rate": 7.767521367521368e-05,
      "loss": 0.9801,
      "step": 4230
    },
    {
      "epoch": 0.6522826045152109,
      "grad_norm": 0.518099844455719,
      "learning_rate": 7.733333333333333e-05,
      "loss": 1.0094,
      "step": 4240
    },
    {
      "epoch": 0.6538210068843506,
      "grad_norm": 0.49984046816825867,
      "learning_rate": 7.699145299145299e-05,
      "loss": 1.0631,
      "step": 4250
    },
    {
      "epoch": 0.6553594092534902,
      "grad_norm": 0.5042322874069214,
      "learning_rate": 7.664957264957265e-05,
      "loss": 0.9769,
      "step": 4260
    },
    {
      "epoch": 0.65689781162263,
      "grad_norm": 0.43659842014312744,
      "learning_rate": 7.630769230769231e-05,
      "loss": 0.9597,
      "step": 4270
    },
    {
      "epoch": 0.6584362139917695,
      "grad_norm": 0.5034261345863342,
      "learning_rate": 7.596581196581198e-05,
      "loss": 1.0501,
      "step": 4280
    },
    {
      "epoch": 0.6599746163609091,
      "grad_norm": 0.5292542576789856,
      "learning_rate": 7.562393162393164e-05,
      "loss": 0.978,
      "step": 4290
    },
    {
      "epoch": 0.6615130187300489,
      "grad_norm": 0.5163887739181519,
      "learning_rate": 7.528205128205128e-05,
      "loss": 1.0168,
      "step": 4300
    },
    {
      "epoch": 0.6630514210991885,
      "grad_norm": 0.43550893664360046,
      "learning_rate": 7.494017094017094e-05,
      "loss": 0.9807,
      "step": 4310
    },
    {
      "epoch": 0.6645898234683282,
      "grad_norm": 0.5076808333396912,
      "learning_rate": 7.45982905982906e-05,
      "loss": 0.9897,
      "step": 4320
    },
    {
      "epoch": 0.6661282258374678,
      "grad_norm": 0.49274855852127075,
      "learning_rate": 7.425641025641025e-05,
      "loss": 1.0322,
      "step": 4330
    },
    {
      "epoch": 0.6676666282066075,
      "grad_norm": 0.4688161313533783,
      "learning_rate": 7.391452991452992e-05,
      "loss": 0.9099,
      "step": 4340
    },
    {
      "epoch": 0.6692050305757471,
      "grad_norm": 0.5523702502250671,
      "learning_rate": 7.357264957264958e-05,
      "loss": 1.0192,
      "step": 4350
    },
    {
      "epoch": 0.6707434329448867,
      "grad_norm": 0.46958303451538086,
      "learning_rate": 7.323076923076924e-05,
      "loss": 1.0505,
      "step": 4360
    },
    {
      "epoch": 0.6722818353140264,
      "grad_norm": 0.4783111810684204,
      "learning_rate": 7.28888888888889e-05,
      "loss": 0.9685,
      "step": 4370
    },
    {
      "epoch": 0.673820237683166,
      "grad_norm": 0.5339074730873108,
      "learning_rate": 7.254700854700855e-05,
      "loss": 1.0064,
      "step": 4380
    },
    {
      "epoch": 0.6753586400523057,
      "grad_norm": 0.4265858829021454,
      "learning_rate": 7.220512820512821e-05,
      "loss": 1.0077,
      "step": 4390
    },
    {
      "epoch": 0.6768970424214453,
      "grad_norm": 0.5210985541343689,
      "learning_rate": 7.186324786324786e-05,
      "loss": 1.0384,
      "step": 4400
    },
    {
      "epoch": 0.6784354447905849,
      "grad_norm": 0.5227885842323303,
      "learning_rate": 7.152136752136753e-05,
      "loss": 1.0088,
      "step": 4410
    },
    {
      "epoch": 0.6799738471597246,
      "grad_norm": 0.500190019607544,
      "learning_rate": 7.117948717948719e-05,
      "loss": 1.0867,
      "step": 4420
    },
    {
      "epoch": 0.6815122495288642,
      "grad_norm": 0.5075272917747498,
      "learning_rate": 7.083760683760684e-05,
      "loss": 1.0057,
      "step": 4430
    },
    {
      "epoch": 0.683050651898004,
      "grad_norm": 0.4295794367790222,
      "learning_rate": 7.04957264957265e-05,
      "loss": 0.9242,
      "step": 4440
    },
    {
      "epoch": 0.6845890542671436,
      "grad_norm": 0.4278978407382965,
      "learning_rate": 7.015384615384616e-05,
      "loss": 0.9768,
      "step": 4450
    },
    {
      "epoch": 0.6861274566362833,
      "grad_norm": 0.4457150995731354,
      "learning_rate": 6.981196581196582e-05,
      "loss": 0.9245,
      "step": 4460
    },
    {
      "epoch": 0.6876658590054229,
      "grad_norm": 0.5517175793647766,
      "learning_rate": 6.947008547008547e-05,
      "loss": 1.0103,
      "step": 4470
    },
    {
      "epoch": 0.6892042613745625,
      "grad_norm": 0.5487529039382935,
      "learning_rate": 6.912820512820513e-05,
      "loss": 0.9893,
      "step": 4480
    },
    {
      "epoch": 0.6907426637437022,
      "grad_norm": 0.4264967143535614,
      "learning_rate": 6.878632478632479e-05,
      "loss": 1.0154,
      "step": 4490
    },
    {
      "epoch": 0.6922810661128418,
      "grad_norm": 0.4651111662387848,
      "learning_rate": 6.844444444444445e-05,
      "loss": 1.0129,
      "step": 4500
    },
    {
      "epoch": 0.6938194684819815,
      "grad_norm": 0.5527991652488708,
      "learning_rate": 6.81025641025641e-05,
      "loss": 1.0091,
      "step": 4510
    },
    {
      "epoch": 0.6953578708511211,
      "grad_norm": 0.48918816447257996,
      "learning_rate": 6.776068376068376e-05,
      "loss": 1.033,
      "step": 4520
    },
    {
      "epoch": 0.6968962732202607,
      "grad_norm": 0.4812573790550232,
      "learning_rate": 6.741880341880342e-05,
      "loss": 0.9554,
      "step": 4530
    },
    {
      "epoch": 0.6984346755894004,
      "grad_norm": 0.5144619941711426,
      "learning_rate": 6.707692307692308e-05,
      "loss": 0.9993,
      "step": 4540
    },
    {
      "epoch": 0.69997307795854,
      "grad_norm": 0.47508326172828674,
      "learning_rate": 6.673504273504274e-05,
      "loss": 0.997,
      "step": 4550
    },
    {
      "epoch": 0.7015114803276797,
      "grad_norm": 0.47432783246040344,
      "learning_rate": 6.639316239316241e-05,
      "loss": 1.0175,
      "step": 4560
    },
    {
      "epoch": 0.7030498826968193,
      "grad_norm": 0.44962775707244873,
      "learning_rate": 6.605128205128206e-05,
      "loss": 1.0483,
      "step": 4570
    },
    {
      "epoch": 0.704588285065959,
      "grad_norm": 0.4540018141269684,
      "learning_rate": 6.570940170940171e-05,
      "loss": 0.9847,
      "step": 4580
    },
    {
      "epoch": 0.7061266874350987,
      "grad_norm": 0.48827221989631653,
      "learning_rate": 6.536752136752137e-05,
      "loss": 1.049,
      "step": 4590
    },
    {
      "epoch": 0.7076650898042383,
      "grad_norm": 0.4784558415412903,
      "learning_rate": 6.502564102564102e-05,
      "loss": 1.0098,
      "step": 4600
    },
    {
      "epoch": 0.709203492173378,
      "grad_norm": 0.5173927545547485,
      "learning_rate": 6.468376068376068e-05,
      "loss": 1.0355,
      "step": 4610
    },
    {
      "epoch": 0.7107418945425176,
      "grad_norm": 0.44758179783821106,
      "learning_rate": 6.434188034188034e-05,
      "loss": 1.0528,
      "step": 4620
    },
    {
      "epoch": 0.7122802969116573,
      "grad_norm": 0.5385869741439819,
      "learning_rate": 6.400000000000001e-05,
      "loss": 0.9726,
      "step": 4630
    },
    {
      "epoch": 0.7138186992807969,
      "grad_norm": 0.47865593433380127,
      "learning_rate": 6.365811965811967e-05,
      "loss": 1.0273,
      "step": 4640
    },
    {
      "epoch": 0.7153571016499365,
      "grad_norm": 0.5220633149147034,
      "learning_rate": 6.331623931623933e-05,
      "loss": 1.0093,
      "step": 4650
    },
    {
      "epoch": 0.7168955040190762,
      "grad_norm": 0.5040693283081055,
      "learning_rate": 6.297435897435898e-05,
      "loss": 0.9637,
      "step": 4660
    },
    {
      "epoch": 0.7184339063882158,
      "grad_norm": 0.5053882598876953,
      "learning_rate": 6.263247863247863e-05,
      "loss": 0.9499,
      "step": 4670
    },
    {
      "epoch": 0.7199723087573555,
      "grad_norm": 0.5113081932067871,
      "learning_rate": 6.229059829059829e-05,
      "loss": 0.9887,
      "step": 4680
    },
    {
      "epoch": 0.7215107111264951,
      "grad_norm": 0.4200296401977539,
      "learning_rate": 6.194871794871794e-05,
      "loss": 1.0542,
      "step": 4690
    },
    {
      "epoch": 0.7230491134956348,
      "grad_norm": 0.4608495235443115,
      "learning_rate": 6.160683760683761e-05,
      "loss": 0.9392,
      "step": 4700
    },
    {
      "epoch": 0.7245875158647744,
      "grad_norm": 0.4401058554649353,
      "learning_rate": 6.126495726495727e-05,
      "loss": 1.0036,
      "step": 4710
    },
    {
      "epoch": 0.726125918233914,
      "grad_norm": 0.46448248624801636,
      "learning_rate": 6.092307692307693e-05,
      "loss": 1.0071,
      "step": 4720
    },
    {
      "epoch": 0.7276643206030537,
      "grad_norm": 0.4454417824745178,
      "learning_rate": 6.058119658119659e-05,
      "loss": 0.9695,
      "step": 4730
    },
    {
      "epoch": 0.7292027229721934,
      "grad_norm": 0.5152463912963867,
      "learning_rate": 6.0239316239316245e-05,
      "loss": 0.9866,
      "step": 4740
    },
    {
      "epoch": 0.7307411253413331,
      "grad_norm": 0.45089805126190186,
      "learning_rate": 5.98974358974359e-05,
      "loss": 1.0199,
      "step": 4750
    },
    {
      "epoch": 0.7322795277104727,
      "grad_norm": 0.4582236409187317,
      "learning_rate": 5.9555555555555554e-05,
      "loss": 0.9765,
      "step": 4760
    },
    {
      "epoch": 0.7338179300796123,
      "grad_norm": 0.42687633633613586,
      "learning_rate": 5.921367521367521e-05,
      "loss": 1.0317,
      "step": 4770
    },
    {
      "epoch": 0.735356332448752,
      "grad_norm": 0.3997184634208679,
      "learning_rate": 5.887179487179487e-05,
      "loss": 1.004,
      "step": 4780
    },
    {
      "epoch": 0.7368947348178916,
      "grad_norm": 0.5636053085327148,
      "learning_rate": 5.8529914529914534e-05,
      "loss": 0.9908,
      "step": 4790
    },
    {
      "epoch": 0.7384331371870313,
      "grad_norm": 0.46361154317855835,
      "learning_rate": 5.818803418803419e-05,
      "loss": 1.0117,
      "step": 4800
    },
    {
      "epoch": 0.7399715395561709,
      "grad_norm": 0.44611918926239014,
      "learning_rate": 5.784615384615385e-05,
      "loss": 1.0576,
      "step": 4810
    },
    {
      "epoch": 0.7415099419253106,
      "grad_norm": 0.5063352584838867,
      "learning_rate": 5.750427350427351e-05,
      "loss": 1.0524,
      "step": 4820
    },
    {
      "epoch": 0.7430483442944502,
      "grad_norm": 0.46819621324539185,
      "learning_rate": 5.716239316239317e-05,
      "loss": 1.0374,
      "step": 4830
    },
    {
      "epoch": 0.7445867466635898,
      "grad_norm": 0.4321654736995697,
      "learning_rate": 5.682051282051283e-05,
      "loss": 0.9377,
      "step": 4840
    },
    {
      "epoch": 0.7461251490327295,
      "grad_norm": 0.48679694533348083,
      "learning_rate": 5.647863247863247e-05,
      "loss": 1.0317,
      "step": 4850
    },
    {
      "epoch": 0.7476635514018691,
      "grad_norm": 0.35704225301742554,
      "learning_rate": 5.613675213675214e-05,
      "loss": 0.8986,
      "step": 4860
    },
    {
      "epoch": 0.7492019537710088,
      "grad_norm": 0.5096700191497803,
      "learning_rate": 5.5794871794871795e-05,
      "loss": 0.9293,
      "step": 4870
    },
    {
      "epoch": 0.7507403561401484,
      "grad_norm": 0.43335118889808655,
      "learning_rate": 5.545299145299145e-05,
      "loss": 1.0125,
      "step": 4880
    },
    {
      "epoch": 0.7522787585092882,
      "grad_norm": 0.4446438252925873,
      "learning_rate": 5.511111111111111e-05,
      "loss": 1.0376,
      "step": 4890
    },
    {
      "epoch": 0.7538171608784278,
      "grad_norm": 0.5559123158454895,
      "learning_rate": 5.4769230769230775e-05,
      "loss": 1.0157,
      "step": 4900
    },
    {
      "epoch": 0.7553555632475674,
      "grad_norm": 0.5112246870994568,
      "learning_rate": 5.442735042735043e-05,
      "loss": 0.9855,
      "step": 4910
    },
    {
      "epoch": 0.7568939656167071,
      "grad_norm": 0.4763055443763733,
      "learning_rate": 5.408547008547009e-05,
      "loss": 1.0197,
      "step": 4920
    },
    {
      "epoch": 0.7584323679858467,
      "grad_norm": 0.46983465552330017,
      "learning_rate": 5.374358974358975e-05,
      "loss": 1.0516,
      "step": 4930
    },
    {
      "epoch": 0.7599707703549864,
      "grad_norm": 0.453414648771286,
      "learning_rate": 5.34017094017094e-05,
      "loss": 1.0288,
      "step": 4940
    },
    {
      "epoch": 0.761509172724126,
      "grad_norm": 0.4713113605976105,
      "learning_rate": 5.305982905982906e-05,
      "loss": 0.988,
      "step": 4950
    },
    {
      "epoch": 0.7630475750932656,
      "grad_norm": 0.4446909427642822,
      "learning_rate": 5.2717948717948714e-05,
      "loss": 0.9225,
      "step": 4960
    },
    {
      "epoch": 0.7645859774624053,
      "grad_norm": 0.525851309299469,
      "learning_rate": 5.237606837606838e-05,
      "loss": 1.0974,
      "step": 4970
    },
    {
      "epoch": 0.7661243798315449,
      "grad_norm": 0.5993674993515015,
      "learning_rate": 5.2034188034188036e-05,
      "loss": 0.9675,
      "step": 4980
    },
    {
      "epoch": 0.7676627822006846,
      "grad_norm": 0.5457700490951538,
      "learning_rate": 5.1692307692307694e-05,
      "loss": 1.0181,
      "step": 4990
    },
    {
      "epoch": 0.7692011845698242,
      "grad_norm": 0.4815864562988281,
      "learning_rate": 5.135042735042735e-05,
      "loss": 1.0012,
      "step": 5000
    },
    {
      "epoch": 0.7707395869389639,
      "grad_norm": 0.4857502579689026,
      "learning_rate": 5.1008547008547016e-05,
      "loss": 0.9965,
      "step": 5010
    },
    {
      "epoch": 0.7722779893081035,
      "grad_norm": 0.495656281709671,
      "learning_rate": 5.0666666666666674e-05,
      "loss": 0.9767,
      "step": 5020
    },
    {
      "epoch": 0.7738163916772431,
      "grad_norm": 0.5600410103797913,
      "learning_rate": 5.032478632478633e-05,
      "loss": 1.0298,
      "step": 5030
    },
    {
      "epoch": 0.7753547940463829,
      "grad_norm": 0.46019330620765686,
      "learning_rate": 4.998290598290599e-05,
      "loss": 1.0442,
      "step": 5040
    },
    {
      "epoch": 0.7768931964155225,
      "grad_norm": 0.4856325685977936,
      "learning_rate": 4.964102564102565e-05,
      "loss": 0.9626,
      "step": 5050
    },
    {
      "epoch": 0.7784315987846622,
      "grad_norm": 0.4469749331474304,
      "learning_rate": 4.92991452991453e-05,
      "loss": 1.0225,
      "step": 5060
    },
    {
      "epoch": 0.7799700011538018,
      "grad_norm": 0.5330506563186646,
      "learning_rate": 4.8957264957264956e-05,
      "loss": 0.9577,
      "step": 5070
    },
    {
      "epoch": 0.7815084035229414,
      "grad_norm": 0.46761974692344666,
      "learning_rate": 4.861538461538462e-05,
      "loss": 1.0424,
      "step": 5080
    },
    {
      "epoch": 0.7830468058920811,
      "grad_norm": 0.44334667921066284,
      "learning_rate": 4.827350427350428e-05,
      "loss": 1.0478,
      "step": 5090
    },
    {
      "epoch": 0.7845852082612207,
      "grad_norm": 0.51145339012146,
      "learning_rate": 4.793162393162393e-05,
      "loss": 0.9636,
      "step": 5100
    },
    {
      "epoch": 0.7861236106303604,
      "grad_norm": 0.42216992378234863,
      "learning_rate": 4.758974358974359e-05,
      "loss": 1.053,
      "step": 5110
    },
    {
      "epoch": 0.7876620129995,
      "grad_norm": 0.47059905529022217,
      "learning_rate": 4.724786324786325e-05,
      "loss": 1.0639,
      "step": 5120
    },
    {
      "epoch": 0.7892004153686397,
      "grad_norm": 0.4768812358379364,
      "learning_rate": 4.690598290598291e-05,
      "loss": 1.0209,
      "step": 5130
    },
    {
      "epoch": 0.7907388177377793,
      "grad_norm": 0.4527531564235687,
      "learning_rate": 4.6564102564102566e-05,
      "loss": 1.0049,
      "step": 5140
    },
    {
      "epoch": 0.7922772201069189,
      "grad_norm": 0.6095728278160095,
      "learning_rate": 4.6222222222222224e-05,
      "loss": 0.9716,
      "step": 5150
    },
    {
      "epoch": 0.7938156224760586,
      "grad_norm": 0.5003243088722229,
      "learning_rate": 4.588034188034188e-05,
      "loss": 1.0162,
      "step": 5160
    },
    {
      "epoch": 0.7953540248451982,
      "grad_norm": 0.4919436275959015,
      "learning_rate": 4.553846153846154e-05,
      "loss": 1.0385,
      "step": 5170
    },
    {
      "epoch": 0.796892427214338,
      "grad_norm": 0.4470413327217102,
      "learning_rate": 4.51965811965812e-05,
      "loss": 0.9882,
      "step": 5180
    },
    {
      "epoch": 0.7984308295834776,
      "grad_norm": 0.41498279571533203,
      "learning_rate": 4.485470085470086e-05,
      "loss": 0.9741,
      "step": 5190
    },
    {
      "epoch": 0.7999692319526172,
      "grad_norm": 0.5279072523117065,
      "learning_rate": 4.451282051282051e-05,
      "loss": 1.0287,
      "step": 5200
    },
    {
      "epoch": 0.8015076343217569,
      "grad_norm": 0.4863632321357727,
      "learning_rate": 4.417094017094017e-05,
      "loss": 1.0024,
      "step": 5210
    },
    {
      "epoch": 0.8030460366908965,
      "grad_norm": 0.5177703499794006,
      "learning_rate": 4.3829059829059834e-05,
      "loss": 0.9819,
      "step": 5220
    },
    {
      "epoch": 0.8045844390600362,
      "grad_norm": 0.4076552093029022,
      "learning_rate": 4.348717948717949e-05,
      "loss": 1.0095,
      "step": 5230
    },
    {
      "epoch": 0.8061228414291758,
      "grad_norm": 0.4968482255935669,
      "learning_rate": 4.314529914529914e-05,
      "loss": 0.9606,
      "step": 5240
    },
    {
      "epoch": 0.8076612437983155,
      "grad_norm": 0.4862675964832306,
      "learning_rate": 4.280341880341881e-05,
      "loss": 0.9872,
      "step": 5250
    },
    {
      "epoch": 0.8091996461674551,
      "grad_norm": 0.5125622153282166,
      "learning_rate": 4.2461538461538465e-05,
      "loss": 1.0202,
      "step": 5260
    },
    {
      "epoch": 0.8107380485365947,
      "grad_norm": 0.5356016755104065,
      "learning_rate": 4.211965811965812e-05,
      "loss": 0.9685,
      "step": 5270
    },
    {
      "epoch": 0.8122764509057344,
      "grad_norm": 0.5434045195579529,
      "learning_rate": 4.177777777777778e-05,
      "loss": 1.0544,
      "step": 5280
    },
    {
      "epoch": 0.813814853274874,
      "grad_norm": 0.4273916482925415,
      "learning_rate": 4.143589743589744e-05,
      "loss": 0.9463,
      "step": 5290
    },
    {
      "epoch": 0.8153532556440137,
      "grad_norm": 0.4868357479572296,
      "learning_rate": 4.1094017094017096e-05,
      "loss": 1.0374,
      "step": 5300
    },
    {
      "epoch": 0.8168916580131533,
      "grad_norm": 0.5125134587287903,
      "learning_rate": 4.0752136752136754e-05,
      "loss": 0.9953,
      "step": 5310
    },
    {
      "epoch": 0.8184300603822929,
      "grad_norm": 0.4431968033313751,
      "learning_rate": 4.041025641025641e-05,
      "loss": 1.009,
      "step": 5320
    },
    {
      "epoch": 0.8199684627514326,
      "grad_norm": 0.5009307861328125,
      "learning_rate": 4.006837606837607e-05,
      "loss": 1.0081,
      "step": 5330
    },
    {
      "epoch": 0.8215068651205722,
      "grad_norm": 0.4812498390674591,
      "learning_rate": 3.972649572649573e-05,
      "loss": 1.0652,
      "step": 5340
    },
    {
      "epoch": 0.823045267489712,
      "grad_norm": 0.4600459337234497,
      "learning_rate": 3.9384615384615384e-05,
      "loss": 0.9658,
      "step": 5350
    },
    {
      "epoch": 0.8245836698588516,
      "grad_norm": 0.4735737144947052,
      "learning_rate": 3.904273504273505e-05,
      "loss": 0.9891,
      "step": 5360
    },
    {
      "epoch": 0.8261220722279913,
      "grad_norm": 0.4351332187652588,
      "learning_rate": 3.8700854700854707e-05,
      "loss": 0.989,
      "step": 5370
    },
    {
      "epoch": 0.8276604745971309,
      "grad_norm": 0.49648937582969666,
      "learning_rate": 3.835897435897436e-05,
      "loss": 1.0071,
      "step": 5380
    },
    {
      "epoch": 0.8291988769662705,
      "grad_norm": 0.5284995436668396,
      "learning_rate": 3.8017094017094015e-05,
      "loss": 1.0574,
      "step": 5390
    },
    {
      "epoch": 0.8307372793354102,
      "grad_norm": 0.4532010853290558,
      "learning_rate": 3.767521367521368e-05,
      "loss": 1.0297,
      "step": 5400
    },
    {
      "epoch": 0.8322756817045498,
      "grad_norm": 0.479759156703949,
      "learning_rate": 3.733333333333334e-05,
      "loss": 0.9375,
      "step": 5410
    },
    {
      "epoch": 0.8338140840736895,
      "grad_norm": 0.5223960280418396,
      "learning_rate": 3.699145299145299e-05,
      "loss": 1.0044,
      "step": 5420
    },
    {
      "epoch": 0.8353524864428291,
      "grad_norm": 0.45373937487602234,
      "learning_rate": 3.664957264957265e-05,
      "loss": 1.0141,
      "step": 5430
    },
    {
      "epoch": 0.8368908888119688,
      "grad_norm": 0.47863417863845825,
      "learning_rate": 3.630769230769231e-05,
      "loss": 1.0411,
      "step": 5440
    },
    {
      "epoch": 0.8384292911811084,
      "grad_norm": 0.4798305630683899,
      "learning_rate": 3.596581196581197e-05,
      "loss": 1.0091,
      "step": 5450
    },
    {
      "epoch": 0.839967693550248,
      "grad_norm": 0.42831945419311523,
      "learning_rate": 3.5623931623931626e-05,
      "loss": 1.0171,
      "step": 5460
    },
    {
      "epoch": 0.8415060959193877,
      "grad_norm": 0.5154745578765869,
      "learning_rate": 3.5282051282051283e-05,
      "loss": 1.0249,
      "step": 5470
    },
    {
      "epoch": 0.8430444982885273,
      "grad_norm": 0.43537846207618713,
      "learning_rate": 3.494017094017094e-05,
      "loss": 0.9876,
      "step": 5480
    },
    {
      "epoch": 0.8445829006576671,
      "grad_norm": 0.5299913287162781,
      "learning_rate": 3.45982905982906e-05,
      "loss": 1.0002,
      "step": 5490
    },
    {
      "epoch": 0.8461213030268067,
      "grad_norm": 0.4388516843318939,
      "learning_rate": 3.4256410256410256e-05,
      "loss": 0.9565,
      "step": 5500
    },
    {
      "epoch": 0.8476597053959463,
      "grad_norm": 0.4678807556629181,
      "learning_rate": 3.391452991452992e-05,
      "loss": 1.0081,
      "step": 5510
    },
    {
      "epoch": 0.849198107765086,
      "grad_norm": 0.45344278216362,
      "learning_rate": 3.357264957264957e-05,
      "loss": 0.9639,
      "step": 5520
    },
    {
      "epoch": 0.8507365101342256,
      "grad_norm": 0.5456284284591675,
      "learning_rate": 3.323076923076923e-05,
      "loss": 1.0172,
      "step": 5530
    },
    {
      "epoch": 0.8522749125033653,
      "grad_norm": 0.5731152296066284,
      "learning_rate": 3.2888888888888894e-05,
      "loss": 1.0328,
      "step": 5540
    },
    {
      "epoch": 0.8538133148725049,
      "grad_norm": 0.4936108887195587,
      "learning_rate": 3.254700854700855e-05,
      "loss": 0.9412,
      "step": 5550
    },
    {
      "epoch": 0.8553517172416446,
      "grad_norm": 0.48347049951553345,
      "learning_rate": 3.22051282051282e-05,
      "loss": 0.9637,
      "step": 5560
    },
    {
      "epoch": 0.8568901196107842,
      "grad_norm": 0.4742794334888458,
      "learning_rate": 3.186324786324787e-05,
      "loss": 0.918,
      "step": 5570
    },
    {
      "epoch": 0.8584285219799238,
      "grad_norm": 0.4892593324184418,
      "learning_rate": 3.1521367521367525e-05,
      "loss": 0.9501,
      "step": 5580
    },
    {
      "epoch": 0.8599669243490635,
      "grad_norm": 0.41517090797424316,
      "learning_rate": 3.117948717948718e-05,
      "loss": 0.9569,
      "step": 5590
    },
    {
      "epoch": 0.8615053267182031,
      "grad_norm": 0.40100255608558655,
      "learning_rate": 3.083760683760684e-05,
      "loss": 1.0055,
      "step": 5600
    },
    {
      "epoch": 0.8630437290873428,
      "grad_norm": 0.4680721163749695,
      "learning_rate": 3.0495726495726494e-05,
      "loss": 1.0378,
      "step": 5610
    },
    {
      "epoch": 0.8645821314564824,
      "grad_norm": 0.43888726830482483,
      "learning_rate": 3.0153846153846155e-05,
      "loss": 0.9842,
      "step": 5620
    },
    {
      "epoch": 0.866120533825622,
      "grad_norm": 0.47475552558898926,
      "learning_rate": 2.9811965811965813e-05,
      "loss": 1.0299,
      "step": 5630
    },
    {
      "epoch": 0.8676589361947618,
      "grad_norm": 0.5192009210586548,
      "learning_rate": 2.9470085470085474e-05,
      "loss": 0.9288,
      "step": 5640
    },
    {
      "epoch": 0.8691973385639014,
      "grad_norm": 0.43495574593544006,
      "learning_rate": 2.912820512820513e-05,
      "loss": 0.98,
      "step": 5650
    },
    {
      "epoch": 0.8707357409330411,
      "grad_norm": 0.411559522151947,
      "learning_rate": 2.8786324786324786e-05,
      "loss": 0.9513,
      "step": 5660
    },
    {
      "epoch": 0.8722741433021807,
      "grad_norm": 0.4398134648799896,
      "learning_rate": 2.8444444444444447e-05,
      "loss": 1.0555,
      "step": 5670
    },
    {
      "epoch": 0.8738125456713204,
      "grad_norm": 0.4502900242805481,
      "learning_rate": 2.8102564102564105e-05,
      "loss": 0.9577,
      "step": 5680
    },
    {
      "epoch": 0.87535094804046,
      "grad_norm": 0.5196011662483215,
      "learning_rate": 2.7760683760683766e-05,
      "loss": 1.0233,
      "step": 5690
    },
    {
      "epoch": 0.8768893504095996,
      "grad_norm": 0.44745805859565735,
      "learning_rate": 2.7418803418803417e-05,
      "loss": 0.9928,
      "step": 5700
    },
    {
      "epoch": 0.8784277527787393,
      "grad_norm": 0.45543819665908813,
      "learning_rate": 2.7076923076923078e-05,
      "loss": 1.0048,
      "step": 5710
    },
    {
      "epoch": 0.8799661551478789,
      "grad_norm": 0.47366246581077576,
      "learning_rate": 2.6735042735042736e-05,
      "loss": 0.9453,
      "step": 5720
    },
    {
      "epoch": 0.8815045575170186,
      "grad_norm": 0.4228874146938324,
      "learning_rate": 2.6393162393162397e-05,
      "loss": 1.0907,
      "step": 5730
    },
    {
      "epoch": 0.8830429598861582,
      "grad_norm": 0.5459772348403931,
      "learning_rate": 2.605128205128205e-05,
      "loss": 0.9651,
      "step": 5740
    },
    {
      "epoch": 0.8845813622552978,
      "grad_norm": 0.44243931770324707,
      "learning_rate": 2.570940170940171e-05,
      "loss": 0.9484,
      "step": 5750
    },
    {
      "epoch": 0.8861197646244375,
      "grad_norm": 0.5058144330978394,
      "learning_rate": 2.536752136752137e-05,
      "loss": 0.9533,
      "step": 5760
    },
    {
      "epoch": 0.8876581669935771,
      "grad_norm": 0.5460396409034729,
      "learning_rate": 2.5025641025641028e-05,
      "loss": 0.8854,
      "step": 5770
    },
    {
      "epoch": 0.8891965693627168,
      "grad_norm": 0.531411349773407,
      "learning_rate": 2.4683760683760685e-05,
      "loss": 0.9999,
      "step": 5780
    },
    {
      "epoch": 0.8907349717318565,
      "grad_norm": 0.5206071138381958,
      "learning_rate": 2.4341880341880343e-05,
      "loss": 0.9855,
      "step": 5790
    },
    {
      "epoch": 0.8922733741009962,
      "grad_norm": 0.44113942980766296,
      "learning_rate": 2.4e-05,
      "loss": 0.9562,
      "step": 5800
    },
    {
      "epoch": 0.8938117764701358,
      "grad_norm": 0.4853367805480957,
      "learning_rate": 2.3658119658119658e-05,
      "loss": 1.0302,
      "step": 5810
    },
    {
      "epoch": 0.8953501788392754,
      "grad_norm": 0.49491745233535767,
      "learning_rate": 2.3316239316239316e-05,
      "loss": 1.0036,
      "step": 5820
    },
    {
      "epoch": 0.8968885812084151,
      "grad_norm": 0.4416457712650299,
      "learning_rate": 2.2974358974358977e-05,
      "loss": 0.9866,
      "step": 5830
    },
    {
      "epoch": 0.8984269835775547,
      "grad_norm": 0.4761943519115448,
      "learning_rate": 2.2632478632478635e-05,
      "loss": 1.0144,
      "step": 5840
    },
    {
      "epoch": 0.8999653859466944,
      "grad_norm": 0.5124083757400513,
      "learning_rate": 2.2290598290598292e-05,
      "loss": 0.9696,
      "step": 5850
    },
    {
      "epoch": 0.901503788315834,
      "grad_norm": 0.5461603403091431,
      "learning_rate": 2.194871794871795e-05,
      "loss": 1.0441,
      "step": 5860
    },
    {
      "epoch": 0.9030421906849736,
      "grad_norm": 0.46501144766807556,
      "learning_rate": 2.1606837606837608e-05,
      "loss": 0.9643,
      "step": 5870
    },
    {
      "epoch": 0.9045805930541133,
      "grad_norm": 0.4978991150856018,
      "learning_rate": 2.1264957264957265e-05,
      "loss": 0.9729,
      "step": 5880
    },
    {
      "epoch": 0.9061189954232529,
      "grad_norm": 0.44682934880256653,
      "learning_rate": 2.0923076923076923e-05,
      "loss": 1.0019,
      "step": 5890
    },
    {
      "epoch": 0.9076573977923926,
      "grad_norm": 0.4804850220680237,
      "learning_rate": 2.058119658119658e-05,
      "loss": 0.9868,
      "step": 5900
    },
    {
      "epoch": 0.9091958001615322,
      "grad_norm": 0.5217598676681519,
      "learning_rate": 2.0239316239316242e-05,
      "loss": 0.9441,
      "step": 5910
    },
    {
      "epoch": 0.9107342025306719,
      "grad_norm": 0.4299830198287964,
      "learning_rate": 1.98974358974359e-05,
      "loss": 1.0661,
      "step": 5920
    },
    {
      "epoch": 0.9122726048998115,
      "grad_norm": 0.4497761130332947,
      "learning_rate": 1.9555555555555557e-05,
      "loss": 1.0085,
      "step": 5930
    },
    {
      "epoch": 0.9138110072689511,
      "grad_norm": 0.4901949465274811,
      "learning_rate": 1.9213675213675215e-05,
      "loss": 1.03,
      "step": 5940
    },
    {
      "epoch": 0.9153494096380909,
      "grad_norm": 0.5285939574241638,
      "learning_rate": 1.8871794871794873e-05,
      "loss": 0.9811,
      "step": 5950
    },
    {
      "epoch": 0.9168878120072305,
      "grad_norm": 0.4680168032646179,
      "learning_rate": 1.852991452991453e-05,
      "loss": 1.0508,
      "step": 5960
    },
    {
      "epoch": 0.9184262143763702,
      "grad_norm": 0.46622374653816223,
      "learning_rate": 1.8188034188034188e-05,
      "loss": 1.0039,
      "step": 5970
    },
    {
      "epoch": 0.9199646167455098,
      "grad_norm": 0.4472272992134094,
      "learning_rate": 1.7846153846153846e-05,
      "loss": 0.9146,
      "step": 5980
    },
    {
      "epoch": 0.9215030191146495,
      "grad_norm": 0.4618111550807953,
      "learning_rate": 1.7504273504273503e-05,
      "loss": 0.9114,
      "step": 5990
    },
    {
      "epoch": 0.9230414214837891,
      "grad_norm": 0.5266721248626709,
      "learning_rate": 1.7162393162393164e-05,
      "loss": 1.0841,
      "step": 6000
    }
  ],
  "logging_steps": 10,
  "max_steps": 6501,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3.671729634768077e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
