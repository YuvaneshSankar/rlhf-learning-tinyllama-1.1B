{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.3076804738279297,
  "eval_steps": 500,
  "global_step": 2000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0015384023691396484,
      "grad_norm": 2.584967613220215,
      "learning_rate": 2.7649769585253458e-06,
      "loss": 3.3921,
      "step": 10
    },
    {
      "epoch": 0.003076804738279297,
      "grad_norm": 2.1197686195373535,
      "learning_rate": 5.837173579109064e-06,
      "loss": 3.4355,
      "step": 20
    },
    {
      "epoch": 0.004615207107418946,
      "grad_norm": 1.4546103477478027,
      "learning_rate": 8.90937019969278e-06,
      "loss": 3.2477,
      "step": 30
    },
    {
      "epoch": 0.006153609476558594,
      "grad_norm": 1.344617486000061,
      "learning_rate": 1.1981566820276497e-05,
      "loss": 3.1501,
      "step": 40
    },
    {
      "epoch": 0.007692011845698243,
      "grad_norm": 2.066725730895996,
      "learning_rate": 1.5053763440860215e-05,
      "loss": 3.1164,
      "step": 50
    },
    {
      "epoch": 0.009230414214837892,
      "grad_norm": 1.6152411699295044,
      "learning_rate": 1.8125960061443932e-05,
      "loss": 2.7623,
      "step": 60
    },
    {
      "epoch": 0.010768816583977539,
      "grad_norm": 1.8350930213928223,
      "learning_rate": 2.1198156682027652e-05,
      "loss": 2.5937,
      "step": 70
    },
    {
      "epoch": 0.012307218953117188,
      "grad_norm": 2.344829797744751,
      "learning_rate": 2.427035330261137e-05,
      "loss": 2.4205,
      "step": 80
    },
    {
      "epoch": 0.013845621322256836,
      "grad_norm": 4.050056457519531,
      "learning_rate": 2.734254992319509e-05,
      "loss": 2.2338,
      "step": 90
    },
    {
      "epoch": 0.015384023691396485,
      "grad_norm": 2.1490421295166016,
      "learning_rate": 3.0414746543778806e-05,
      "loss": 2.0444,
      "step": 100
    },
    {
      "epoch": 0.016922426060536132,
      "grad_norm": 1.358504056930542,
      "learning_rate": 3.348694316436252e-05,
      "loss": 1.8553,
      "step": 110
    },
    {
      "epoch": 0.018460828429675783,
      "grad_norm": 1.3946998119354248,
      "learning_rate": 3.655913978494624e-05,
      "loss": 1.8513,
      "step": 120
    },
    {
      "epoch": 0.01999923079881543,
      "grad_norm": 2.3798844814300537,
      "learning_rate": 3.963133640552996e-05,
      "loss": 1.7238,
      "step": 130
    },
    {
      "epoch": 0.021537633167955077,
      "grad_norm": 1.6720370054244995,
      "learning_rate": 4.270353302611367e-05,
      "loss": 1.6326,
      "step": 140
    },
    {
      "epoch": 0.023076035537094728,
      "grad_norm": 1.9096741676330566,
      "learning_rate": 4.577572964669739e-05,
      "loss": 1.499,
      "step": 150
    },
    {
      "epoch": 0.024614437906234375,
      "grad_norm": 1.5747382640838623,
      "learning_rate": 4.8847926267281106e-05,
      "loss": 1.3412,
      "step": 160
    },
    {
      "epoch": 0.026152840275374026,
      "grad_norm": 1.592766523361206,
      "learning_rate": 5.192012288786483e-05,
      "loss": 1.3982,
      "step": 170
    },
    {
      "epoch": 0.027691242644513673,
      "grad_norm": 1.6842422485351562,
      "learning_rate": 5.499231950844854e-05,
      "loss": 1.3376,
      "step": 180
    },
    {
      "epoch": 0.02922964501365332,
      "grad_norm": 1.5417200326919556,
      "learning_rate": 5.8064516129032266e-05,
      "loss": 1.2252,
      "step": 190
    },
    {
      "epoch": 0.03076804738279297,
      "grad_norm": 3.287029504776001,
      "learning_rate": 6.113671274961598e-05,
      "loss": 1.315,
      "step": 200
    },
    {
      "epoch": 0.03230644975193262,
      "grad_norm": 1.9193211793899536,
      "learning_rate": 6.42089093701997e-05,
      "loss": 1.2919,
      "step": 210
    },
    {
      "epoch": 0.033844852121072265,
      "grad_norm": 1.1237131357192993,
      "learning_rate": 6.72811059907834e-05,
      "loss": 1.2607,
      "step": 220
    },
    {
      "epoch": 0.035383254490211916,
      "grad_norm": 5.890222072601318,
      "learning_rate": 7.035330261136714e-05,
      "loss": 1.1936,
      "step": 230
    },
    {
      "epoch": 0.036921656859351566,
      "grad_norm": 1.1118741035461426,
      "learning_rate": 7.342549923195085e-05,
      "loss": 1.3291,
      "step": 240
    },
    {
      "epoch": 0.03846005922849121,
      "grad_norm": 1.3041982650756836,
      "learning_rate": 7.649769585253457e-05,
      "loss": 1.1517,
      "step": 250
    },
    {
      "epoch": 0.03999846159763086,
      "grad_norm": 0.8721215128898621,
      "learning_rate": 7.956989247311829e-05,
      "loss": 1.1674,
      "step": 260
    },
    {
      "epoch": 0.04153686396677051,
      "grad_norm": 1.0562869310379028,
      "learning_rate": 8.2642089093702e-05,
      "loss": 1.2735,
      "step": 270
    },
    {
      "epoch": 0.043075266335910155,
      "grad_norm": 1.0866798162460327,
      "learning_rate": 8.571428571428571e-05,
      "loss": 1.2501,
      "step": 280
    },
    {
      "epoch": 0.044613668705049805,
      "grad_norm": 1.4433777332305908,
      "learning_rate": 8.878648233486943e-05,
      "loss": 1.1162,
      "step": 290
    },
    {
      "epoch": 0.046152071074189456,
      "grad_norm": 1.0068490505218506,
      "learning_rate": 9.185867895545315e-05,
      "loss": 1.1211,
      "step": 300
    },
    {
      "epoch": 0.0476904734433291,
      "grad_norm": 1.2410961389541626,
      "learning_rate": 9.493087557603687e-05,
      "loss": 1.1446,
      "step": 310
    },
    {
      "epoch": 0.04922887581246875,
      "grad_norm": 0.9184087514877319,
      "learning_rate": 9.800307219662059e-05,
      "loss": 1.1429,
      "step": 320
    },
    {
      "epoch": 0.0507672781816084,
      "grad_norm": 0.8276410698890686,
      "learning_rate": 0.0001010752688172043,
      "loss": 1.2176,
      "step": 330
    },
    {
      "epoch": 0.05230568055074805,
      "grad_norm": 0.9394607543945312,
      "learning_rate": 0.00010414746543778802,
      "loss": 1.1135,
      "step": 340
    },
    {
      "epoch": 0.053844082919887695,
      "grad_norm": 0.7618582844734192,
      "learning_rate": 0.00010721966205837175,
      "loss": 1.1033,
      "step": 350
    },
    {
      "epoch": 0.055382485289027346,
      "grad_norm": 0.8039534091949463,
      "learning_rate": 0.00011029185867895546,
      "loss": 1.1847,
      "step": 360
    },
    {
      "epoch": 0.056920887658166996,
      "grad_norm": 1.115044355392456,
      "learning_rate": 0.00011336405529953918,
      "loss": 1.1742,
      "step": 370
    },
    {
      "epoch": 0.05845929002730664,
      "grad_norm": 0.9048172235488892,
      "learning_rate": 0.00011643625192012289,
      "loss": 1.214,
      "step": 380
    },
    {
      "epoch": 0.05999769239644629,
      "grad_norm": 0.8437467217445374,
      "learning_rate": 0.00011950844854070662,
      "loss": 1.1377,
      "step": 390
    },
    {
      "epoch": 0.06153609476558594,
      "grad_norm": 0.7477501630783081,
      "learning_rate": 0.00012258064516129034,
      "loss": 1.1254,
      "step": 400
    },
    {
      "epoch": 0.06307449713472559,
      "grad_norm": 0.7767701148986816,
      "learning_rate": 0.00012565284178187405,
      "loss": 1.107,
      "step": 410
    },
    {
      "epoch": 0.06461289950386524,
      "grad_norm": 1.1635019779205322,
      "learning_rate": 0.00012872503840245775,
      "loss": 1.1033,
      "step": 420
    },
    {
      "epoch": 0.06615130187300489,
      "grad_norm": 0.8846574425697327,
      "learning_rate": 0.0001317972350230415,
      "loss": 1.1624,
      "step": 430
    },
    {
      "epoch": 0.06768970424214453,
      "grad_norm": 0.9813984036445618,
      "learning_rate": 0.0001348694316436252,
      "loss": 1.2409,
      "step": 440
    },
    {
      "epoch": 0.06922810661128419,
      "grad_norm": 0.9507609009742737,
      "learning_rate": 0.0001379416282642089,
      "loss": 1.2503,
      "step": 450
    },
    {
      "epoch": 0.07076650898042383,
      "grad_norm": 0.7813093066215515,
      "learning_rate": 0.00014101382488479263,
      "loss": 1.0684,
      "step": 460
    },
    {
      "epoch": 0.07230491134956347,
      "grad_norm": 0.7689453363418579,
      "learning_rate": 0.00014408602150537637,
      "loss": 1.1374,
      "step": 470
    },
    {
      "epoch": 0.07384331371870313,
      "grad_norm": 0.7696683406829834,
      "learning_rate": 0.00014715821812596007,
      "loss": 1.0953,
      "step": 480
    },
    {
      "epoch": 0.07538171608784278,
      "grad_norm": 0.6287794709205627,
      "learning_rate": 0.00015023041474654378,
      "loss": 1.107,
      "step": 490
    },
    {
      "epoch": 0.07692011845698242,
      "grad_norm": 0.8339664340019226,
      "learning_rate": 0.0001533026113671275,
      "loss": 1.1726,
      "step": 500
    },
    {
      "epoch": 0.07845852082612208,
      "grad_norm": 0.7385021448135376,
      "learning_rate": 0.00015637480798771122,
      "loss": 1.1105,
      "step": 510
    },
    {
      "epoch": 0.07999692319526172,
      "grad_norm": 1.2106229066848755,
      "learning_rate": 0.00015944700460829493,
      "loss": 1.0924,
      "step": 520
    },
    {
      "epoch": 0.08153532556440136,
      "grad_norm": 0.7910128235816956,
      "learning_rate": 0.00016251920122887866,
      "loss": 1.0595,
      "step": 530
    },
    {
      "epoch": 0.08307372793354102,
      "grad_norm": 0.6995137333869934,
      "learning_rate": 0.0001655913978494624,
      "loss": 1.1278,
      "step": 540
    },
    {
      "epoch": 0.08461213030268067,
      "grad_norm": 0.6679721474647522,
      "learning_rate": 0.0001686635944700461,
      "loss": 1.1902,
      "step": 550
    },
    {
      "epoch": 0.08615053267182031,
      "grad_norm": 0.7743341326713562,
      "learning_rate": 0.0001717357910906298,
      "loss": 1.1896,
      "step": 560
    },
    {
      "epoch": 0.08768893504095997,
      "grad_norm": 0.537539005279541,
      "learning_rate": 0.0001748079877112135,
      "loss": 1.1159,
      "step": 570
    },
    {
      "epoch": 0.08922733741009961,
      "grad_norm": 0.7204731702804565,
      "learning_rate": 0.00017788018433179725,
      "loss": 1.0316,
      "step": 580
    },
    {
      "epoch": 0.09076573977923925,
      "grad_norm": 0.7807141542434692,
      "learning_rate": 0.00018095238095238095,
      "loss": 1.1666,
      "step": 590
    },
    {
      "epoch": 0.09230414214837891,
      "grad_norm": 0.6946710348129272,
      "learning_rate": 0.0001840245775729647,
      "loss": 1.0602,
      "step": 600
    },
    {
      "epoch": 0.09384254451751856,
      "grad_norm": 0.6253068447113037,
      "learning_rate": 0.0001870967741935484,
      "loss": 1.1165,
      "step": 610
    },
    {
      "epoch": 0.0953809468866582,
      "grad_norm": 0.6554427146911621,
      "learning_rate": 0.00019016897081413213,
      "loss": 1.1488,
      "step": 620
    },
    {
      "epoch": 0.09691934925579786,
      "grad_norm": 0.7545556426048279,
      "learning_rate": 0.00019324116743471583,
      "loss": 1.0227,
      "step": 630
    },
    {
      "epoch": 0.0984577516249375,
      "grad_norm": 0.654740035533905,
      "learning_rate": 0.00019631336405529954,
      "loss": 1.0965,
      "step": 640
    },
    {
      "epoch": 0.09999615399407714,
      "grad_norm": 0.5748230218887329,
      "learning_rate": 0.00019938556067588327,
      "loss": 1.1018,
      "step": 650
    },
    {
      "epoch": 0.1015345563632168,
      "grad_norm": 0.49472448229789734,
      "learning_rate": 0.00019972649572649572,
      "loss": 1.1408,
      "step": 660
    },
    {
      "epoch": 0.10307295873235645,
      "grad_norm": 0.5874394178390503,
      "learning_rate": 0.0001993846153846154,
      "loss": 1.0593,
      "step": 670
    },
    {
      "epoch": 0.1046113611014961,
      "grad_norm": 0.7135041952133179,
      "learning_rate": 0.00019904273504273506,
      "loss": 1.177,
      "step": 680
    },
    {
      "epoch": 0.10614976347063575,
      "grad_norm": 0.659172773361206,
      "learning_rate": 0.00019870085470085472,
      "loss": 1.0972,
      "step": 690
    },
    {
      "epoch": 0.10768816583977539,
      "grad_norm": 0.5658200979232788,
      "learning_rate": 0.00019835897435897438,
      "loss": 0.998,
      "step": 700
    },
    {
      "epoch": 0.10922656820891505,
      "grad_norm": 0.6410398483276367,
      "learning_rate": 0.00019801709401709404,
      "loss": 1.0226,
      "step": 710
    },
    {
      "epoch": 0.11076497057805469,
      "grad_norm": 0.6680729389190674,
      "learning_rate": 0.0001976752136752137,
      "loss": 1.0757,
      "step": 720
    },
    {
      "epoch": 0.11230337294719434,
      "grad_norm": 0.9527504444122314,
      "learning_rate": 0.00019733333333333335,
      "loss": 1.0509,
      "step": 730
    },
    {
      "epoch": 0.11384177531633399,
      "grad_norm": 0.5344208478927612,
      "learning_rate": 0.000196991452991453,
      "loss": 1.137,
      "step": 740
    },
    {
      "epoch": 0.11538017768547364,
      "grad_norm": 0.5921201705932617,
      "learning_rate": 0.00019664957264957267,
      "loss": 1.0559,
      "step": 750
    },
    {
      "epoch": 0.11691858005461328,
      "grad_norm": 0.711140513420105,
      "learning_rate": 0.00019630769230769232,
      "loss": 1.0359,
      "step": 760
    },
    {
      "epoch": 0.11845698242375294,
      "grad_norm": 0.4643453061580658,
      "learning_rate": 0.00019596581196581198,
      "loss": 1.051,
      "step": 770
    },
    {
      "epoch": 0.11999538479289258,
      "grad_norm": 0.6035484671592712,
      "learning_rate": 0.0001956239316239316,
      "loss": 1.088,
      "step": 780
    },
    {
      "epoch": 0.12153378716203223,
      "grad_norm": 0.7892462015151978,
      "learning_rate": 0.00019528205128205127,
      "loss": 1.0369,
      "step": 790
    },
    {
      "epoch": 0.12307218953117188,
      "grad_norm": 0.4591948986053467,
      "learning_rate": 0.00019494017094017095,
      "loss": 1.0998,
      "step": 800
    },
    {
      "epoch": 0.12461059190031153,
      "grad_norm": 0.5988150835037231,
      "learning_rate": 0.0001945982905982906,
      "loss": 1.0529,
      "step": 810
    },
    {
      "epoch": 0.12614899426945117,
      "grad_norm": 0.5166341662406921,
      "learning_rate": 0.00019425641025641027,
      "loss": 1.0788,
      "step": 820
    },
    {
      "epoch": 0.12768739663859083,
      "grad_norm": 0.5803174376487732,
      "learning_rate": 0.00019391452991452993,
      "loss": 0.9944,
      "step": 830
    },
    {
      "epoch": 0.12922579900773049,
      "grad_norm": 0.617644727230072,
      "learning_rate": 0.00019357264957264958,
      "loss": 1.0888,
      "step": 840
    },
    {
      "epoch": 0.13076420137687012,
      "grad_norm": 0.61956787109375,
      "learning_rate": 0.00019323076923076924,
      "loss": 1.083,
      "step": 850
    },
    {
      "epoch": 0.13230260374600977,
      "grad_norm": 0.6188405156135559,
      "learning_rate": 0.0001928888888888889,
      "loss": 1.1391,
      "step": 860
    },
    {
      "epoch": 0.13384100611514943,
      "grad_norm": 0.6159250736236572,
      "learning_rate": 0.00019254700854700856,
      "loss": 1.0782,
      "step": 870
    },
    {
      "epoch": 0.13537940848428906,
      "grad_norm": 0.4905155301094055,
      "learning_rate": 0.00019220512820512822,
      "loss": 1.0785,
      "step": 880
    },
    {
      "epoch": 0.13691781085342872,
      "grad_norm": 0.476400226354599,
      "learning_rate": 0.00019186324786324787,
      "loss": 1.0371,
      "step": 890
    },
    {
      "epoch": 0.13845621322256838,
      "grad_norm": 0.5361171960830688,
      "learning_rate": 0.00019152136752136753,
      "loss": 1.1167,
      "step": 900
    },
    {
      "epoch": 0.139994615591708,
      "grad_norm": 0.45032036304473877,
      "learning_rate": 0.0001911794871794872,
      "loss": 1.0379,
      "step": 910
    },
    {
      "epoch": 0.14153301796084766,
      "grad_norm": 0.5094605684280396,
      "learning_rate": 0.00019083760683760685,
      "loss": 1.0336,
      "step": 920
    },
    {
      "epoch": 0.14307142032998732,
      "grad_norm": 0.6864129900932312,
      "learning_rate": 0.0001904957264957265,
      "loss": 1.0494,
      "step": 930
    },
    {
      "epoch": 0.14460982269912695,
      "grad_norm": 0.5435264706611633,
      "learning_rate": 0.00019015384615384616,
      "loss": 1.0812,
      "step": 940
    },
    {
      "epoch": 0.1461482250682666,
      "grad_norm": 0.5616250038146973,
      "learning_rate": 0.00018981196581196582,
      "loss": 1.0181,
      "step": 950
    },
    {
      "epoch": 0.14768662743740626,
      "grad_norm": 0.5459400415420532,
      "learning_rate": 0.00018947008547008548,
      "loss": 1.0613,
      "step": 960
    },
    {
      "epoch": 0.1492250298065459,
      "grad_norm": 0.5072353482246399,
      "learning_rate": 0.00018912820512820513,
      "loss": 1.0952,
      "step": 970
    },
    {
      "epoch": 0.15076343217568555,
      "grad_norm": 0.5611528158187866,
      "learning_rate": 0.0001887863247863248,
      "loss": 1.034,
      "step": 980
    },
    {
      "epoch": 0.1523018345448252,
      "grad_norm": 0.5034070014953613,
      "learning_rate": 0.00018844444444444445,
      "loss": 1.1059,
      "step": 990
    },
    {
      "epoch": 0.15384023691396484,
      "grad_norm": 0.4489002227783203,
      "learning_rate": 0.0001881025641025641,
      "loss": 1.1294,
      "step": 1000
    },
    {
      "epoch": 0.1553786392831045,
      "grad_norm": 0.5371571779251099,
      "learning_rate": 0.00018776068376068377,
      "loss": 1.0166,
      "step": 1010
    },
    {
      "epoch": 0.15691704165224415,
      "grad_norm": 0.5085307955741882,
      "learning_rate": 0.00018741880341880342,
      "loss": 1.0421,
      "step": 1020
    },
    {
      "epoch": 0.15845544402138378,
      "grad_norm": 0.49393656849861145,
      "learning_rate": 0.00018707692307692308,
      "loss": 1.1003,
      "step": 1030
    },
    {
      "epoch": 0.15999384639052344,
      "grad_norm": 0.47883597016334534,
      "learning_rate": 0.00018673504273504274,
      "loss": 1.015,
      "step": 1040
    },
    {
      "epoch": 0.1615322487596631,
      "grad_norm": 0.4961189329624176,
      "learning_rate": 0.0001863931623931624,
      "loss": 1.0595,
      "step": 1050
    },
    {
      "epoch": 0.16307065112880273,
      "grad_norm": 0.5869439840316772,
      "learning_rate": 0.00018605128205128205,
      "loss": 1.0432,
      "step": 1060
    },
    {
      "epoch": 0.1646090534979424,
      "grad_norm": 0.4931619167327881,
      "learning_rate": 0.0001857094017094017,
      "loss": 1.0956,
      "step": 1070
    },
    {
      "epoch": 0.16614745586708204,
      "grad_norm": 0.5093882083892822,
      "learning_rate": 0.00018536752136752137,
      "loss": 1.0178,
      "step": 1080
    },
    {
      "epoch": 0.16768585823622167,
      "grad_norm": 0.5186927318572998,
      "learning_rate": 0.00018502564102564103,
      "loss": 1.0256,
      "step": 1090
    },
    {
      "epoch": 0.16922426060536133,
      "grad_norm": 0.443472683429718,
      "learning_rate": 0.00018468376068376068,
      "loss": 1.0748,
      "step": 1100
    },
    {
      "epoch": 0.170762662974501,
      "grad_norm": 0.4937427043914795,
      "learning_rate": 0.00018434188034188037,
      "loss": 1.1118,
      "step": 1110
    },
    {
      "epoch": 0.17230106534364062,
      "grad_norm": 0.48177462816238403,
      "learning_rate": 0.00018400000000000003,
      "loss": 1.0347,
      "step": 1120
    },
    {
      "epoch": 0.17383946771278028,
      "grad_norm": 0.48172661662101746,
      "learning_rate": 0.00018365811965811968,
      "loss": 1.0622,
      "step": 1130
    },
    {
      "epoch": 0.17537787008191993,
      "grad_norm": 0.5767273306846619,
      "learning_rate": 0.00018331623931623934,
      "loss": 1.0836,
      "step": 1140
    },
    {
      "epoch": 0.17691627245105956,
      "grad_norm": 0.5713164806365967,
      "learning_rate": 0.00018297435897435897,
      "loss": 1.0415,
      "step": 1150
    },
    {
      "epoch": 0.17845467482019922,
      "grad_norm": 0.7022517919540405,
      "learning_rate": 0.00018263247863247863,
      "loss": 1.0547,
      "step": 1160
    },
    {
      "epoch": 0.17999307718933888,
      "grad_norm": 0.5203202962875366,
      "learning_rate": 0.0001822905982905983,
      "loss": 1.0455,
      "step": 1170
    },
    {
      "epoch": 0.1815314795584785,
      "grad_norm": 0.6131396293640137,
      "learning_rate": 0.00018194871794871795,
      "loss": 1.1274,
      "step": 1180
    },
    {
      "epoch": 0.18306988192761817,
      "grad_norm": 0.4761025011539459,
      "learning_rate": 0.0001816068376068376,
      "loss": 1.0878,
      "step": 1190
    },
    {
      "epoch": 0.18460828429675782,
      "grad_norm": 0.5496890544891357,
      "learning_rate": 0.00018126495726495726,
      "loss": 1.0284,
      "step": 1200
    },
    {
      "epoch": 0.18614668666589745,
      "grad_norm": 0.5592815279960632,
      "learning_rate": 0.00018092307692307692,
      "loss": 1.0159,
      "step": 1210
    },
    {
      "epoch": 0.1876850890350371,
      "grad_norm": 0.4768582284450531,
      "learning_rate": 0.00018058119658119658,
      "loss": 1.005,
      "step": 1220
    },
    {
      "epoch": 0.18922349140417677,
      "grad_norm": 0.46654272079467773,
      "learning_rate": 0.00018023931623931623,
      "loss": 1.0125,
      "step": 1230
    },
    {
      "epoch": 0.1907618937733164,
      "grad_norm": 0.46942490339279175,
      "learning_rate": 0.00017989743589743592,
      "loss": 1.0969,
      "step": 1240
    },
    {
      "epoch": 0.19230029614245606,
      "grad_norm": 0.4688929319381714,
      "learning_rate": 0.00017955555555555558,
      "loss": 0.9848,
      "step": 1250
    },
    {
      "epoch": 0.19383869851159571,
      "grad_norm": 0.4767244756221771,
      "learning_rate": 0.00017921367521367523,
      "loss": 1.0352,
      "step": 1260
    },
    {
      "epoch": 0.19537710088073534,
      "grad_norm": 0.51897794008255,
      "learning_rate": 0.0001788717948717949,
      "loss": 1.0773,
      "step": 1270
    },
    {
      "epoch": 0.196915503249875,
      "grad_norm": 0.5375524759292603,
      "learning_rate": 0.00017852991452991455,
      "loss": 1.0726,
      "step": 1280
    },
    {
      "epoch": 0.19845390561901466,
      "grad_norm": 0.4755353033542633,
      "learning_rate": 0.0001781880341880342,
      "loss": 1.0804,
      "step": 1290
    },
    {
      "epoch": 0.1999923079881543,
      "grad_norm": 0.534662127494812,
      "learning_rate": 0.00017784615384615387,
      "loss": 1.0072,
      "step": 1300
    },
    {
      "epoch": 0.20153071035729395,
      "grad_norm": 0.4839552044868469,
      "learning_rate": 0.00017750427350427352,
      "loss": 1.003,
      "step": 1310
    },
    {
      "epoch": 0.2030691127264336,
      "grad_norm": 0.5820987820625305,
      "learning_rate": 0.00017716239316239318,
      "loss": 1.0435,
      "step": 1320
    },
    {
      "epoch": 0.20460751509557323,
      "grad_norm": 0.6065599322319031,
      "learning_rate": 0.0001768205128205128,
      "loss": 1.0177,
      "step": 1330
    },
    {
      "epoch": 0.2061459174647129,
      "grad_norm": 0.5341425538063049,
      "learning_rate": 0.00017647863247863247,
      "loss": 1.0608,
      "step": 1340
    },
    {
      "epoch": 0.20768431983385255,
      "grad_norm": 0.7015015482902527,
      "learning_rate": 0.00017613675213675213,
      "loss": 1.0008,
      "step": 1350
    },
    {
      "epoch": 0.2092227222029922,
      "grad_norm": 0.7774748802185059,
      "learning_rate": 0.00017579487179487178,
      "loss": 1.0223,
      "step": 1360
    },
    {
      "epoch": 0.21076112457213184,
      "grad_norm": 0.4953027069568634,
      "learning_rate": 0.00017545299145299144,
      "loss": 1.0264,
      "step": 1370
    },
    {
      "epoch": 0.2122995269412715,
      "grad_norm": 0.3876659870147705,
      "learning_rate": 0.00017511111111111113,
      "loss": 1.0861,
      "step": 1380
    },
    {
      "epoch": 0.21383792931041115,
      "grad_norm": 0.4554290771484375,
      "learning_rate": 0.00017476923076923078,
      "loss": 1.0197,
      "step": 1390
    },
    {
      "epoch": 0.21537633167955078,
      "grad_norm": 0.47685301303863525,
      "learning_rate": 0.00017442735042735044,
      "loss": 1.0574,
      "step": 1400
    },
    {
      "epoch": 0.21691473404869044,
      "grad_norm": 0.5016463994979858,
      "learning_rate": 0.0001740854700854701,
      "loss": 1.0582,
      "step": 1410
    },
    {
      "epoch": 0.2184531364178301,
      "grad_norm": 0.48101872205734253,
      "learning_rate": 0.00017374358974358976,
      "loss": 1.0172,
      "step": 1420
    },
    {
      "epoch": 0.21999153878696973,
      "grad_norm": 0.47797712683677673,
      "learning_rate": 0.00017340170940170942,
      "loss": 1.0505,
      "step": 1430
    },
    {
      "epoch": 0.22152994115610938,
      "grad_norm": 0.5501773357391357,
      "learning_rate": 0.00017305982905982907,
      "loss": 1.019,
      "step": 1440
    },
    {
      "epoch": 0.22306834352524904,
      "grad_norm": 0.4892399311065674,
      "learning_rate": 0.00017271794871794873,
      "loss": 1.0413,
      "step": 1450
    },
    {
      "epoch": 0.22460674589438867,
      "grad_norm": 0.49763554334640503,
      "learning_rate": 0.0001723760683760684,
      "loss": 1.0176,
      "step": 1460
    },
    {
      "epoch": 0.22614514826352833,
      "grad_norm": 0.4826153516769409,
      "learning_rate": 0.00017203418803418805,
      "loss": 0.9775,
      "step": 1470
    },
    {
      "epoch": 0.22768355063266799,
      "grad_norm": 0.4612135589122772,
      "learning_rate": 0.0001716923076923077,
      "loss": 1.0594,
      "step": 1480
    },
    {
      "epoch": 0.22922195300180762,
      "grad_norm": 0.4901314973831177,
      "learning_rate": 0.00017135042735042736,
      "loss": 1.0009,
      "step": 1490
    },
    {
      "epoch": 0.23076035537094727,
      "grad_norm": 0.589230477809906,
      "learning_rate": 0.00017100854700854702,
      "loss": 0.9804,
      "step": 1500
    },
    {
      "epoch": 0.23229875774008693,
      "grad_norm": 0.5508666634559631,
      "learning_rate": 0.00017066666666666668,
      "loss": 0.9797,
      "step": 1510
    },
    {
      "epoch": 0.23383716010922656,
      "grad_norm": 0.4979475736618042,
      "learning_rate": 0.00017032478632478633,
      "loss": 0.9843,
      "step": 1520
    },
    {
      "epoch": 0.23537556247836622,
      "grad_norm": 0.6265875101089478,
      "learning_rate": 0.000169982905982906,
      "loss": 0.9785,
      "step": 1530
    },
    {
      "epoch": 0.23691396484750588,
      "grad_norm": 0.593346118927002,
      "learning_rate": 0.00016964102564102565,
      "loss": 1.0834,
      "step": 1540
    },
    {
      "epoch": 0.2384523672166455,
      "grad_norm": 0.4974205195903778,
      "learning_rate": 0.0001692991452991453,
      "loss": 1.0233,
      "step": 1550
    },
    {
      "epoch": 0.23999076958578516,
      "grad_norm": 0.4518866539001465,
      "learning_rate": 0.00016895726495726497,
      "loss": 1.0232,
      "step": 1560
    },
    {
      "epoch": 0.24152917195492482,
      "grad_norm": 0.8069553375244141,
      "learning_rate": 0.00016861538461538462,
      "loss": 1.0455,
      "step": 1570
    },
    {
      "epoch": 0.24306757432406445,
      "grad_norm": 0.432888388633728,
      "learning_rate": 0.00016827350427350428,
      "loss": 1.0586,
      "step": 1580
    },
    {
      "epoch": 0.2446059766932041,
      "grad_norm": 0.44304749369621277,
      "learning_rate": 0.00016793162393162394,
      "loss": 1.0687,
      "step": 1590
    },
    {
      "epoch": 0.24614437906234377,
      "grad_norm": 0.5872960686683655,
      "learning_rate": 0.0001675897435897436,
      "loss": 0.9978,
      "step": 1600
    },
    {
      "epoch": 0.2476827814314834,
      "grad_norm": 1.649830937385559,
      "learning_rate": 0.00016724786324786325,
      "loss": 1.091,
      "step": 1610
    },
    {
      "epoch": 0.24922118380062305,
      "grad_norm": 0.566842257976532,
      "learning_rate": 0.0001669059829059829,
      "loss": 0.9668,
      "step": 1620
    },
    {
      "epoch": 0.2507595861697627,
      "grad_norm": 0.5304808616638184,
      "learning_rate": 0.00016656410256410257,
      "loss": 1.0779,
      "step": 1630
    },
    {
      "epoch": 0.25229798853890234,
      "grad_norm": 0.5211077332496643,
      "learning_rate": 0.00016622222222222223,
      "loss": 1.0482,
      "step": 1640
    },
    {
      "epoch": 0.253836390908042,
      "grad_norm": 0.5898643732070923,
      "learning_rate": 0.00016588034188034188,
      "loss": 0.9736,
      "step": 1650
    },
    {
      "epoch": 0.25537479327718166,
      "grad_norm": 0.438373327255249,
      "learning_rate": 0.00016553846153846154,
      "loss": 0.978,
      "step": 1660
    },
    {
      "epoch": 0.2569131956463213,
      "grad_norm": 0.5471181273460388,
      "learning_rate": 0.0001651965811965812,
      "loss": 1.079,
      "step": 1670
    },
    {
      "epoch": 0.25845159801546097,
      "grad_norm": 0.4805585741996765,
      "learning_rate": 0.00016485470085470088,
      "loss": 1.0538,
      "step": 1680
    },
    {
      "epoch": 0.2599900003846006,
      "grad_norm": 0.5803124904632568,
      "learning_rate": 0.00016451282051282054,
      "loss": 1.096,
      "step": 1690
    },
    {
      "epoch": 0.26152840275374023,
      "grad_norm": 0.5438754558563232,
      "learning_rate": 0.00016417094017094017,
      "loss": 0.9875,
      "step": 1700
    },
    {
      "epoch": 0.2630668051228799,
      "grad_norm": 0.4743230938911438,
      "learning_rate": 0.00016382905982905983,
      "loss": 0.9778,
      "step": 1710
    },
    {
      "epoch": 0.26460520749201955,
      "grad_norm": 0.48273923993110657,
      "learning_rate": 0.0001634871794871795,
      "loss": 1.0281,
      "step": 1720
    },
    {
      "epoch": 0.2661436098611592,
      "grad_norm": 0.456667959690094,
      "learning_rate": 0.00016314529914529915,
      "loss": 0.9784,
      "step": 1730
    },
    {
      "epoch": 0.26768201223029886,
      "grad_norm": 0.5305406451225281,
      "learning_rate": 0.0001628034188034188,
      "loss": 0.9641,
      "step": 1740
    },
    {
      "epoch": 0.26922041459943846,
      "grad_norm": 0.521521270275116,
      "learning_rate": 0.00016246153846153846,
      "loss": 0.9471,
      "step": 1750
    },
    {
      "epoch": 0.2707588169685781,
      "grad_norm": 0.5061790943145752,
      "learning_rate": 0.00016211965811965812,
      "loss": 1.0047,
      "step": 1760
    },
    {
      "epoch": 0.2722972193377178,
      "grad_norm": 0.45628097653388977,
      "learning_rate": 0.00016177777777777778,
      "loss": 0.9981,
      "step": 1770
    },
    {
      "epoch": 0.27383562170685743,
      "grad_norm": 0.5362868905067444,
      "learning_rate": 0.00016143589743589743,
      "loss": 1.0533,
      "step": 1780
    },
    {
      "epoch": 0.2753740240759971,
      "grad_norm": 0.475908488035202,
      "learning_rate": 0.0001610940170940171,
      "loss": 1.0608,
      "step": 1790
    },
    {
      "epoch": 0.27691242644513675,
      "grad_norm": 0.517157256603241,
      "learning_rate": 0.00016075213675213675,
      "loss": 1.0611,
      "step": 1800
    },
    {
      "epoch": 0.27845082881427635,
      "grad_norm": 0.4431826174259186,
      "learning_rate": 0.0001604102564102564,
      "loss": 1.0208,
      "step": 1810
    },
    {
      "epoch": 0.279989231183416,
      "grad_norm": 0.4075940251350403,
      "learning_rate": 0.0001600683760683761,
      "loss": 1.0293,
      "step": 1820
    },
    {
      "epoch": 0.28152763355255567,
      "grad_norm": 0.4613097608089447,
      "learning_rate": 0.00015972649572649575,
      "loss": 1.0069,
      "step": 1830
    },
    {
      "epoch": 0.2830660359216953,
      "grad_norm": 0.4482089877128601,
      "learning_rate": 0.0001593846153846154,
      "loss": 0.9918,
      "step": 1840
    },
    {
      "epoch": 0.284604438290835,
      "grad_norm": 0.4703086018562317,
      "learning_rate": 0.00015904273504273507,
      "loss": 1.0441,
      "step": 1850
    },
    {
      "epoch": 0.28614284065997464,
      "grad_norm": 0.45097023248672485,
      "learning_rate": 0.00015870085470085472,
      "loss": 1.0661,
      "step": 1860
    },
    {
      "epoch": 0.28768124302911424,
      "grad_norm": 0.515642523765564,
      "learning_rate": 0.00015835897435897438,
      "loss": 1.0211,
      "step": 1870
    },
    {
      "epoch": 0.2892196453982539,
      "grad_norm": 0.4621179699897766,
      "learning_rate": 0.000158017094017094,
      "loss": 1.0384,
      "step": 1880
    },
    {
      "epoch": 0.29075804776739356,
      "grad_norm": 0.5528222322463989,
      "learning_rate": 0.00015767521367521367,
      "loss": 1.0289,
      "step": 1890
    },
    {
      "epoch": 0.2922964501365332,
      "grad_norm": 0.4765111804008484,
      "learning_rate": 0.00015733333333333333,
      "loss": 1.0358,
      "step": 1900
    },
    {
      "epoch": 0.29383485250567287,
      "grad_norm": 0.44668295979499817,
      "learning_rate": 0.00015699145299145298,
      "loss": 1.0387,
      "step": 1910
    },
    {
      "epoch": 0.29537325487481253,
      "grad_norm": 0.5040528178215027,
      "learning_rate": 0.00015664957264957264,
      "loss": 1.0139,
      "step": 1920
    },
    {
      "epoch": 0.29691165724395213,
      "grad_norm": 0.4558364450931549,
      "learning_rate": 0.0001563076923076923,
      "loss": 1.0437,
      "step": 1930
    },
    {
      "epoch": 0.2984500596130918,
      "grad_norm": 0.45144742727279663,
      "learning_rate": 0.00015596581196581196,
      "loss": 1.0264,
      "step": 1940
    },
    {
      "epoch": 0.29998846198223145,
      "grad_norm": 0.5601553320884705,
      "learning_rate": 0.00015562393162393164,
      "loss": 0.9812,
      "step": 1950
    },
    {
      "epoch": 0.3015268643513711,
      "grad_norm": 0.4512892961502075,
      "learning_rate": 0.0001552820512820513,
      "loss": 1.0399,
      "step": 1960
    },
    {
      "epoch": 0.30306526672051076,
      "grad_norm": 0.4634018838405609,
      "learning_rate": 0.00015494017094017096,
      "loss": 0.9926,
      "step": 1970
    },
    {
      "epoch": 0.3046036690896504,
      "grad_norm": 0.46779969334602356,
      "learning_rate": 0.00015459829059829062,
      "loss": 1.0136,
      "step": 1980
    },
    {
      "epoch": 0.30614207145879,
      "grad_norm": 0.46477067470550537,
      "learning_rate": 0.00015425641025641027,
      "loss": 1.1196,
      "step": 1990
    },
    {
      "epoch": 0.3076804738279297,
      "grad_norm": 0.4696638286113739,
      "learning_rate": 0.00015391452991452993,
      "loss": 1.0334,
      "step": 2000
    }
  ],
  "logging_steps": 10,
  "max_steps": 6501,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.2256022466330624e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
