{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.6922810661128418,
  "eval_steps": 500,
  "global_step": 4500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0015384023691396484,
      "grad_norm": 2.584967613220215,
      "learning_rate": 2.7649769585253458e-06,
      "loss": 3.3921,
      "step": 10
    },
    {
      "epoch": 0.003076804738279297,
      "grad_norm": 2.1197686195373535,
      "learning_rate": 5.837173579109064e-06,
      "loss": 3.4355,
      "step": 20
    },
    {
      "epoch": 0.004615207107418946,
      "grad_norm": 1.4546103477478027,
      "learning_rate": 8.90937019969278e-06,
      "loss": 3.2477,
      "step": 30
    },
    {
      "epoch": 0.006153609476558594,
      "grad_norm": 1.344617486000061,
      "learning_rate": 1.1981566820276497e-05,
      "loss": 3.1501,
      "step": 40
    },
    {
      "epoch": 0.007692011845698243,
      "grad_norm": 2.066725730895996,
      "learning_rate": 1.5053763440860215e-05,
      "loss": 3.1164,
      "step": 50
    },
    {
      "epoch": 0.009230414214837892,
      "grad_norm": 1.6152411699295044,
      "learning_rate": 1.8125960061443932e-05,
      "loss": 2.7623,
      "step": 60
    },
    {
      "epoch": 0.010768816583977539,
      "grad_norm": 1.8350930213928223,
      "learning_rate": 2.1198156682027652e-05,
      "loss": 2.5937,
      "step": 70
    },
    {
      "epoch": 0.012307218953117188,
      "grad_norm": 2.344829797744751,
      "learning_rate": 2.427035330261137e-05,
      "loss": 2.4205,
      "step": 80
    },
    {
      "epoch": 0.013845621322256836,
      "grad_norm": 4.050056457519531,
      "learning_rate": 2.734254992319509e-05,
      "loss": 2.2338,
      "step": 90
    },
    {
      "epoch": 0.015384023691396485,
      "grad_norm": 2.1490421295166016,
      "learning_rate": 3.0414746543778806e-05,
      "loss": 2.0444,
      "step": 100
    },
    {
      "epoch": 0.016922426060536132,
      "grad_norm": 1.358504056930542,
      "learning_rate": 3.348694316436252e-05,
      "loss": 1.8553,
      "step": 110
    },
    {
      "epoch": 0.018460828429675783,
      "grad_norm": 1.3946998119354248,
      "learning_rate": 3.655913978494624e-05,
      "loss": 1.8513,
      "step": 120
    },
    {
      "epoch": 0.01999923079881543,
      "grad_norm": 2.3798844814300537,
      "learning_rate": 3.963133640552996e-05,
      "loss": 1.7238,
      "step": 130
    },
    {
      "epoch": 0.021537633167955077,
      "grad_norm": 1.6720370054244995,
      "learning_rate": 4.270353302611367e-05,
      "loss": 1.6326,
      "step": 140
    },
    {
      "epoch": 0.023076035537094728,
      "grad_norm": 1.9096741676330566,
      "learning_rate": 4.577572964669739e-05,
      "loss": 1.499,
      "step": 150
    },
    {
      "epoch": 0.024614437906234375,
      "grad_norm": 1.5747382640838623,
      "learning_rate": 4.8847926267281106e-05,
      "loss": 1.3412,
      "step": 160
    },
    {
      "epoch": 0.026152840275374026,
      "grad_norm": 1.592766523361206,
      "learning_rate": 5.192012288786483e-05,
      "loss": 1.3982,
      "step": 170
    },
    {
      "epoch": 0.027691242644513673,
      "grad_norm": 1.6842422485351562,
      "learning_rate": 5.499231950844854e-05,
      "loss": 1.3376,
      "step": 180
    },
    {
      "epoch": 0.02922964501365332,
      "grad_norm": 1.5417200326919556,
      "learning_rate": 5.8064516129032266e-05,
      "loss": 1.2252,
      "step": 190
    },
    {
      "epoch": 0.03076804738279297,
      "grad_norm": 3.287029504776001,
      "learning_rate": 6.113671274961598e-05,
      "loss": 1.315,
      "step": 200
    },
    {
      "epoch": 0.03230644975193262,
      "grad_norm": 1.9193211793899536,
      "learning_rate": 6.42089093701997e-05,
      "loss": 1.2919,
      "step": 210
    },
    {
      "epoch": 0.033844852121072265,
      "grad_norm": 1.1237131357192993,
      "learning_rate": 6.72811059907834e-05,
      "loss": 1.2607,
      "step": 220
    },
    {
      "epoch": 0.035383254490211916,
      "grad_norm": 5.890222072601318,
      "learning_rate": 7.035330261136714e-05,
      "loss": 1.1936,
      "step": 230
    },
    {
      "epoch": 0.036921656859351566,
      "grad_norm": 1.1118741035461426,
      "learning_rate": 7.342549923195085e-05,
      "loss": 1.3291,
      "step": 240
    },
    {
      "epoch": 0.03846005922849121,
      "grad_norm": 1.3041982650756836,
      "learning_rate": 7.649769585253457e-05,
      "loss": 1.1517,
      "step": 250
    },
    {
      "epoch": 0.03999846159763086,
      "grad_norm": 0.8721215128898621,
      "learning_rate": 7.956989247311829e-05,
      "loss": 1.1674,
      "step": 260
    },
    {
      "epoch": 0.04153686396677051,
      "grad_norm": 1.0562869310379028,
      "learning_rate": 8.2642089093702e-05,
      "loss": 1.2735,
      "step": 270
    },
    {
      "epoch": 0.043075266335910155,
      "grad_norm": 1.0866798162460327,
      "learning_rate": 8.571428571428571e-05,
      "loss": 1.2501,
      "step": 280
    },
    {
      "epoch": 0.044613668705049805,
      "grad_norm": 1.4433777332305908,
      "learning_rate": 8.878648233486943e-05,
      "loss": 1.1162,
      "step": 290
    },
    {
      "epoch": 0.046152071074189456,
      "grad_norm": 1.0068490505218506,
      "learning_rate": 9.185867895545315e-05,
      "loss": 1.1211,
      "step": 300
    },
    {
      "epoch": 0.0476904734433291,
      "grad_norm": 1.2410961389541626,
      "learning_rate": 9.493087557603687e-05,
      "loss": 1.1446,
      "step": 310
    },
    {
      "epoch": 0.04922887581246875,
      "grad_norm": 0.9184087514877319,
      "learning_rate": 9.800307219662059e-05,
      "loss": 1.1429,
      "step": 320
    },
    {
      "epoch": 0.0507672781816084,
      "grad_norm": 0.8276410698890686,
      "learning_rate": 0.0001010752688172043,
      "loss": 1.2176,
      "step": 330
    },
    {
      "epoch": 0.05230568055074805,
      "grad_norm": 0.9394607543945312,
      "learning_rate": 0.00010414746543778802,
      "loss": 1.1135,
      "step": 340
    },
    {
      "epoch": 0.053844082919887695,
      "grad_norm": 0.7618582844734192,
      "learning_rate": 0.00010721966205837175,
      "loss": 1.1033,
      "step": 350
    },
    {
      "epoch": 0.055382485289027346,
      "grad_norm": 0.8039534091949463,
      "learning_rate": 0.00011029185867895546,
      "loss": 1.1847,
      "step": 360
    },
    {
      "epoch": 0.056920887658166996,
      "grad_norm": 1.115044355392456,
      "learning_rate": 0.00011336405529953918,
      "loss": 1.1742,
      "step": 370
    },
    {
      "epoch": 0.05845929002730664,
      "grad_norm": 0.9048172235488892,
      "learning_rate": 0.00011643625192012289,
      "loss": 1.214,
      "step": 380
    },
    {
      "epoch": 0.05999769239644629,
      "grad_norm": 0.8437467217445374,
      "learning_rate": 0.00011950844854070662,
      "loss": 1.1377,
      "step": 390
    },
    {
      "epoch": 0.06153609476558594,
      "grad_norm": 0.7477501630783081,
      "learning_rate": 0.00012258064516129034,
      "loss": 1.1254,
      "step": 400
    },
    {
      "epoch": 0.06307449713472559,
      "grad_norm": 0.7767701148986816,
      "learning_rate": 0.00012565284178187405,
      "loss": 1.107,
      "step": 410
    },
    {
      "epoch": 0.06461289950386524,
      "grad_norm": 1.1635019779205322,
      "learning_rate": 0.00012872503840245775,
      "loss": 1.1033,
      "step": 420
    },
    {
      "epoch": 0.06615130187300489,
      "grad_norm": 0.8846574425697327,
      "learning_rate": 0.0001317972350230415,
      "loss": 1.1624,
      "step": 430
    },
    {
      "epoch": 0.06768970424214453,
      "grad_norm": 0.9813984036445618,
      "learning_rate": 0.0001348694316436252,
      "loss": 1.2409,
      "step": 440
    },
    {
      "epoch": 0.06922810661128419,
      "grad_norm": 0.9507609009742737,
      "learning_rate": 0.0001379416282642089,
      "loss": 1.2503,
      "step": 450
    },
    {
      "epoch": 0.07076650898042383,
      "grad_norm": 0.7813093066215515,
      "learning_rate": 0.00014101382488479263,
      "loss": 1.0684,
      "step": 460
    },
    {
      "epoch": 0.07230491134956347,
      "grad_norm": 0.7689453363418579,
      "learning_rate": 0.00014408602150537637,
      "loss": 1.1374,
      "step": 470
    },
    {
      "epoch": 0.07384331371870313,
      "grad_norm": 0.7696683406829834,
      "learning_rate": 0.00014715821812596007,
      "loss": 1.0953,
      "step": 480
    },
    {
      "epoch": 0.07538171608784278,
      "grad_norm": 0.6287794709205627,
      "learning_rate": 0.00015023041474654378,
      "loss": 1.107,
      "step": 490
    },
    {
      "epoch": 0.07692011845698242,
      "grad_norm": 0.8339664340019226,
      "learning_rate": 0.0001533026113671275,
      "loss": 1.1726,
      "step": 500
    },
    {
      "epoch": 0.07845852082612208,
      "grad_norm": 0.7385021448135376,
      "learning_rate": 0.00015637480798771122,
      "loss": 1.1105,
      "step": 510
    },
    {
      "epoch": 0.07999692319526172,
      "grad_norm": 1.2106229066848755,
      "learning_rate": 0.00015944700460829493,
      "loss": 1.0924,
      "step": 520
    },
    {
      "epoch": 0.08153532556440136,
      "grad_norm": 0.7910128235816956,
      "learning_rate": 0.00016251920122887866,
      "loss": 1.0595,
      "step": 530
    },
    {
      "epoch": 0.08307372793354102,
      "grad_norm": 0.6995137333869934,
      "learning_rate": 0.0001655913978494624,
      "loss": 1.1278,
      "step": 540
    },
    {
      "epoch": 0.08461213030268067,
      "grad_norm": 0.6679721474647522,
      "learning_rate": 0.0001686635944700461,
      "loss": 1.1902,
      "step": 550
    },
    {
      "epoch": 0.08615053267182031,
      "grad_norm": 0.7743341326713562,
      "learning_rate": 0.0001717357910906298,
      "loss": 1.1896,
      "step": 560
    },
    {
      "epoch": 0.08768893504095997,
      "grad_norm": 0.537539005279541,
      "learning_rate": 0.0001748079877112135,
      "loss": 1.1159,
      "step": 570
    },
    {
      "epoch": 0.08922733741009961,
      "grad_norm": 0.7204731702804565,
      "learning_rate": 0.00017788018433179725,
      "loss": 1.0316,
      "step": 580
    },
    {
      "epoch": 0.09076573977923925,
      "grad_norm": 0.7807141542434692,
      "learning_rate": 0.00018095238095238095,
      "loss": 1.1666,
      "step": 590
    },
    {
      "epoch": 0.09230414214837891,
      "grad_norm": 0.6946710348129272,
      "learning_rate": 0.0001840245775729647,
      "loss": 1.0602,
      "step": 600
    },
    {
      "epoch": 0.09384254451751856,
      "grad_norm": 0.6253068447113037,
      "learning_rate": 0.0001870967741935484,
      "loss": 1.1165,
      "step": 610
    },
    {
      "epoch": 0.0953809468866582,
      "grad_norm": 0.6554427146911621,
      "learning_rate": 0.00019016897081413213,
      "loss": 1.1488,
      "step": 620
    },
    {
      "epoch": 0.09691934925579786,
      "grad_norm": 0.7545556426048279,
      "learning_rate": 0.00019324116743471583,
      "loss": 1.0227,
      "step": 630
    },
    {
      "epoch": 0.0984577516249375,
      "grad_norm": 0.654740035533905,
      "learning_rate": 0.00019631336405529954,
      "loss": 1.0965,
      "step": 640
    },
    {
      "epoch": 0.09999615399407714,
      "grad_norm": 0.5748230218887329,
      "learning_rate": 0.00019938556067588327,
      "loss": 1.1018,
      "step": 650
    },
    {
      "epoch": 0.1015345563632168,
      "grad_norm": 0.49472448229789734,
      "learning_rate": 0.00019972649572649572,
      "loss": 1.1408,
      "step": 660
    },
    {
      "epoch": 0.10307295873235645,
      "grad_norm": 0.5874394178390503,
      "learning_rate": 0.0001993846153846154,
      "loss": 1.0593,
      "step": 670
    },
    {
      "epoch": 0.1046113611014961,
      "grad_norm": 0.7135041952133179,
      "learning_rate": 0.00019904273504273506,
      "loss": 1.177,
      "step": 680
    },
    {
      "epoch": 0.10614976347063575,
      "grad_norm": 0.659172773361206,
      "learning_rate": 0.00019870085470085472,
      "loss": 1.0972,
      "step": 690
    },
    {
      "epoch": 0.10768816583977539,
      "grad_norm": 0.5658200979232788,
      "learning_rate": 0.00019835897435897438,
      "loss": 0.998,
      "step": 700
    },
    {
      "epoch": 0.10922656820891505,
      "grad_norm": 0.6410398483276367,
      "learning_rate": 0.00019801709401709404,
      "loss": 1.0226,
      "step": 710
    },
    {
      "epoch": 0.11076497057805469,
      "grad_norm": 0.6680729389190674,
      "learning_rate": 0.0001976752136752137,
      "loss": 1.0757,
      "step": 720
    },
    {
      "epoch": 0.11230337294719434,
      "grad_norm": 0.9527504444122314,
      "learning_rate": 0.00019733333333333335,
      "loss": 1.0509,
      "step": 730
    },
    {
      "epoch": 0.11384177531633399,
      "grad_norm": 0.5344208478927612,
      "learning_rate": 0.000196991452991453,
      "loss": 1.137,
      "step": 740
    },
    {
      "epoch": 0.11538017768547364,
      "grad_norm": 0.5921201705932617,
      "learning_rate": 0.00019664957264957267,
      "loss": 1.0559,
      "step": 750
    },
    {
      "epoch": 0.11691858005461328,
      "grad_norm": 0.711140513420105,
      "learning_rate": 0.00019630769230769232,
      "loss": 1.0359,
      "step": 760
    },
    {
      "epoch": 0.11845698242375294,
      "grad_norm": 0.4643453061580658,
      "learning_rate": 0.00019596581196581198,
      "loss": 1.051,
      "step": 770
    },
    {
      "epoch": 0.11999538479289258,
      "grad_norm": 0.6035484671592712,
      "learning_rate": 0.0001956239316239316,
      "loss": 1.088,
      "step": 780
    },
    {
      "epoch": 0.12153378716203223,
      "grad_norm": 0.7892462015151978,
      "learning_rate": 0.00019528205128205127,
      "loss": 1.0369,
      "step": 790
    },
    {
      "epoch": 0.12307218953117188,
      "grad_norm": 0.4591948986053467,
      "learning_rate": 0.00019494017094017095,
      "loss": 1.0998,
      "step": 800
    },
    {
      "epoch": 0.12461059190031153,
      "grad_norm": 0.5988150835037231,
      "learning_rate": 0.0001945982905982906,
      "loss": 1.0529,
      "step": 810
    },
    {
      "epoch": 0.12614899426945117,
      "grad_norm": 0.5166341662406921,
      "learning_rate": 0.00019425641025641027,
      "loss": 1.0788,
      "step": 820
    },
    {
      "epoch": 0.12768739663859083,
      "grad_norm": 0.5803174376487732,
      "learning_rate": 0.00019391452991452993,
      "loss": 0.9944,
      "step": 830
    },
    {
      "epoch": 0.12922579900773049,
      "grad_norm": 0.617644727230072,
      "learning_rate": 0.00019357264957264958,
      "loss": 1.0888,
      "step": 840
    },
    {
      "epoch": 0.13076420137687012,
      "grad_norm": 0.61956787109375,
      "learning_rate": 0.00019323076923076924,
      "loss": 1.083,
      "step": 850
    },
    {
      "epoch": 0.13230260374600977,
      "grad_norm": 0.6188405156135559,
      "learning_rate": 0.0001928888888888889,
      "loss": 1.1391,
      "step": 860
    },
    {
      "epoch": 0.13384100611514943,
      "grad_norm": 0.6159250736236572,
      "learning_rate": 0.00019254700854700856,
      "loss": 1.0782,
      "step": 870
    },
    {
      "epoch": 0.13537940848428906,
      "grad_norm": 0.4905155301094055,
      "learning_rate": 0.00019220512820512822,
      "loss": 1.0785,
      "step": 880
    },
    {
      "epoch": 0.13691781085342872,
      "grad_norm": 0.476400226354599,
      "learning_rate": 0.00019186324786324787,
      "loss": 1.0371,
      "step": 890
    },
    {
      "epoch": 0.13845621322256838,
      "grad_norm": 0.5361171960830688,
      "learning_rate": 0.00019152136752136753,
      "loss": 1.1167,
      "step": 900
    },
    {
      "epoch": 0.139994615591708,
      "grad_norm": 0.45032036304473877,
      "learning_rate": 0.0001911794871794872,
      "loss": 1.0379,
      "step": 910
    },
    {
      "epoch": 0.14153301796084766,
      "grad_norm": 0.5094605684280396,
      "learning_rate": 0.00019083760683760685,
      "loss": 1.0336,
      "step": 920
    },
    {
      "epoch": 0.14307142032998732,
      "grad_norm": 0.6864129900932312,
      "learning_rate": 0.0001904957264957265,
      "loss": 1.0494,
      "step": 930
    },
    {
      "epoch": 0.14460982269912695,
      "grad_norm": 0.5435264706611633,
      "learning_rate": 0.00019015384615384616,
      "loss": 1.0812,
      "step": 940
    },
    {
      "epoch": 0.1461482250682666,
      "grad_norm": 0.5616250038146973,
      "learning_rate": 0.00018981196581196582,
      "loss": 1.0181,
      "step": 950
    },
    {
      "epoch": 0.14768662743740626,
      "grad_norm": 0.5459400415420532,
      "learning_rate": 0.00018947008547008548,
      "loss": 1.0613,
      "step": 960
    },
    {
      "epoch": 0.1492250298065459,
      "grad_norm": 0.5072353482246399,
      "learning_rate": 0.00018912820512820513,
      "loss": 1.0952,
      "step": 970
    },
    {
      "epoch": 0.15076343217568555,
      "grad_norm": 0.5611528158187866,
      "learning_rate": 0.0001887863247863248,
      "loss": 1.034,
      "step": 980
    },
    {
      "epoch": 0.1523018345448252,
      "grad_norm": 0.5034070014953613,
      "learning_rate": 0.00018844444444444445,
      "loss": 1.1059,
      "step": 990
    },
    {
      "epoch": 0.15384023691396484,
      "grad_norm": 0.4489002227783203,
      "learning_rate": 0.0001881025641025641,
      "loss": 1.1294,
      "step": 1000
    },
    {
      "epoch": 0.1553786392831045,
      "grad_norm": 0.5371571779251099,
      "learning_rate": 0.00018776068376068377,
      "loss": 1.0166,
      "step": 1010
    },
    {
      "epoch": 0.15691704165224415,
      "grad_norm": 0.5085307955741882,
      "learning_rate": 0.00018741880341880342,
      "loss": 1.0421,
      "step": 1020
    },
    {
      "epoch": 0.15845544402138378,
      "grad_norm": 0.49393656849861145,
      "learning_rate": 0.00018707692307692308,
      "loss": 1.1003,
      "step": 1030
    },
    {
      "epoch": 0.15999384639052344,
      "grad_norm": 0.47883597016334534,
      "learning_rate": 0.00018673504273504274,
      "loss": 1.015,
      "step": 1040
    },
    {
      "epoch": 0.1615322487596631,
      "grad_norm": 0.4961189329624176,
      "learning_rate": 0.0001863931623931624,
      "loss": 1.0595,
      "step": 1050
    },
    {
      "epoch": 0.16307065112880273,
      "grad_norm": 0.5869439840316772,
      "learning_rate": 0.00018605128205128205,
      "loss": 1.0432,
      "step": 1060
    },
    {
      "epoch": 0.1646090534979424,
      "grad_norm": 0.4931619167327881,
      "learning_rate": 0.0001857094017094017,
      "loss": 1.0956,
      "step": 1070
    },
    {
      "epoch": 0.16614745586708204,
      "grad_norm": 0.5093882083892822,
      "learning_rate": 0.00018536752136752137,
      "loss": 1.0178,
      "step": 1080
    },
    {
      "epoch": 0.16768585823622167,
      "grad_norm": 0.5186927318572998,
      "learning_rate": 0.00018502564102564103,
      "loss": 1.0256,
      "step": 1090
    },
    {
      "epoch": 0.16922426060536133,
      "grad_norm": 0.443472683429718,
      "learning_rate": 0.00018468376068376068,
      "loss": 1.0748,
      "step": 1100
    },
    {
      "epoch": 0.170762662974501,
      "grad_norm": 0.4937427043914795,
      "learning_rate": 0.00018434188034188037,
      "loss": 1.1118,
      "step": 1110
    },
    {
      "epoch": 0.17230106534364062,
      "grad_norm": 0.48177462816238403,
      "learning_rate": 0.00018400000000000003,
      "loss": 1.0347,
      "step": 1120
    },
    {
      "epoch": 0.17383946771278028,
      "grad_norm": 0.48172661662101746,
      "learning_rate": 0.00018365811965811968,
      "loss": 1.0622,
      "step": 1130
    },
    {
      "epoch": 0.17537787008191993,
      "grad_norm": 0.5767273306846619,
      "learning_rate": 0.00018331623931623934,
      "loss": 1.0836,
      "step": 1140
    },
    {
      "epoch": 0.17691627245105956,
      "grad_norm": 0.5713164806365967,
      "learning_rate": 0.00018297435897435897,
      "loss": 1.0415,
      "step": 1150
    },
    {
      "epoch": 0.17845467482019922,
      "grad_norm": 0.7022517919540405,
      "learning_rate": 0.00018263247863247863,
      "loss": 1.0547,
      "step": 1160
    },
    {
      "epoch": 0.17999307718933888,
      "grad_norm": 0.5203202962875366,
      "learning_rate": 0.0001822905982905983,
      "loss": 1.0455,
      "step": 1170
    },
    {
      "epoch": 0.1815314795584785,
      "grad_norm": 0.6131396293640137,
      "learning_rate": 0.00018194871794871795,
      "loss": 1.1274,
      "step": 1180
    },
    {
      "epoch": 0.18306988192761817,
      "grad_norm": 0.4761025011539459,
      "learning_rate": 0.0001816068376068376,
      "loss": 1.0878,
      "step": 1190
    },
    {
      "epoch": 0.18460828429675782,
      "grad_norm": 0.5496890544891357,
      "learning_rate": 0.00018126495726495726,
      "loss": 1.0284,
      "step": 1200
    },
    {
      "epoch": 0.18614668666589745,
      "grad_norm": 0.5592815279960632,
      "learning_rate": 0.00018092307692307692,
      "loss": 1.0159,
      "step": 1210
    },
    {
      "epoch": 0.1876850890350371,
      "grad_norm": 0.4768582284450531,
      "learning_rate": 0.00018058119658119658,
      "loss": 1.005,
      "step": 1220
    },
    {
      "epoch": 0.18922349140417677,
      "grad_norm": 0.46654272079467773,
      "learning_rate": 0.00018023931623931623,
      "loss": 1.0125,
      "step": 1230
    },
    {
      "epoch": 0.1907618937733164,
      "grad_norm": 0.46942490339279175,
      "learning_rate": 0.00017989743589743592,
      "loss": 1.0969,
      "step": 1240
    },
    {
      "epoch": 0.19230029614245606,
      "grad_norm": 0.4688929319381714,
      "learning_rate": 0.00017955555555555558,
      "loss": 0.9848,
      "step": 1250
    },
    {
      "epoch": 0.19383869851159571,
      "grad_norm": 0.4767244756221771,
      "learning_rate": 0.00017921367521367523,
      "loss": 1.0352,
      "step": 1260
    },
    {
      "epoch": 0.19537710088073534,
      "grad_norm": 0.51897794008255,
      "learning_rate": 0.0001788717948717949,
      "loss": 1.0773,
      "step": 1270
    },
    {
      "epoch": 0.196915503249875,
      "grad_norm": 0.5375524759292603,
      "learning_rate": 0.00017852991452991455,
      "loss": 1.0726,
      "step": 1280
    },
    {
      "epoch": 0.19845390561901466,
      "grad_norm": 0.4755353033542633,
      "learning_rate": 0.0001781880341880342,
      "loss": 1.0804,
      "step": 1290
    },
    {
      "epoch": 0.1999923079881543,
      "grad_norm": 0.534662127494812,
      "learning_rate": 0.00017784615384615387,
      "loss": 1.0072,
      "step": 1300
    },
    {
      "epoch": 0.20153071035729395,
      "grad_norm": 0.4839552044868469,
      "learning_rate": 0.00017750427350427352,
      "loss": 1.003,
      "step": 1310
    },
    {
      "epoch": 0.2030691127264336,
      "grad_norm": 0.5820987820625305,
      "learning_rate": 0.00017716239316239318,
      "loss": 1.0435,
      "step": 1320
    },
    {
      "epoch": 0.20460751509557323,
      "grad_norm": 0.6065599322319031,
      "learning_rate": 0.0001768205128205128,
      "loss": 1.0177,
      "step": 1330
    },
    {
      "epoch": 0.2061459174647129,
      "grad_norm": 0.5341425538063049,
      "learning_rate": 0.00017647863247863247,
      "loss": 1.0608,
      "step": 1340
    },
    {
      "epoch": 0.20768431983385255,
      "grad_norm": 0.7015015482902527,
      "learning_rate": 0.00017613675213675213,
      "loss": 1.0008,
      "step": 1350
    },
    {
      "epoch": 0.2092227222029922,
      "grad_norm": 0.7774748802185059,
      "learning_rate": 0.00017579487179487178,
      "loss": 1.0223,
      "step": 1360
    },
    {
      "epoch": 0.21076112457213184,
      "grad_norm": 0.4953027069568634,
      "learning_rate": 0.00017545299145299144,
      "loss": 1.0264,
      "step": 1370
    },
    {
      "epoch": 0.2122995269412715,
      "grad_norm": 0.3876659870147705,
      "learning_rate": 0.00017511111111111113,
      "loss": 1.0861,
      "step": 1380
    },
    {
      "epoch": 0.21383792931041115,
      "grad_norm": 0.4554290771484375,
      "learning_rate": 0.00017476923076923078,
      "loss": 1.0197,
      "step": 1390
    },
    {
      "epoch": 0.21537633167955078,
      "grad_norm": 0.47685301303863525,
      "learning_rate": 0.00017442735042735044,
      "loss": 1.0574,
      "step": 1400
    },
    {
      "epoch": 0.21691473404869044,
      "grad_norm": 0.5016463994979858,
      "learning_rate": 0.0001740854700854701,
      "loss": 1.0582,
      "step": 1410
    },
    {
      "epoch": 0.2184531364178301,
      "grad_norm": 0.48101872205734253,
      "learning_rate": 0.00017374358974358976,
      "loss": 1.0172,
      "step": 1420
    },
    {
      "epoch": 0.21999153878696973,
      "grad_norm": 0.47797712683677673,
      "learning_rate": 0.00017340170940170942,
      "loss": 1.0505,
      "step": 1430
    },
    {
      "epoch": 0.22152994115610938,
      "grad_norm": 0.5501773357391357,
      "learning_rate": 0.00017305982905982907,
      "loss": 1.019,
      "step": 1440
    },
    {
      "epoch": 0.22306834352524904,
      "grad_norm": 0.4892399311065674,
      "learning_rate": 0.00017271794871794873,
      "loss": 1.0413,
      "step": 1450
    },
    {
      "epoch": 0.22460674589438867,
      "grad_norm": 0.49763554334640503,
      "learning_rate": 0.0001723760683760684,
      "loss": 1.0176,
      "step": 1460
    },
    {
      "epoch": 0.22614514826352833,
      "grad_norm": 0.4826153516769409,
      "learning_rate": 0.00017203418803418805,
      "loss": 0.9775,
      "step": 1470
    },
    {
      "epoch": 0.22768355063266799,
      "grad_norm": 0.4612135589122772,
      "learning_rate": 0.0001716923076923077,
      "loss": 1.0594,
      "step": 1480
    },
    {
      "epoch": 0.22922195300180762,
      "grad_norm": 0.4901314973831177,
      "learning_rate": 0.00017135042735042736,
      "loss": 1.0009,
      "step": 1490
    },
    {
      "epoch": 0.23076035537094727,
      "grad_norm": 0.589230477809906,
      "learning_rate": 0.00017100854700854702,
      "loss": 0.9804,
      "step": 1500
    },
    {
      "epoch": 0.23229875774008693,
      "grad_norm": 0.5508666634559631,
      "learning_rate": 0.00017066666666666668,
      "loss": 0.9797,
      "step": 1510
    },
    {
      "epoch": 0.23383716010922656,
      "grad_norm": 0.4979475736618042,
      "learning_rate": 0.00017032478632478633,
      "loss": 0.9843,
      "step": 1520
    },
    {
      "epoch": 0.23537556247836622,
      "grad_norm": 0.6265875101089478,
      "learning_rate": 0.000169982905982906,
      "loss": 0.9785,
      "step": 1530
    },
    {
      "epoch": 0.23691396484750588,
      "grad_norm": 0.593346118927002,
      "learning_rate": 0.00016964102564102565,
      "loss": 1.0834,
      "step": 1540
    },
    {
      "epoch": 0.2384523672166455,
      "grad_norm": 0.4974205195903778,
      "learning_rate": 0.0001692991452991453,
      "loss": 1.0233,
      "step": 1550
    },
    {
      "epoch": 0.23999076958578516,
      "grad_norm": 0.4518866539001465,
      "learning_rate": 0.00016895726495726497,
      "loss": 1.0232,
      "step": 1560
    },
    {
      "epoch": 0.24152917195492482,
      "grad_norm": 0.8069553375244141,
      "learning_rate": 0.00016861538461538462,
      "loss": 1.0455,
      "step": 1570
    },
    {
      "epoch": 0.24306757432406445,
      "grad_norm": 0.432888388633728,
      "learning_rate": 0.00016827350427350428,
      "loss": 1.0586,
      "step": 1580
    },
    {
      "epoch": 0.2446059766932041,
      "grad_norm": 0.44304749369621277,
      "learning_rate": 0.00016793162393162394,
      "loss": 1.0687,
      "step": 1590
    },
    {
      "epoch": 0.24614437906234377,
      "grad_norm": 0.5872960686683655,
      "learning_rate": 0.0001675897435897436,
      "loss": 0.9978,
      "step": 1600
    },
    {
      "epoch": 0.2476827814314834,
      "grad_norm": 1.649830937385559,
      "learning_rate": 0.00016724786324786325,
      "loss": 1.091,
      "step": 1610
    },
    {
      "epoch": 0.24922118380062305,
      "grad_norm": 0.566842257976532,
      "learning_rate": 0.0001669059829059829,
      "loss": 0.9668,
      "step": 1620
    },
    {
      "epoch": 0.2507595861697627,
      "grad_norm": 0.5304808616638184,
      "learning_rate": 0.00016656410256410257,
      "loss": 1.0779,
      "step": 1630
    },
    {
      "epoch": 0.25229798853890234,
      "grad_norm": 0.5211077332496643,
      "learning_rate": 0.00016622222222222223,
      "loss": 1.0482,
      "step": 1640
    },
    {
      "epoch": 0.253836390908042,
      "grad_norm": 0.5898643732070923,
      "learning_rate": 0.00016588034188034188,
      "loss": 0.9736,
      "step": 1650
    },
    {
      "epoch": 0.25537479327718166,
      "grad_norm": 0.438373327255249,
      "learning_rate": 0.00016553846153846154,
      "loss": 0.978,
      "step": 1660
    },
    {
      "epoch": 0.2569131956463213,
      "grad_norm": 0.5471181273460388,
      "learning_rate": 0.0001651965811965812,
      "loss": 1.079,
      "step": 1670
    },
    {
      "epoch": 0.25845159801546097,
      "grad_norm": 0.4805585741996765,
      "learning_rate": 0.00016485470085470088,
      "loss": 1.0538,
      "step": 1680
    },
    {
      "epoch": 0.2599900003846006,
      "grad_norm": 0.5803124904632568,
      "learning_rate": 0.00016451282051282054,
      "loss": 1.096,
      "step": 1690
    },
    {
      "epoch": 0.26152840275374023,
      "grad_norm": 0.5438754558563232,
      "learning_rate": 0.00016417094017094017,
      "loss": 0.9875,
      "step": 1700
    },
    {
      "epoch": 0.2630668051228799,
      "grad_norm": 0.4743230938911438,
      "learning_rate": 0.00016382905982905983,
      "loss": 0.9778,
      "step": 1710
    },
    {
      "epoch": 0.26460520749201955,
      "grad_norm": 0.48273923993110657,
      "learning_rate": 0.0001634871794871795,
      "loss": 1.0281,
      "step": 1720
    },
    {
      "epoch": 0.2661436098611592,
      "grad_norm": 0.456667959690094,
      "learning_rate": 0.00016314529914529915,
      "loss": 0.9784,
      "step": 1730
    },
    {
      "epoch": 0.26768201223029886,
      "grad_norm": 0.5305406451225281,
      "learning_rate": 0.0001628034188034188,
      "loss": 0.9641,
      "step": 1740
    },
    {
      "epoch": 0.26922041459943846,
      "grad_norm": 0.521521270275116,
      "learning_rate": 0.00016246153846153846,
      "loss": 0.9471,
      "step": 1750
    },
    {
      "epoch": 0.2707588169685781,
      "grad_norm": 0.5061790943145752,
      "learning_rate": 0.00016211965811965812,
      "loss": 1.0047,
      "step": 1760
    },
    {
      "epoch": 0.2722972193377178,
      "grad_norm": 0.45628097653388977,
      "learning_rate": 0.00016177777777777778,
      "loss": 0.9981,
      "step": 1770
    },
    {
      "epoch": 0.27383562170685743,
      "grad_norm": 0.5362868905067444,
      "learning_rate": 0.00016143589743589743,
      "loss": 1.0533,
      "step": 1780
    },
    {
      "epoch": 0.2753740240759971,
      "grad_norm": 0.475908488035202,
      "learning_rate": 0.0001610940170940171,
      "loss": 1.0608,
      "step": 1790
    },
    {
      "epoch": 0.27691242644513675,
      "grad_norm": 0.517157256603241,
      "learning_rate": 0.00016075213675213675,
      "loss": 1.0611,
      "step": 1800
    },
    {
      "epoch": 0.27845082881427635,
      "grad_norm": 0.4431826174259186,
      "learning_rate": 0.0001604102564102564,
      "loss": 1.0208,
      "step": 1810
    },
    {
      "epoch": 0.279989231183416,
      "grad_norm": 0.4075940251350403,
      "learning_rate": 0.0001600683760683761,
      "loss": 1.0293,
      "step": 1820
    },
    {
      "epoch": 0.28152763355255567,
      "grad_norm": 0.4613097608089447,
      "learning_rate": 0.00015972649572649575,
      "loss": 1.0069,
      "step": 1830
    },
    {
      "epoch": 0.2830660359216953,
      "grad_norm": 0.4482089877128601,
      "learning_rate": 0.0001593846153846154,
      "loss": 0.9918,
      "step": 1840
    },
    {
      "epoch": 0.284604438290835,
      "grad_norm": 0.4703086018562317,
      "learning_rate": 0.00015904273504273507,
      "loss": 1.0441,
      "step": 1850
    },
    {
      "epoch": 0.28614284065997464,
      "grad_norm": 0.45097023248672485,
      "learning_rate": 0.00015870085470085472,
      "loss": 1.0661,
      "step": 1860
    },
    {
      "epoch": 0.28768124302911424,
      "grad_norm": 0.515642523765564,
      "learning_rate": 0.00015835897435897438,
      "loss": 1.0211,
      "step": 1870
    },
    {
      "epoch": 0.2892196453982539,
      "grad_norm": 0.4621179699897766,
      "learning_rate": 0.000158017094017094,
      "loss": 1.0384,
      "step": 1880
    },
    {
      "epoch": 0.29075804776739356,
      "grad_norm": 0.5528222322463989,
      "learning_rate": 0.00015767521367521367,
      "loss": 1.0289,
      "step": 1890
    },
    {
      "epoch": 0.2922964501365332,
      "grad_norm": 0.4765111804008484,
      "learning_rate": 0.00015733333333333333,
      "loss": 1.0358,
      "step": 1900
    },
    {
      "epoch": 0.29383485250567287,
      "grad_norm": 0.44668295979499817,
      "learning_rate": 0.00015699145299145298,
      "loss": 1.0387,
      "step": 1910
    },
    {
      "epoch": 0.29537325487481253,
      "grad_norm": 0.5040528178215027,
      "learning_rate": 0.00015664957264957264,
      "loss": 1.0139,
      "step": 1920
    },
    {
      "epoch": 0.29691165724395213,
      "grad_norm": 0.4558364450931549,
      "learning_rate": 0.0001563076923076923,
      "loss": 1.0437,
      "step": 1930
    },
    {
      "epoch": 0.2984500596130918,
      "grad_norm": 0.45144742727279663,
      "learning_rate": 0.00015596581196581196,
      "loss": 1.0264,
      "step": 1940
    },
    {
      "epoch": 0.29998846198223145,
      "grad_norm": 0.5601553320884705,
      "learning_rate": 0.00015562393162393164,
      "loss": 0.9812,
      "step": 1950
    },
    {
      "epoch": 0.3015268643513711,
      "grad_norm": 0.4512892961502075,
      "learning_rate": 0.0001552820512820513,
      "loss": 1.0399,
      "step": 1960
    },
    {
      "epoch": 0.30306526672051076,
      "grad_norm": 0.4634018838405609,
      "learning_rate": 0.00015494017094017096,
      "loss": 0.9926,
      "step": 1970
    },
    {
      "epoch": 0.3046036690896504,
      "grad_norm": 0.46779969334602356,
      "learning_rate": 0.00015459829059829062,
      "loss": 1.0136,
      "step": 1980
    },
    {
      "epoch": 0.30614207145879,
      "grad_norm": 0.46477067470550537,
      "learning_rate": 0.00015425641025641027,
      "loss": 1.1196,
      "step": 1990
    },
    {
      "epoch": 0.3076804738279297,
      "grad_norm": 0.4696638286113739,
      "learning_rate": 0.00015391452991452993,
      "loss": 1.0334,
      "step": 2000
    },
    {
      "epoch": 0.30921887619706934,
      "grad_norm": 0.452246755361557,
      "learning_rate": 0.0001535726495726496,
      "loss": 1.065,
      "step": 2010
    },
    {
      "epoch": 0.310757278566209,
      "grad_norm": 0.45645472407341003,
      "learning_rate": 0.00015323076923076925,
      "loss": 1.0647,
      "step": 2020
    },
    {
      "epoch": 0.31229568093534865,
      "grad_norm": 0.43664100766181946,
      "learning_rate": 0.0001528888888888889,
      "loss": 1.0037,
      "step": 2030
    },
    {
      "epoch": 0.3138340833044883,
      "grad_norm": 0.44121554493904114,
      "learning_rate": 0.00015254700854700856,
      "loss": 1.0471,
      "step": 2040
    },
    {
      "epoch": 0.3153724856736279,
      "grad_norm": 0.4761958122253418,
      "learning_rate": 0.00015220512820512822,
      "loss": 1.0821,
      "step": 2050
    },
    {
      "epoch": 0.31691088804276757,
      "grad_norm": 0.6178991794586182,
      "learning_rate": 0.00015186324786324785,
      "loss": 1.0819,
      "step": 2060
    },
    {
      "epoch": 0.3184492904119072,
      "grad_norm": 0.4778635799884796,
      "learning_rate": 0.0001515213675213675,
      "loss": 0.9992,
      "step": 2070
    },
    {
      "epoch": 0.3199876927810469,
      "grad_norm": 0.5040847063064575,
      "learning_rate": 0.00015117948717948717,
      "loss": 0.9499,
      "step": 2080
    },
    {
      "epoch": 0.32152609515018654,
      "grad_norm": 0.5121925473213196,
      "learning_rate": 0.00015083760683760685,
      "loss": 1.0061,
      "step": 2090
    },
    {
      "epoch": 0.3230644975193262,
      "grad_norm": 0.4523119330406189,
      "learning_rate": 0.0001504957264957265,
      "loss": 1.0362,
      "step": 2100
    },
    {
      "epoch": 0.3246028998884658,
      "grad_norm": 0.42942947149276733,
      "learning_rate": 0.00015015384615384617,
      "loss": 1.0222,
      "step": 2110
    },
    {
      "epoch": 0.32614130225760546,
      "grad_norm": 0.512516975402832,
      "learning_rate": 0.00014981196581196582,
      "loss": 0.9766,
      "step": 2120
    },
    {
      "epoch": 0.3276797046267451,
      "grad_norm": 0.4922696352005005,
      "learning_rate": 0.00014947008547008548,
      "loss": 1.0475,
      "step": 2130
    },
    {
      "epoch": 0.3292181069958848,
      "grad_norm": 0.4694203734397888,
      "learning_rate": 0.00014912820512820514,
      "loss": 1.0292,
      "step": 2140
    },
    {
      "epoch": 0.33075650936502443,
      "grad_norm": 0.5093258023262024,
      "learning_rate": 0.0001487863247863248,
      "loss": 1.0582,
      "step": 2150
    },
    {
      "epoch": 0.3322949117341641,
      "grad_norm": 0.46809127926826477,
      "learning_rate": 0.00014844444444444445,
      "loss": 1.0181,
      "step": 2160
    },
    {
      "epoch": 0.33383331410330375,
      "grad_norm": 0.43712982535362244,
      "learning_rate": 0.0001481025641025641,
      "loss": 1.02,
      "step": 2170
    },
    {
      "epoch": 0.33537171647244335,
      "grad_norm": 0.5161628723144531,
      "learning_rate": 0.00014776068376068377,
      "loss": 1.075,
      "step": 2180
    },
    {
      "epoch": 0.336910118841583,
      "grad_norm": 0.4963924288749695,
      "learning_rate": 0.00014741880341880343,
      "loss": 1.0453,
      "step": 2190
    },
    {
      "epoch": 0.33844852121072266,
      "grad_norm": 0.44669291377067566,
      "learning_rate": 0.00014707692307692308,
      "loss": 0.9485,
      "step": 2200
    },
    {
      "epoch": 0.3399869235798623,
      "grad_norm": 0.4726347029209137,
      "learning_rate": 0.00014673504273504274,
      "loss": 1.0377,
      "step": 2210
    },
    {
      "epoch": 0.341525325949002,
      "grad_norm": 0.38600215315818787,
      "learning_rate": 0.0001463931623931624,
      "loss": 1.0553,
      "step": 2220
    },
    {
      "epoch": 0.34306372831814164,
      "grad_norm": 0.47462794184684753,
      "learning_rate": 0.00014605128205128206,
      "loss": 0.9994,
      "step": 2230
    },
    {
      "epoch": 0.34460213068728124,
      "grad_norm": 0.4465763568878174,
      "learning_rate": 0.00014570940170940172,
      "loss": 0.9951,
      "step": 2240
    },
    {
      "epoch": 0.3461405330564209,
      "grad_norm": 0.4504924714565277,
      "learning_rate": 0.00014536752136752137,
      "loss": 1.0283,
      "step": 2250
    },
    {
      "epoch": 0.34767893542556055,
      "grad_norm": 0.4634293019771576,
      "learning_rate": 0.00014502564102564103,
      "loss": 0.9875,
      "step": 2260
    },
    {
      "epoch": 0.3492173377947002,
      "grad_norm": 0.4401499629020691,
      "learning_rate": 0.0001446837606837607,
      "loss": 1.0025,
      "step": 2270
    },
    {
      "epoch": 0.35075574016383987,
      "grad_norm": 0.5444926619529724,
      "learning_rate": 0.00014434188034188035,
      "loss": 1.0173,
      "step": 2280
    },
    {
      "epoch": 0.3522941425329795,
      "grad_norm": 0.4763002395629883,
      "learning_rate": 0.000144,
      "loss": 0.9704,
      "step": 2290
    },
    {
      "epoch": 0.35383254490211913,
      "grad_norm": 0.5075652599334717,
      "learning_rate": 0.00014365811965811966,
      "loss": 1.008,
      "step": 2300
    },
    {
      "epoch": 0.3553709472712588,
      "grad_norm": 0.489330917596817,
      "learning_rate": 0.00014331623931623932,
      "loss": 0.9758,
      "step": 2310
    },
    {
      "epoch": 0.35690934964039844,
      "grad_norm": 0.4513956308364868,
      "learning_rate": 0.00014297435897435898,
      "loss": 1.0316,
      "step": 2320
    },
    {
      "epoch": 0.3584477520095381,
      "grad_norm": 0.4388315975666046,
      "learning_rate": 0.00014263247863247863,
      "loss": 1.063,
      "step": 2330
    },
    {
      "epoch": 0.35998615437867776,
      "grad_norm": 0.5001503229141235,
      "learning_rate": 0.0001422905982905983,
      "loss": 1.0446,
      "step": 2340
    },
    {
      "epoch": 0.3615245567478174,
      "grad_norm": 0.5730881690979004,
      "learning_rate": 0.00014194871794871795,
      "loss": 0.9892,
      "step": 2350
    },
    {
      "epoch": 0.363062959116957,
      "grad_norm": 0.56324702501297,
      "learning_rate": 0.0001416068376068376,
      "loss": 1.0799,
      "step": 2360
    },
    {
      "epoch": 0.3646013614860967,
      "grad_norm": 0.47002285718917847,
      "learning_rate": 0.00014126495726495727,
      "loss": 1.0665,
      "step": 2370
    },
    {
      "epoch": 0.36613976385523633,
      "grad_norm": 0.551929771900177,
      "learning_rate": 0.00014092307692307692,
      "loss": 1.0239,
      "step": 2380
    },
    {
      "epoch": 0.367678166224376,
      "grad_norm": 0.5911964178085327,
      "learning_rate": 0.0001405811965811966,
      "loss": 1.0258,
      "step": 2390
    },
    {
      "epoch": 0.36921656859351565,
      "grad_norm": 0.5289866924285889,
      "learning_rate": 0.00014023931623931627,
      "loss": 0.9667,
      "step": 2400
    },
    {
      "epoch": 0.3707549709626553,
      "grad_norm": 0.4729161262512207,
      "learning_rate": 0.00013989743589743592,
      "loss": 1.0122,
      "step": 2410
    },
    {
      "epoch": 0.3722933733317949,
      "grad_norm": 0.426726758480072,
      "learning_rate": 0.00013955555555555558,
      "loss": 0.9985,
      "step": 2420
    },
    {
      "epoch": 0.37383177570093457,
      "grad_norm": 0.526434063911438,
      "learning_rate": 0.0001392136752136752,
      "loss": 0.9952,
      "step": 2430
    },
    {
      "epoch": 0.3753701780700742,
      "grad_norm": 0.6388368606567383,
      "learning_rate": 0.00013887179487179487,
      "loss": 1.017,
      "step": 2440
    },
    {
      "epoch": 0.3769085804392139,
      "grad_norm": 0.4309264123439789,
      "learning_rate": 0.00013852991452991453,
      "loss": 0.9927,
      "step": 2450
    },
    {
      "epoch": 0.37844698280835354,
      "grad_norm": 0.48485633730888367,
      "learning_rate": 0.00013818803418803418,
      "loss": 1.0217,
      "step": 2460
    },
    {
      "epoch": 0.3799853851774932,
      "grad_norm": 0.482890784740448,
      "learning_rate": 0.00013784615384615384,
      "loss": 0.9824,
      "step": 2470
    },
    {
      "epoch": 0.3815237875466328,
      "grad_norm": 0.4928055703639984,
      "learning_rate": 0.0001375042735042735,
      "loss": 1.0613,
      "step": 2480
    },
    {
      "epoch": 0.38306218991577246,
      "grad_norm": 0.5213229060173035,
      "learning_rate": 0.00013716239316239316,
      "loss": 1.0067,
      "step": 2490
    },
    {
      "epoch": 0.3846005922849121,
      "grad_norm": 0.46435466408729553,
      "learning_rate": 0.00013682051282051282,
      "loss": 0.993,
      "step": 2500
    },
    {
      "epoch": 0.38613899465405177,
      "grad_norm": 0.5699139833450317,
      "learning_rate": 0.00013647863247863247,
      "loss": 1.0204,
      "step": 2510
    },
    {
      "epoch": 0.38767739702319143,
      "grad_norm": 0.45603734254837036,
      "learning_rate": 0.00013613675213675213,
      "loss": 1.0268,
      "step": 2520
    },
    {
      "epoch": 0.3892157993923311,
      "grad_norm": 0.5721235871315002,
      "learning_rate": 0.00013579487179487182,
      "loss": 0.9732,
      "step": 2530
    },
    {
      "epoch": 0.3907542017614707,
      "grad_norm": 0.4519679844379425,
      "learning_rate": 0.00013545299145299147,
      "loss": 1.0198,
      "step": 2540
    },
    {
      "epoch": 0.39229260413061035,
      "grad_norm": 0.49018147587776184,
      "learning_rate": 0.00013511111111111113,
      "loss": 1.0004,
      "step": 2550
    },
    {
      "epoch": 0.39383100649975,
      "grad_norm": 0.4484868049621582,
      "learning_rate": 0.0001347692307692308,
      "loss": 1.012,
      "step": 2560
    },
    {
      "epoch": 0.39536940886888966,
      "grad_norm": 0.4514232575893402,
      "learning_rate": 0.00013442735042735045,
      "loss": 1.0314,
      "step": 2570
    },
    {
      "epoch": 0.3969078112380293,
      "grad_norm": 0.5341078042984009,
      "learning_rate": 0.0001340854700854701,
      "loss": 1.0429,
      "step": 2580
    },
    {
      "epoch": 0.398446213607169,
      "grad_norm": 0.47826820611953735,
      "learning_rate": 0.00013374358974358976,
      "loss": 0.9353,
      "step": 2590
    },
    {
      "epoch": 0.3999846159763086,
      "grad_norm": 0.44649574160575867,
      "learning_rate": 0.00013340170940170942,
      "loss": 1.0336,
      "step": 2600
    },
    {
      "epoch": 0.40152301834544823,
      "grad_norm": 0.5894924998283386,
      "learning_rate": 0.00013305982905982905,
      "loss": 0.9613,
      "step": 2610
    },
    {
      "epoch": 0.4030614207145879,
      "grad_norm": 0.6241974234580994,
      "learning_rate": 0.0001327179487179487,
      "loss": 0.9576,
      "step": 2620
    },
    {
      "epoch": 0.40459982308372755,
      "grad_norm": 0.5636184811592102,
      "learning_rate": 0.00013237606837606836,
      "loss": 1.0198,
      "step": 2630
    },
    {
      "epoch": 0.4061382254528672,
      "grad_norm": 0.4794470965862274,
      "learning_rate": 0.00013203418803418802,
      "loss": 1.0137,
      "step": 2640
    },
    {
      "epoch": 0.40767662782200687,
      "grad_norm": 0.4346396327018738,
      "learning_rate": 0.00013169230769230768,
      "loss": 1.0509,
      "step": 2650
    },
    {
      "epoch": 0.40921503019114647,
      "grad_norm": 0.6024537086486816,
      "learning_rate": 0.00013135042735042737,
      "loss": 0.971,
      "step": 2660
    },
    {
      "epoch": 0.4107534325602861,
      "grad_norm": 0.48606881499290466,
      "learning_rate": 0.00013100854700854702,
      "loss": 0.986,
      "step": 2670
    },
    {
      "epoch": 0.4122918349294258,
      "grad_norm": 0.44571158289909363,
      "learning_rate": 0.00013066666666666668,
      "loss": 1.0989,
      "step": 2680
    },
    {
      "epoch": 0.41383023729856544,
      "grad_norm": 0.4881031811237335,
      "learning_rate": 0.00013032478632478634,
      "loss": 1.0537,
      "step": 2690
    },
    {
      "epoch": 0.4153686396677051,
      "grad_norm": 0.5769477486610413,
      "learning_rate": 0.000129982905982906,
      "loss": 1.0136,
      "step": 2700
    },
    {
      "epoch": 0.41690704203684475,
      "grad_norm": 0.4960319697856903,
      "learning_rate": 0.00012964102564102565,
      "loss": 1.0478,
      "step": 2710
    },
    {
      "epoch": 0.4184454444059844,
      "grad_norm": 0.47399699687957764,
      "learning_rate": 0.0001292991452991453,
      "loss": 1.011,
      "step": 2720
    },
    {
      "epoch": 0.419983846775124,
      "grad_norm": 1.5989514589309692,
      "learning_rate": 0.00012895726495726497,
      "loss": 0.9706,
      "step": 2730
    },
    {
      "epoch": 0.42152224914426367,
      "grad_norm": 0.46955832839012146,
      "learning_rate": 0.00012861538461538463,
      "loss": 1.0276,
      "step": 2740
    },
    {
      "epoch": 0.42306065151340333,
      "grad_norm": 0.48883306980133057,
      "learning_rate": 0.00012827350427350428,
      "loss": 1.044,
      "step": 2750
    },
    {
      "epoch": 0.424599053882543,
      "grad_norm": 0.5509262681007385,
      "learning_rate": 0.00012793162393162394,
      "loss": 1.0287,
      "step": 2760
    },
    {
      "epoch": 0.42613745625168264,
      "grad_norm": 0.49244505167007446,
      "learning_rate": 0.0001275897435897436,
      "loss": 1.0132,
      "step": 2770
    },
    {
      "epoch": 0.4276758586208223,
      "grad_norm": 0.5097609758377075,
      "learning_rate": 0.00012724786324786326,
      "loss": 1.0386,
      "step": 2780
    },
    {
      "epoch": 0.4292142609899619,
      "grad_norm": 0.4813893437385559,
      "learning_rate": 0.00012690598290598292,
      "loss": 1.0349,
      "step": 2790
    },
    {
      "epoch": 0.43075266335910156,
      "grad_norm": 0.4296726584434509,
      "learning_rate": 0.00012656410256410257,
      "loss": 0.97,
      "step": 2800
    },
    {
      "epoch": 0.4322910657282412,
      "grad_norm": 0.4754703938961029,
      "learning_rate": 0.00012622222222222223,
      "loss": 1.0236,
      "step": 2810
    },
    {
      "epoch": 0.4338294680973809,
      "grad_norm": 0.44898521900177,
      "learning_rate": 0.0001258803418803419,
      "loss": 1.0008,
      "step": 2820
    },
    {
      "epoch": 0.43536787046652053,
      "grad_norm": 0.4491705298423767,
      "learning_rate": 0.00012553846153846155,
      "loss": 0.9552,
      "step": 2830
    },
    {
      "epoch": 0.4369062728356602,
      "grad_norm": 0.4652876853942871,
      "learning_rate": 0.0001251965811965812,
      "loss": 1.0188,
      "step": 2840
    },
    {
      "epoch": 0.4384446752047998,
      "grad_norm": 0.48203080892562866,
      "learning_rate": 0.00012485470085470086,
      "loss": 1.0227,
      "step": 2850
    },
    {
      "epoch": 0.43998307757393945,
      "grad_norm": 0.5408573746681213,
      "learning_rate": 0.00012451282051282052,
      "loss": 1.0383,
      "step": 2860
    },
    {
      "epoch": 0.4415214799430791,
      "grad_norm": 0.7079630494117737,
      "learning_rate": 0.00012417094017094018,
      "loss": 1.0821,
      "step": 2870
    },
    {
      "epoch": 0.44305988231221877,
      "grad_norm": 0.49403104186058044,
      "learning_rate": 0.00012382905982905983,
      "loss": 1.0192,
      "step": 2880
    },
    {
      "epoch": 0.4445982846813584,
      "grad_norm": 0.44623225927352905,
      "learning_rate": 0.0001234871794871795,
      "loss": 1.0238,
      "step": 2890
    },
    {
      "epoch": 0.4461366870504981,
      "grad_norm": 0.4913828670978546,
      "learning_rate": 0.00012314529914529915,
      "loss": 1.0098,
      "step": 2900
    },
    {
      "epoch": 0.4476750894196377,
      "grad_norm": 0.4788702428340912,
      "learning_rate": 0.0001228034188034188,
      "loss": 0.9463,
      "step": 2910
    },
    {
      "epoch": 0.44921349178877734,
      "grad_norm": 0.476877361536026,
      "learning_rate": 0.00012246153846153846,
      "loss": 1.0684,
      "step": 2920
    },
    {
      "epoch": 0.450751894157917,
      "grad_norm": 0.4510871469974518,
      "learning_rate": 0.00012211965811965812,
      "loss": 1.022,
      "step": 2930
    },
    {
      "epoch": 0.45229029652705666,
      "grad_norm": 0.44846615195274353,
      "learning_rate": 0.0001217777777777778,
      "loss": 0.9378,
      "step": 2940
    },
    {
      "epoch": 0.4538286988961963,
      "grad_norm": 0.5603658556938171,
      "learning_rate": 0.00012143589743589745,
      "loss": 1.0696,
      "step": 2950
    },
    {
      "epoch": 0.45536710126533597,
      "grad_norm": 0.4440000355243683,
      "learning_rate": 0.00012109401709401711,
      "loss": 0.9738,
      "step": 2960
    },
    {
      "epoch": 0.4569055036344756,
      "grad_norm": 0.4420330226421356,
      "learning_rate": 0.00012075213675213677,
      "loss": 1.0835,
      "step": 2970
    },
    {
      "epoch": 0.45844390600361523,
      "grad_norm": 0.5237401127815247,
      "learning_rate": 0.00012041025641025641,
      "loss": 1.0565,
      "step": 2980
    },
    {
      "epoch": 0.4599823083727549,
      "grad_norm": 0.47431764006614685,
      "learning_rate": 0.00012006837606837607,
      "loss": 1.0452,
      "step": 2990
    },
    {
      "epoch": 0.46152071074189455,
      "grad_norm": 0.5057913661003113,
      "learning_rate": 0.00011972649572649573,
      "loss": 1.04,
      "step": 3000
    },
    {
      "epoch": 0.4630591131110342,
      "grad_norm": 0.4987821877002716,
      "learning_rate": 0.00011938461538461538,
      "loss": 1.115,
      "step": 3010
    },
    {
      "epoch": 0.46459751548017386,
      "grad_norm": 0.5371751189231873,
      "learning_rate": 0.00011904273504273504,
      "loss": 0.9389,
      "step": 3020
    },
    {
      "epoch": 0.46613591784931346,
      "grad_norm": 0.4830132722854614,
      "learning_rate": 0.0001187008547008547,
      "loss": 0.9976,
      "step": 3030
    },
    {
      "epoch": 0.4676743202184531,
      "grad_norm": 0.45632466673851013,
      "learning_rate": 0.00011835897435897436,
      "loss": 0.9714,
      "step": 3040
    },
    {
      "epoch": 0.4692127225875928,
      "grad_norm": 0.397664874792099,
      "learning_rate": 0.00011801709401709401,
      "loss": 1.0266,
      "step": 3050
    },
    {
      "epoch": 0.47075112495673244,
      "grad_norm": 0.5173747539520264,
      "learning_rate": 0.00011767521367521369,
      "loss": 1.0108,
      "step": 3060
    },
    {
      "epoch": 0.4722895273258721,
      "grad_norm": 0.4755321443080902,
      "learning_rate": 0.00011733333333333334,
      "loss": 1.0571,
      "step": 3070
    },
    {
      "epoch": 0.47382792969501175,
      "grad_norm": 0.489154577255249,
      "learning_rate": 0.000116991452991453,
      "loss": 0.9252,
      "step": 3080
    },
    {
      "epoch": 0.47536633206415135,
      "grad_norm": 0.44085070490837097,
      "learning_rate": 0.00011664957264957266,
      "loss": 1.015,
      "step": 3090
    },
    {
      "epoch": 0.476904734433291,
      "grad_norm": 0.5117154717445374,
      "learning_rate": 0.00011630769230769232,
      "loss": 1.0788,
      "step": 3100
    },
    {
      "epoch": 0.47844313680243067,
      "grad_norm": 0.5192663073539734,
      "learning_rate": 0.00011596581196581197,
      "loss": 1.0173,
      "step": 3110
    },
    {
      "epoch": 0.4799815391715703,
      "grad_norm": 0.4797329604625702,
      "learning_rate": 0.00011562393162393163,
      "loss": 1.0285,
      "step": 3120
    },
    {
      "epoch": 0.48151994154071,
      "grad_norm": 0.4516552686691284,
      "learning_rate": 0.00011528205128205129,
      "loss": 1.0534,
      "step": 3130
    },
    {
      "epoch": 0.48305834390984964,
      "grad_norm": 0.3859146237373352,
      "learning_rate": 0.00011494017094017096,
      "loss": 0.9954,
      "step": 3140
    },
    {
      "epoch": 0.48459674627898924,
      "grad_norm": 0.43039536476135254,
      "learning_rate": 0.00011459829059829062,
      "loss": 1.0256,
      "step": 3150
    },
    {
      "epoch": 0.4861351486481289,
      "grad_norm": 0.4785546362400055,
      "learning_rate": 0.00011425641025641025,
      "loss": 0.9854,
      "step": 3160
    },
    {
      "epoch": 0.48767355101726856,
      "grad_norm": 0.6272962093353271,
      "learning_rate": 0.00011391452991452991,
      "loss": 0.9883,
      "step": 3170
    },
    {
      "epoch": 0.4892119533864082,
      "grad_norm": 0.5059628486633301,
      "learning_rate": 0.00011357264957264956,
      "loss": 0.9654,
      "step": 3180
    },
    {
      "epoch": 0.4907503557555479,
      "grad_norm": 0.4502537250518799,
      "learning_rate": 0.00011323076923076922,
      "loss": 0.9219,
      "step": 3190
    },
    {
      "epoch": 0.49228875812468753,
      "grad_norm": 0.5902127027511597,
      "learning_rate": 0.0001128888888888889,
      "loss": 0.9966,
      "step": 3200
    },
    {
      "epoch": 0.49382716049382713,
      "grad_norm": 0.49637946486473083,
      "learning_rate": 0.00011254700854700855,
      "loss": 0.9747,
      "step": 3210
    },
    {
      "epoch": 0.4953655628629668,
      "grad_norm": 0.4538553059101105,
      "learning_rate": 0.00011220512820512821,
      "loss": 1.0122,
      "step": 3220
    },
    {
      "epoch": 0.49690396523210645,
      "grad_norm": 0.4777303636074066,
      "learning_rate": 0.00011186324786324787,
      "loss": 1.0046,
      "step": 3230
    },
    {
      "epoch": 0.4984423676012461,
      "grad_norm": 0.4581485986709595,
      "learning_rate": 0.00011152136752136752,
      "loss": 1.0116,
      "step": 3240
    },
    {
      "epoch": 0.49998076997038576,
      "grad_norm": 0.45275676250457764,
      "learning_rate": 0.00011117948717948718,
      "loss": 1.0269,
      "step": 3250
    },
    {
      "epoch": 0.5015191723395254,
      "grad_norm": 1.0057971477508545,
      "learning_rate": 0.00011083760683760684,
      "loss": 1.0592,
      "step": 3260
    },
    {
      "epoch": 0.5030575747086651,
      "grad_norm": 0.433308869600296,
      "learning_rate": 0.0001104957264957265,
      "loss": 1.0466,
      "step": 3270
    },
    {
      "epoch": 0.5045959770778047,
      "grad_norm": 0.6959103345870972,
      "learning_rate": 0.00011015384615384617,
      "loss": 1.0653,
      "step": 3280
    },
    {
      "epoch": 0.5061343794469444,
      "grad_norm": 0.4877837300300598,
      "learning_rate": 0.00010981196581196583,
      "loss": 1.0481,
      "step": 3290
    },
    {
      "epoch": 0.507672781816084,
      "grad_norm": 0.4811130464076996,
      "learning_rate": 0.00010947008547008548,
      "loss": 1.0379,
      "step": 3300
    },
    {
      "epoch": 0.5092111841852236,
      "grad_norm": 0.4736965000629425,
      "learning_rate": 0.00010912820512820514,
      "loss": 0.9726,
      "step": 3310
    },
    {
      "epoch": 0.5107495865543633,
      "grad_norm": 0.4696624279022217,
      "learning_rate": 0.0001087863247863248,
      "loss": 1.0257,
      "step": 3320
    },
    {
      "epoch": 0.5122879889235029,
      "grad_norm": 0.42184942960739136,
      "learning_rate": 0.00010844444444444446,
      "loss": 0.97,
      "step": 3330
    },
    {
      "epoch": 0.5138263912926426,
      "grad_norm": 0.4114851951599121,
      "learning_rate": 0.0001081025641025641,
      "loss": 1.0316,
      "step": 3340
    },
    {
      "epoch": 0.5153647936617822,
      "grad_norm": 0.536302387714386,
      "learning_rate": 0.00010776068376068376,
      "loss": 0.9849,
      "step": 3350
    },
    {
      "epoch": 0.5169031960309219,
      "grad_norm": 0.47962480783462524,
      "learning_rate": 0.00010741880341880342,
      "loss": 0.9854,
      "step": 3360
    },
    {
      "epoch": 0.5184415984000615,
      "grad_norm": 0.5005306005477905,
      "learning_rate": 0.00010707692307692307,
      "loss": 1.0238,
      "step": 3370
    },
    {
      "epoch": 0.5199800007692011,
      "grad_norm": 0.46713271737098694,
      "learning_rate": 0.00010673504273504273,
      "loss": 1.0486,
      "step": 3380
    },
    {
      "epoch": 0.5215184031383409,
      "grad_norm": 0.47879284620285034,
      "learning_rate": 0.00010639316239316239,
      "loss": 0.9914,
      "step": 3390
    },
    {
      "epoch": 0.5230568055074805,
      "grad_norm": 0.7694814205169678,
      "learning_rate": 0.00010605128205128205,
      "loss": 1.0241,
      "step": 3400
    },
    {
      "epoch": 0.5245952078766202,
      "grad_norm": 0.48376962542533875,
      "learning_rate": 0.00010570940170940172,
      "loss": 1.0161,
      "step": 3410
    },
    {
      "epoch": 0.5261336102457598,
      "grad_norm": 0.5859001278877258,
      "learning_rate": 0.00010536752136752138,
      "loss": 1.0533,
      "step": 3420
    },
    {
      "epoch": 0.5276720126148994,
      "grad_norm": 0.45253825187683105,
      "learning_rate": 0.00010502564102564103,
      "loss": 1.0521,
      "step": 3430
    },
    {
      "epoch": 0.5292104149840391,
      "grad_norm": 0.46115732192993164,
      "learning_rate": 0.00010468376068376069,
      "loss": 1.0543,
      "step": 3440
    },
    {
      "epoch": 0.5307488173531787,
      "grad_norm": 0.4528706669807434,
      "learning_rate": 0.00010434188034188035,
      "loss": 1.0578,
      "step": 3450
    },
    {
      "epoch": 0.5322872197223184,
      "grad_norm": 0.45014455914497375,
      "learning_rate": 0.00010400000000000001,
      "loss": 1.057,
      "step": 3460
    },
    {
      "epoch": 0.533825622091458,
      "grad_norm": 0.4986632764339447,
      "learning_rate": 0.00010365811965811966,
      "loss": 0.9669,
      "step": 3470
    },
    {
      "epoch": 0.5353640244605977,
      "grad_norm": 0.4341486692428589,
      "learning_rate": 0.00010331623931623932,
      "loss": 0.9694,
      "step": 3480
    },
    {
      "epoch": 0.5369024268297373,
      "grad_norm": 0.5026548504829407,
      "learning_rate": 0.00010297435897435898,
      "loss": 0.9783,
      "step": 3490
    },
    {
      "epoch": 0.5384408291988769,
      "grad_norm": 0.5059952735900879,
      "learning_rate": 0.00010263247863247865,
      "loss": 1.0209,
      "step": 3500
    },
    {
      "epoch": 0.5399792315680166,
      "grad_norm": 0.5065003633499146,
      "learning_rate": 0.00010229059829059831,
      "loss": 1.0432,
      "step": 3510
    },
    {
      "epoch": 0.5415176339371562,
      "grad_norm": 0.47472190856933594,
      "learning_rate": 0.00010194871794871797,
      "loss": 0.986,
      "step": 3520
    },
    {
      "epoch": 0.543056036306296,
      "grad_norm": 0.5457119941711426,
      "learning_rate": 0.0001016068376068376,
      "loss": 1.1381,
      "step": 3530
    },
    {
      "epoch": 0.5445944386754356,
      "grad_norm": 0.5610067844390869,
      "learning_rate": 0.00010126495726495726,
      "loss": 0.9173,
      "step": 3540
    },
    {
      "epoch": 0.5461328410445752,
      "grad_norm": 0.5882203578948975,
      "learning_rate": 0.00010092307692307693,
      "loss": 1.018,
      "step": 3550
    },
    {
      "epoch": 0.5476712434137149,
      "grad_norm": 0.4444960653781891,
      "learning_rate": 0.00010058119658119658,
      "loss": 0.9871,
      "step": 3560
    },
    {
      "epoch": 0.5492096457828545,
      "grad_norm": 0.5150115489959717,
      "learning_rate": 0.00010023931623931624,
      "loss": 0.9607,
      "step": 3570
    },
    {
      "epoch": 0.5507480481519942,
      "grad_norm": 0.5042332410812378,
      "learning_rate": 9.98974358974359e-05,
      "loss": 0.969,
      "step": 3580
    },
    {
      "epoch": 0.5522864505211338,
      "grad_norm": 0.4922487139701843,
      "learning_rate": 9.955555555555556e-05,
      "loss": 1.0209,
      "step": 3590
    },
    {
      "epoch": 0.5538248528902735,
      "grad_norm": 0.4377758204936981,
      "learning_rate": 9.921367521367521e-05,
      "loss": 1.0329,
      "step": 3600
    },
    {
      "epoch": 0.5553632552594131,
      "grad_norm": 0.47488436102867126,
      "learning_rate": 9.887179487179487e-05,
      "loss": 1.0106,
      "step": 3610
    },
    {
      "epoch": 0.5569016576285527,
      "grad_norm": 0.4387638568878174,
      "learning_rate": 9.852991452991453e-05,
      "loss": 1.0284,
      "step": 3620
    },
    {
      "epoch": 0.5584400599976924,
      "grad_norm": 0.49823302030563354,
      "learning_rate": 9.81880341880342e-05,
      "loss": 0.9525,
      "step": 3630
    },
    {
      "epoch": 0.559978462366832,
      "grad_norm": 0.4711962640285492,
      "learning_rate": 9.784615384615386e-05,
      "loss": 0.9833,
      "step": 3640
    },
    {
      "epoch": 0.5615168647359717,
      "grad_norm": 0.5340354442596436,
      "learning_rate": 9.750427350427352e-05,
      "loss": 0.9514,
      "step": 3650
    },
    {
      "epoch": 0.5630552671051113,
      "grad_norm": 0.48217591643333435,
      "learning_rate": 9.716239316239316e-05,
      "loss": 1.0202,
      "step": 3660
    },
    {
      "epoch": 0.5645936694742509,
      "grad_norm": 0.4130433201789856,
      "learning_rate": 9.682051282051282e-05,
      "loss": 1.0399,
      "step": 3670
    },
    {
      "epoch": 0.5661320718433906,
      "grad_norm": 0.4116210341453552,
      "learning_rate": 9.647863247863248e-05,
      "loss": 1.0161,
      "step": 3680
    },
    {
      "epoch": 0.5676704742125303,
      "grad_norm": 0.463244765996933,
      "learning_rate": 9.613675213675213e-05,
      "loss": 0.9964,
      "step": 3690
    },
    {
      "epoch": 0.56920887658167,
      "grad_norm": 0.48553693294525146,
      "learning_rate": 9.57948717948718e-05,
      "loss": 0.9682,
      "step": 3700
    },
    {
      "epoch": 0.5707472789508096,
      "grad_norm": 0.48212671279907227,
      "learning_rate": 9.545299145299146e-05,
      "loss": 0.9729,
      "step": 3710
    },
    {
      "epoch": 0.5722856813199493,
      "grad_norm": 0.46349358558654785,
      "learning_rate": 9.511111111111112e-05,
      "loss": 0.9736,
      "step": 3720
    },
    {
      "epoch": 0.5738240836890889,
      "grad_norm": 0.4420188069343567,
      "learning_rate": 9.476923076923078e-05,
      "loss": 1.0032,
      "step": 3730
    },
    {
      "epoch": 0.5753624860582285,
      "grad_norm": 0.47891440987586975,
      "learning_rate": 9.442735042735044e-05,
      "loss": 0.9971,
      "step": 3740
    },
    {
      "epoch": 0.5769008884273682,
      "grad_norm": 0.45031991600990295,
      "learning_rate": 9.40854700854701e-05,
      "loss": 1.0218,
      "step": 3750
    },
    {
      "epoch": 0.5784392907965078,
      "grad_norm": 0.5158865451812744,
      "learning_rate": 9.374358974358974e-05,
      "loss": 0.9688,
      "step": 3760
    },
    {
      "epoch": 0.5799776931656475,
      "grad_norm": 0.4990372657775879,
      "learning_rate": 9.340170940170941e-05,
      "loss": 0.9991,
      "step": 3770
    },
    {
      "epoch": 0.5815160955347871,
      "grad_norm": 0.4774261713027954,
      "learning_rate": 9.305982905982907e-05,
      "loss": 0.9904,
      "step": 3780
    },
    {
      "epoch": 0.5830544979039268,
      "grad_norm": 0.4630940556526184,
      "learning_rate": 9.271794871794872e-05,
      "loss": 0.9478,
      "step": 3790
    },
    {
      "epoch": 0.5845929002730664,
      "grad_norm": 0.5120342373847961,
      "learning_rate": 9.237606837606838e-05,
      "loss": 1.0655,
      "step": 3800
    },
    {
      "epoch": 0.586131302642206,
      "grad_norm": 0.46336519718170166,
      "learning_rate": 9.203418803418804e-05,
      "loss": 1.0308,
      "step": 3810
    },
    {
      "epoch": 0.5876697050113457,
      "grad_norm": 0.48117130994796753,
      "learning_rate": 9.16923076923077e-05,
      "loss": 1.0034,
      "step": 3820
    },
    {
      "epoch": 0.5892081073804853,
      "grad_norm": 0.4399054944515228,
      "learning_rate": 9.135042735042736e-05,
      "loss": 1.0637,
      "step": 3830
    },
    {
      "epoch": 0.5907465097496251,
      "grad_norm": 0.5473572611808777,
      "learning_rate": 9.100854700854701e-05,
      "loss": 1.0707,
      "step": 3840
    },
    {
      "epoch": 0.5922849121187647,
      "grad_norm": 0.58238685131073,
      "learning_rate": 9.066666666666667e-05,
      "loss": 1.0111,
      "step": 3850
    },
    {
      "epoch": 0.5938233144879043,
      "grad_norm": 0.4877139627933502,
      "learning_rate": 9.032478632478633e-05,
      "loss": 1.048,
      "step": 3860
    },
    {
      "epoch": 0.595361716857044,
      "grad_norm": 0.4608486592769623,
      "learning_rate": 8.998290598290599e-05,
      "loss": 0.9749,
      "step": 3870
    },
    {
      "epoch": 0.5969001192261836,
      "grad_norm": 0.4056682586669922,
      "learning_rate": 8.964102564102564e-05,
      "loss": 1.0087,
      "step": 3880
    },
    {
      "epoch": 0.5984385215953233,
      "grad_norm": 0.4985199570655823,
      "learning_rate": 8.92991452991453e-05,
      "loss": 1.0314,
      "step": 3890
    },
    {
      "epoch": 0.5999769239644629,
      "grad_norm": 0.4461810290813446,
      "learning_rate": 8.895726495726496e-05,
      "loss": 1.009,
      "step": 3900
    },
    {
      "epoch": 0.6015153263336026,
      "grad_norm": 0.46050533652305603,
      "learning_rate": 8.861538461538462e-05,
      "loss": 1.0012,
      "step": 3910
    },
    {
      "epoch": 0.6030537287027422,
      "grad_norm": 0.4470753073692322,
      "learning_rate": 8.827350427350429e-05,
      "loss": 0.999,
      "step": 3920
    },
    {
      "epoch": 0.6045921310718818,
      "grad_norm": 0.49706217646598816,
      "learning_rate": 8.793162393162395e-05,
      "loss": 0.9905,
      "step": 3930
    },
    {
      "epoch": 0.6061305334410215,
      "grad_norm": 0.5830263495445251,
      "learning_rate": 8.758974358974359e-05,
      "loss": 0.9904,
      "step": 3940
    },
    {
      "epoch": 0.6076689358101611,
      "grad_norm": 0.48658502101898193,
      "learning_rate": 8.724786324786325e-05,
      "loss": 1.001,
      "step": 3950
    },
    {
      "epoch": 0.6092073381793008,
      "grad_norm": 0.4904552102088928,
      "learning_rate": 8.69059829059829e-05,
      "loss": 0.9912,
      "step": 3960
    },
    {
      "epoch": 0.6107457405484404,
      "grad_norm": 0.43994224071502686,
      "learning_rate": 8.656410256410256e-05,
      "loss": 1.0496,
      "step": 3970
    },
    {
      "epoch": 0.61228414291758,
      "grad_norm": 0.4462207555770874,
      "learning_rate": 8.622222222222222e-05,
      "loss": 1.0075,
      "step": 3980
    },
    {
      "epoch": 0.6138225452867198,
      "grad_norm": 0.4386101961135864,
      "learning_rate": 8.588034188034189e-05,
      "loss": 1.0376,
      "step": 3990
    },
    {
      "epoch": 0.6153609476558594,
      "grad_norm": 0.41698208451271057,
      "learning_rate": 8.553846153846155e-05,
      "loss": 1.0427,
      "step": 4000
    },
    {
      "epoch": 0.6168993500249991,
      "grad_norm": 0.5510523319244385,
      "learning_rate": 8.519658119658121e-05,
      "loss": 1.0232,
      "step": 4010
    },
    {
      "epoch": 0.6184377523941387,
      "grad_norm": 0.49737316370010376,
      "learning_rate": 8.485470085470086e-05,
      "loss": 1.0287,
      "step": 4020
    },
    {
      "epoch": 0.6199761547632784,
      "grad_norm": 0.4774438142776489,
      "learning_rate": 8.451282051282051e-05,
      "loss": 1.0546,
      "step": 4030
    },
    {
      "epoch": 0.621514557132418,
      "grad_norm": 0.46045878529548645,
      "learning_rate": 8.417094017094017e-05,
      "loss": 0.9766,
      "step": 4040
    },
    {
      "epoch": 0.6230529595015576,
      "grad_norm": 0.5054082870483398,
      "learning_rate": 8.382905982905982e-05,
      "loss": 0.951,
      "step": 4050
    },
    {
      "epoch": 0.6245913618706973,
      "grad_norm": 0.4494772255420685,
      "learning_rate": 8.34871794871795e-05,
      "loss": 0.9472,
      "step": 4060
    },
    {
      "epoch": 0.6261297642398369,
      "grad_norm": 0.46364492177963257,
      "learning_rate": 8.314529914529915e-05,
      "loss": 0.9799,
      "step": 4070
    },
    {
      "epoch": 0.6276681666089766,
      "grad_norm": 0.45751115679740906,
      "learning_rate": 8.280341880341881e-05,
      "loss": 1.0296,
      "step": 4080
    },
    {
      "epoch": 0.6292065689781162,
      "grad_norm": 0.49335426092147827,
      "learning_rate": 8.246153846153847e-05,
      "loss": 1.0391,
      "step": 4090
    },
    {
      "epoch": 0.6307449713472558,
      "grad_norm": 0.44529882073402405,
      "learning_rate": 8.211965811965813e-05,
      "loss": 1.0302,
      "step": 4100
    },
    {
      "epoch": 0.6322833737163955,
      "grad_norm": 0.5169335007667542,
      "learning_rate": 8.177777777777778e-05,
      "loss": 1.0235,
      "step": 4110
    },
    {
      "epoch": 0.6338217760855351,
      "grad_norm": 0.45856523513793945,
      "learning_rate": 8.143589743589743e-05,
      "loss": 0.9441,
      "step": 4120
    },
    {
      "epoch": 0.6353601784546749,
      "grad_norm": 0.5385552644729614,
      "learning_rate": 8.10940170940171e-05,
      "loss": 1.0214,
      "step": 4130
    },
    {
      "epoch": 0.6368985808238145,
      "grad_norm": 0.49311205744743347,
      "learning_rate": 8.075213675213676e-05,
      "loss": 0.9841,
      "step": 4140
    },
    {
      "epoch": 0.6384369831929542,
      "grad_norm": 0.5630927085876465,
      "learning_rate": 8.041025641025641e-05,
      "loss": 1.0627,
      "step": 4150
    },
    {
      "epoch": 0.6399753855620938,
      "grad_norm": 0.43924352526664734,
      "learning_rate": 8.006837606837607e-05,
      "loss": 0.9903,
      "step": 4160
    },
    {
      "epoch": 0.6415137879312334,
      "grad_norm": 0.4483819305896759,
      "learning_rate": 7.972649572649573e-05,
      "loss": 1.0277,
      "step": 4170
    },
    {
      "epoch": 0.6430521903003731,
      "grad_norm": 0.4714765250682831,
      "learning_rate": 7.938461538461539e-05,
      "loss": 1.041,
      "step": 4180
    },
    {
      "epoch": 0.6445905926695127,
      "grad_norm": 0.4829670488834381,
      "learning_rate": 7.904273504273505e-05,
      "loss": 0.9611,
      "step": 4190
    },
    {
      "epoch": 0.6461289950386524,
      "grad_norm": 0.4727685749530792,
      "learning_rate": 7.87008547008547e-05,
      "loss": 0.9711,
      "step": 4200
    },
    {
      "epoch": 0.647667397407792,
      "grad_norm": 0.45242372155189514,
      "learning_rate": 7.835897435897436e-05,
      "loss": 0.999,
      "step": 4210
    },
    {
      "epoch": 0.6492057997769316,
      "grad_norm": 0.4948423206806183,
      "learning_rate": 7.801709401709402e-05,
      "loss": 1.0317,
      "step": 4220
    },
    {
      "epoch": 0.6507442021460713,
      "grad_norm": 0.49889814853668213,
      "learning_rate": 7.767521367521368e-05,
      "loss": 0.9801,
      "step": 4230
    },
    {
      "epoch": 0.6522826045152109,
      "grad_norm": 0.518099844455719,
      "learning_rate": 7.733333333333333e-05,
      "loss": 1.0094,
      "step": 4240
    },
    {
      "epoch": 0.6538210068843506,
      "grad_norm": 0.49984046816825867,
      "learning_rate": 7.699145299145299e-05,
      "loss": 1.0631,
      "step": 4250
    },
    {
      "epoch": 0.6553594092534902,
      "grad_norm": 0.5042322874069214,
      "learning_rate": 7.664957264957265e-05,
      "loss": 0.9769,
      "step": 4260
    },
    {
      "epoch": 0.65689781162263,
      "grad_norm": 0.43659842014312744,
      "learning_rate": 7.630769230769231e-05,
      "loss": 0.9597,
      "step": 4270
    },
    {
      "epoch": 0.6584362139917695,
      "grad_norm": 0.5034261345863342,
      "learning_rate": 7.596581196581198e-05,
      "loss": 1.0501,
      "step": 4280
    },
    {
      "epoch": 0.6599746163609091,
      "grad_norm": 0.5292542576789856,
      "learning_rate": 7.562393162393164e-05,
      "loss": 0.978,
      "step": 4290
    },
    {
      "epoch": 0.6615130187300489,
      "grad_norm": 0.5163887739181519,
      "learning_rate": 7.528205128205128e-05,
      "loss": 1.0168,
      "step": 4300
    },
    {
      "epoch": 0.6630514210991885,
      "grad_norm": 0.43550893664360046,
      "learning_rate": 7.494017094017094e-05,
      "loss": 0.9807,
      "step": 4310
    },
    {
      "epoch": 0.6645898234683282,
      "grad_norm": 0.5076808333396912,
      "learning_rate": 7.45982905982906e-05,
      "loss": 0.9897,
      "step": 4320
    },
    {
      "epoch": 0.6661282258374678,
      "grad_norm": 0.49274855852127075,
      "learning_rate": 7.425641025641025e-05,
      "loss": 1.0322,
      "step": 4330
    },
    {
      "epoch": 0.6676666282066075,
      "grad_norm": 0.4688161313533783,
      "learning_rate": 7.391452991452992e-05,
      "loss": 0.9099,
      "step": 4340
    },
    {
      "epoch": 0.6692050305757471,
      "grad_norm": 0.5523702502250671,
      "learning_rate": 7.357264957264958e-05,
      "loss": 1.0192,
      "step": 4350
    },
    {
      "epoch": 0.6707434329448867,
      "grad_norm": 0.46958303451538086,
      "learning_rate": 7.323076923076924e-05,
      "loss": 1.0505,
      "step": 4360
    },
    {
      "epoch": 0.6722818353140264,
      "grad_norm": 0.4783111810684204,
      "learning_rate": 7.28888888888889e-05,
      "loss": 0.9685,
      "step": 4370
    },
    {
      "epoch": 0.673820237683166,
      "grad_norm": 0.5339074730873108,
      "learning_rate": 7.254700854700855e-05,
      "loss": 1.0064,
      "step": 4380
    },
    {
      "epoch": 0.6753586400523057,
      "grad_norm": 0.4265858829021454,
      "learning_rate": 7.220512820512821e-05,
      "loss": 1.0077,
      "step": 4390
    },
    {
      "epoch": 0.6768970424214453,
      "grad_norm": 0.5210985541343689,
      "learning_rate": 7.186324786324786e-05,
      "loss": 1.0384,
      "step": 4400
    },
    {
      "epoch": 0.6784354447905849,
      "grad_norm": 0.5227885842323303,
      "learning_rate": 7.152136752136753e-05,
      "loss": 1.0088,
      "step": 4410
    },
    {
      "epoch": 0.6799738471597246,
      "grad_norm": 0.500190019607544,
      "learning_rate": 7.117948717948719e-05,
      "loss": 1.0867,
      "step": 4420
    },
    {
      "epoch": 0.6815122495288642,
      "grad_norm": 0.5075272917747498,
      "learning_rate": 7.083760683760684e-05,
      "loss": 1.0057,
      "step": 4430
    },
    {
      "epoch": 0.683050651898004,
      "grad_norm": 0.4295794367790222,
      "learning_rate": 7.04957264957265e-05,
      "loss": 0.9242,
      "step": 4440
    },
    {
      "epoch": 0.6845890542671436,
      "grad_norm": 0.4278978407382965,
      "learning_rate": 7.015384615384616e-05,
      "loss": 0.9768,
      "step": 4450
    },
    {
      "epoch": 0.6861274566362833,
      "grad_norm": 0.4457150995731354,
      "learning_rate": 6.981196581196582e-05,
      "loss": 0.9245,
      "step": 4460
    },
    {
      "epoch": 0.6876658590054229,
      "grad_norm": 0.5517175793647766,
      "learning_rate": 6.947008547008547e-05,
      "loss": 1.0103,
      "step": 4470
    },
    {
      "epoch": 0.6892042613745625,
      "grad_norm": 0.5487529039382935,
      "learning_rate": 6.912820512820513e-05,
      "loss": 0.9893,
      "step": 4480
    },
    {
      "epoch": 0.6907426637437022,
      "grad_norm": 0.4264967143535614,
      "learning_rate": 6.878632478632479e-05,
      "loss": 1.0154,
      "step": 4490
    },
    {
      "epoch": 0.6922810661128418,
      "grad_norm": 0.4651111662387848,
      "learning_rate": 6.844444444444445e-05,
      "loss": 1.0129,
      "step": 4500
    }
  ],
  "logging_steps": 10,
  "max_steps": 6501,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.75097296461824e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
