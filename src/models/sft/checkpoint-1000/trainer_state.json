{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.15384023691396484,
  "eval_steps": 500,
  "global_step": 1000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0015384023691396484,
      "grad_norm": 2.584967613220215,
      "learning_rate": 2.7649769585253458e-06,
      "loss": 3.3921,
      "step": 10
    },
    {
      "epoch": 0.003076804738279297,
      "grad_norm": 2.1197686195373535,
      "learning_rate": 5.837173579109064e-06,
      "loss": 3.4355,
      "step": 20
    },
    {
      "epoch": 0.004615207107418946,
      "grad_norm": 1.4546103477478027,
      "learning_rate": 8.90937019969278e-06,
      "loss": 3.2477,
      "step": 30
    },
    {
      "epoch": 0.006153609476558594,
      "grad_norm": 1.344617486000061,
      "learning_rate": 1.1981566820276497e-05,
      "loss": 3.1501,
      "step": 40
    },
    {
      "epoch": 0.007692011845698243,
      "grad_norm": 2.066725730895996,
      "learning_rate": 1.5053763440860215e-05,
      "loss": 3.1164,
      "step": 50
    },
    {
      "epoch": 0.009230414214837892,
      "grad_norm": 1.6152411699295044,
      "learning_rate": 1.8125960061443932e-05,
      "loss": 2.7623,
      "step": 60
    },
    {
      "epoch": 0.010768816583977539,
      "grad_norm": 1.8350930213928223,
      "learning_rate": 2.1198156682027652e-05,
      "loss": 2.5937,
      "step": 70
    },
    {
      "epoch": 0.012307218953117188,
      "grad_norm": 2.344829797744751,
      "learning_rate": 2.427035330261137e-05,
      "loss": 2.4205,
      "step": 80
    },
    {
      "epoch": 0.013845621322256836,
      "grad_norm": 4.050056457519531,
      "learning_rate": 2.734254992319509e-05,
      "loss": 2.2338,
      "step": 90
    },
    {
      "epoch": 0.015384023691396485,
      "grad_norm": 2.1490421295166016,
      "learning_rate": 3.0414746543778806e-05,
      "loss": 2.0444,
      "step": 100
    },
    {
      "epoch": 0.016922426060536132,
      "grad_norm": 1.358504056930542,
      "learning_rate": 3.348694316436252e-05,
      "loss": 1.8553,
      "step": 110
    },
    {
      "epoch": 0.018460828429675783,
      "grad_norm": 1.3946998119354248,
      "learning_rate": 3.655913978494624e-05,
      "loss": 1.8513,
      "step": 120
    },
    {
      "epoch": 0.01999923079881543,
      "grad_norm": 2.3798844814300537,
      "learning_rate": 3.963133640552996e-05,
      "loss": 1.7238,
      "step": 130
    },
    {
      "epoch": 0.021537633167955077,
      "grad_norm": 1.6720370054244995,
      "learning_rate": 4.270353302611367e-05,
      "loss": 1.6326,
      "step": 140
    },
    {
      "epoch": 0.023076035537094728,
      "grad_norm": 1.9096741676330566,
      "learning_rate": 4.577572964669739e-05,
      "loss": 1.499,
      "step": 150
    },
    {
      "epoch": 0.024614437906234375,
      "grad_norm": 1.5747382640838623,
      "learning_rate": 4.8847926267281106e-05,
      "loss": 1.3412,
      "step": 160
    },
    {
      "epoch": 0.026152840275374026,
      "grad_norm": 1.592766523361206,
      "learning_rate": 5.192012288786483e-05,
      "loss": 1.3982,
      "step": 170
    },
    {
      "epoch": 0.027691242644513673,
      "grad_norm": 1.6842422485351562,
      "learning_rate": 5.499231950844854e-05,
      "loss": 1.3376,
      "step": 180
    },
    {
      "epoch": 0.02922964501365332,
      "grad_norm": 1.5417200326919556,
      "learning_rate": 5.8064516129032266e-05,
      "loss": 1.2252,
      "step": 190
    },
    {
      "epoch": 0.03076804738279297,
      "grad_norm": 3.287029504776001,
      "learning_rate": 6.113671274961598e-05,
      "loss": 1.315,
      "step": 200
    },
    {
      "epoch": 0.03230644975193262,
      "grad_norm": 1.9193211793899536,
      "learning_rate": 6.42089093701997e-05,
      "loss": 1.2919,
      "step": 210
    },
    {
      "epoch": 0.033844852121072265,
      "grad_norm": 1.1237131357192993,
      "learning_rate": 6.72811059907834e-05,
      "loss": 1.2607,
      "step": 220
    },
    {
      "epoch": 0.035383254490211916,
      "grad_norm": 5.890222072601318,
      "learning_rate": 7.035330261136714e-05,
      "loss": 1.1936,
      "step": 230
    },
    {
      "epoch": 0.036921656859351566,
      "grad_norm": 1.1118741035461426,
      "learning_rate": 7.342549923195085e-05,
      "loss": 1.3291,
      "step": 240
    },
    {
      "epoch": 0.03846005922849121,
      "grad_norm": 1.3041982650756836,
      "learning_rate": 7.649769585253457e-05,
      "loss": 1.1517,
      "step": 250
    },
    {
      "epoch": 0.03999846159763086,
      "grad_norm": 0.8721215128898621,
      "learning_rate": 7.956989247311829e-05,
      "loss": 1.1674,
      "step": 260
    },
    {
      "epoch": 0.04153686396677051,
      "grad_norm": 1.0562869310379028,
      "learning_rate": 8.2642089093702e-05,
      "loss": 1.2735,
      "step": 270
    },
    {
      "epoch": 0.043075266335910155,
      "grad_norm": 1.0866798162460327,
      "learning_rate": 8.571428571428571e-05,
      "loss": 1.2501,
      "step": 280
    },
    {
      "epoch": 0.044613668705049805,
      "grad_norm": 1.4433777332305908,
      "learning_rate": 8.878648233486943e-05,
      "loss": 1.1162,
      "step": 290
    },
    {
      "epoch": 0.046152071074189456,
      "grad_norm": 1.0068490505218506,
      "learning_rate": 9.185867895545315e-05,
      "loss": 1.1211,
      "step": 300
    },
    {
      "epoch": 0.0476904734433291,
      "grad_norm": 1.2410961389541626,
      "learning_rate": 9.493087557603687e-05,
      "loss": 1.1446,
      "step": 310
    },
    {
      "epoch": 0.04922887581246875,
      "grad_norm": 0.9184087514877319,
      "learning_rate": 9.800307219662059e-05,
      "loss": 1.1429,
      "step": 320
    },
    {
      "epoch": 0.0507672781816084,
      "grad_norm": 0.8276410698890686,
      "learning_rate": 0.0001010752688172043,
      "loss": 1.2176,
      "step": 330
    },
    {
      "epoch": 0.05230568055074805,
      "grad_norm": 0.9394607543945312,
      "learning_rate": 0.00010414746543778802,
      "loss": 1.1135,
      "step": 340
    },
    {
      "epoch": 0.053844082919887695,
      "grad_norm": 0.7618582844734192,
      "learning_rate": 0.00010721966205837175,
      "loss": 1.1033,
      "step": 350
    },
    {
      "epoch": 0.055382485289027346,
      "grad_norm": 0.8039534091949463,
      "learning_rate": 0.00011029185867895546,
      "loss": 1.1847,
      "step": 360
    },
    {
      "epoch": 0.056920887658166996,
      "grad_norm": 1.115044355392456,
      "learning_rate": 0.00011336405529953918,
      "loss": 1.1742,
      "step": 370
    },
    {
      "epoch": 0.05845929002730664,
      "grad_norm": 0.9048172235488892,
      "learning_rate": 0.00011643625192012289,
      "loss": 1.214,
      "step": 380
    },
    {
      "epoch": 0.05999769239644629,
      "grad_norm": 0.8437467217445374,
      "learning_rate": 0.00011950844854070662,
      "loss": 1.1377,
      "step": 390
    },
    {
      "epoch": 0.06153609476558594,
      "grad_norm": 0.7477501630783081,
      "learning_rate": 0.00012258064516129034,
      "loss": 1.1254,
      "step": 400
    },
    {
      "epoch": 0.06307449713472559,
      "grad_norm": 0.7767701148986816,
      "learning_rate": 0.00012565284178187405,
      "loss": 1.107,
      "step": 410
    },
    {
      "epoch": 0.06461289950386524,
      "grad_norm": 1.1635019779205322,
      "learning_rate": 0.00012872503840245775,
      "loss": 1.1033,
      "step": 420
    },
    {
      "epoch": 0.06615130187300489,
      "grad_norm": 0.8846574425697327,
      "learning_rate": 0.0001317972350230415,
      "loss": 1.1624,
      "step": 430
    },
    {
      "epoch": 0.06768970424214453,
      "grad_norm": 0.9813984036445618,
      "learning_rate": 0.0001348694316436252,
      "loss": 1.2409,
      "step": 440
    },
    {
      "epoch": 0.06922810661128419,
      "grad_norm": 0.9507609009742737,
      "learning_rate": 0.0001379416282642089,
      "loss": 1.2503,
      "step": 450
    },
    {
      "epoch": 0.07076650898042383,
      "grad_norm": 0.7813093066215515,
      "learning_rate": 0.00014101382488479263,
      "loss": 1.0684,
      "step": 460
    },
    {
      "epoch": 0.07230491134956347,
      "grad_norm": 0.7689453363418579,
      "learning_rate": 0.00014408602150537637,
      "loss": 1.1374,
      "step": 470
    },
    {
      "epoch": 0.07384331371870313,
      "grad_norm": 0.7696683406829834,
      "learning_rate": 0.00014715821812596007,
      "loss": 1.0953,
      "step": 480
    },
    {
      "epoch": 0.07538171608784278,
      "grad_norm": 0.6287794709205627,
      "learning_rate": 0.00015023041474654378,
      "loss": 1.107,
      "step": 490
    },
    {
      "epoch": 0.07692011845698242,
      "grad_norm": 0.8339664340019226,
      "learning_rate": 0.0001533026113671275,
      "loss": 1.1726,
      "step": 500
    },
    {
      "epoch": 0.07845852082612208,
      "grad_norm": 0.7385021448135376,
      "learning_rate": 0.00015637480798771122,
      "loss": 1.1105,
      "step": 510
    },
    {
      "epoch": 0.07999692319526172,
      "grad_norm": 1.2106229066848755,
      "learning_rate": 0.00015944700460829493,
      "loss": 1.0924,
      "step": 520
    },
    {
      "epoch": 0.08153532556440136,
      "grad_norm": 0.7910128235816956,
      "learning_rate": 0.00016251920122887866,
      "loss": 1.0595,
      "step": 530
    },
    {
      "epoch": 0.08307372793354102,
      "grad_norm": 0.6995137333869934,
      "learning_rate": 0.0001655913978494624,
      "loss": 1.1278,
      "step": 540
    },
    {
      "epoch": 0.08461213030268067,
      "grad_norm": 0.6679721474647522,
      "learning_rate": 0.0001686635944700461,
      "loss": 1.1902,
      "step": 550
    },
    {
      "epoch": 0.08615053267182031,
      "grad_norm": 0.7743341326713562,
      "learning_rate": 0.0001717357910906298,
      "loss": 1.1896,
      "step": 560
    },
    {
      "epoch": 0.08768893504095997,
      "grad_norm": 0.537539005279541,
      "learning_rate": 0.0001748079877112135,
      "loss": 1.1159,
      "step": 570
    },
    {
      "epoch": 0.08922733741009961,
      "grad_norm": 0.7204731702804565,
      "learning_rate": 0.00017788018433179725,
      "loss": 1.0316,
      "step": 580
    },
    {
      "epoch": 0.09076573977923925,
      "grad_norm": 0.7807141542434692,
      "learning_rate": 0.00018095238095238095,
      "loss": 1.1666,
      "step": 590
    },
    {
      "epoch": 0.09230414214837891,
      "grad_norm": 0.6946710348129272,
      "learning_rate": 0.0001840245775729647,
      "loss": 1.0602,
      "step": 600
    },
    {
      "epoch": 0.09384254451751856,
      "grad_norm": 0.6253068447113037,
      "learning_rate": 0.0001870967741935484,
      "loss": 1.1165,
      "step": 610
    },
    {
      "epoch": 0.0953809468866582,
      "grad_norm": 0.6554427146911621,
      "learning_rate": 0.00019016897081413213,
      "loss": 1.1488,
      "step": 620
    },
    {
      "epoch": 0.09691934925579786,
      "grad_norm": 0.7545556426048279,
      "learning_rate": 0.00019324116743471583,
      "loss": 1.0227,
      "step": 630
    },
    {
      "epoch": 0.0984577516249375,
      "grad_norm": 0.654740035533905,
      "learning_rate": 0.00019631336405529954,
      "loss": 1.0965,
      "step": 640
    },
    {
      "epoch": 0.09999615399407714,
      "grad_norm": 0.5748230218887329,
      "learning_rate": 0.00019938556067588327,
      "loss": 1.1018,
      "step": 650
    },
    {
      "epoch": 0.1015345563632168,
      "grad_norm": 0.49472448229789734,
      "learning_rate": 0.00019972649572649572,
      "loss": 1.1408,
      "step": 660
    },
    {
      "epoch": 0.10307295873235645,
      "grad_norm": 0.5874394178390503,
      "learning_rate": 0.0001993846153846154,
      "loss": 1.0593,
      "step": 670
    },
    {
      "epoch": 0.1046113611014961,
      "grad_norm": 0.7135041952133179,
      "learning_rate": 0.00019904273504273506,
      "loss": 1.177,
      "step": 680
    },
    {
      "epoch": 0.10614976347063575,
      "grad_norm": 0.659172773361206,
      "learning_rate": 0.00019870085470085472,
      "loss": 1.0972,
      "step": 690
    },
    {
      "epoch": 0.10768816583977539,
      "grad_norm": 0.5658200979232788,
      "learning_rate": 0.00019835897435897438,
      "loss": 0.998,
      "step": 700
    },
    {
      "epoch": 0.10922656820891505,
      "grad_norm": 0.6410398483276367,
      "learning_rate": 0.00019801709401709404,
      "loss": 1.0226,
      "step": 710
    },
    {
      "epoch": 0.11076497057805469,
      "grad_norm": 0.6680729389190674,
      "learning_rate": 0.0001976752136752137,
      "loss": 1.0757,
      "step": 720
    },
    {
      "epoch": 0.11230337294719434,
      "grad_norm": 0.9527504444122314,
      "learning_rate": 0.00019733333333333335,
      "loss": 1.0509,
      "step": 730
    },
    {
      "epoch": 0.11384177531633399,
      "grad_norm": 0.5344208478927612,
      "learning_rate": 0.000196991452991453,
      "loss": 1.137,
      "step": 740
    },
    {
      "epoch": 0.11538017768547364,
      "grad_norm": 0.5921201705932617,
      "learning_rate": 0.00019664957264957267,
      "loss": 1.0559,
      "step": 750
    },
    {
      "epoch": 0.11691858005461328,
      "grad_norm": 0.711140513420105,
      "learning_rate": 0.00019630769230769232,
      "loss": 1.0359,
      "step": 760
    },
    {
      "epoch": 0.11845698242375294,
      "grad_norm": 0.4643453061580658,
      "learning_rate": 0.00019596581196581198,
      "loss": 1.051,
      "step": 770
    },
    {
      "epoch": 0.11999538479289258,
      "grad_norm": 0.6035484671592712,
      "learning_rate": 0.0001956239316239316,
      "loss": 1.088,
      "step": 780
    },
    {
      "epoch": 0.12153378716203223,
      "grad_norm": 0.7892462015151978,
      "learning_rate": 0.00019528205128205127,
      "loss": 1.0369,
      "step": 790
    },
    {
      "epoch": 0.12307218953117188,
      "grad_norm": 0.4591948986053467,
      "learning_rate": 0.00019494017094017095,
      "loss": 1.0998,
      "step": 800
    },
    {
      "epoch": 0.12461059190031153,
      "grad_norm": 0.5988150835037231,
      "learning_rate": 0.0001945982905982906,
      "loss": 1.0529,
      "step": 810
    },
    {
      "epoch": 0.12614899426945117,
      "grad_norm": 0.5166341662406921,
      "learning_rate": 0.00019425641025641027,
      "loss": 1.0788,
      "step": 820
    },
    {
      "epoch": 0.12768739663859083,
      "grad_norm": 0.5803174376487732,
      "learning_rate": 0.00019391452991452993,
      "loss": 0.9944,
      "step": 830
    },
    {
      "epoch": 0.12922579900773049,
      "grad_norm": 0.617644727230072,
      "learning_rate": 0.00019357264957264958,
      "loss": 1.0888,
      "step": 840
    },
    {
      "epoch": 0.13076420137687012,
      "grad_norm": 0.61956787109375,
      "learning_rate": 0.00019323076923076924,
      "loss": 1.083,
      "step": 850
    },
    {
      "epoch": 0.13230260374600977,
      "grad_norm": 0.6188405156135559,
      "learning_rate": 0.0001928888888888889,
      "loss": 1.1391,
      "step": 860
    },
    {
      "epoch": 0.13384100611514943,
      "grad_norm": 0.6159250736236572,
      "learning_rate": 0.00019254700854700856,
      "loss": 1.0782,
      "step": 870
    },
    {
      "epoch": 0.13537940848428906,
      "grad_norm": 0.4905155301094055,
      "learning_rate": 0.00019220512820512822,
      "loss": 1.0785,
      "step": 880
    },
    {
      "epoch": 0.13691781085342872,
      "grad_norm": 0.476400226354599,
      "learning_rate": 0.00019186324786324787,
      "loss": 1.0371,
      "step": 890
    },
    {
      "epoch": 0.13845621322256838,
      "grad_norm": 0.5361171960830688,
      "learning_rate": 0.00019152136752136753,
      "loss": 1.1167,
      "step": 900
    },
    {
      "epoch": 0.139994615591708,
      "grad_norm": 0.45032036304473877,
      "learning_rate": 0.0001911794871794872,
      "loss": 1.0379,
      "step": 910
    },
    {
      "epoch": 0.14153301796084766,
      "grad_norm": 0.5094605684280396,
      "learning_rate": 0.00019083760683760685,
      "loss": 1.0336,
      "step": 920
    },
    {
      "epoch": 0.14307142032998732,
      "grad_norm": 0.6864129900932312,
      "learning_rate": 0.0001904957264957265,
      "loss": 1.0494,
      "step": 930
    },
    {
      "epoch": 0.14460982269912695,
      "grad_norm": 0.5435264706611633,
      "learning_rate": 0.00019015384615384616,
      "loss": 1.0812,
      "step": 940
    },
    {
      "epoch": 0.1461482250682666,
      "grad_norm": 0.5616250038146973,
      "learning_rate": 0.00018981196581196582,
      "loss": 1.0181,
      "step": 950
    },
    {
      "epoch": 0.14768662743740626,
      "grad_norm": 0.5459400415420532,
      "learning_rate": 0.00018947008547008548,
      "loss": 1.0613,
      "step": 960
    },
    {
      "epoch": 0.1492250298065459,
      "grad_norm": 0.5072353482246399,
      "learning_rate": 0.00018912820512820513,
      "loss": 1.0952,
      "step": 970
    },
    {
      "epoch": 0.15076343217568555,
      "grad_norm": 0.5611528158187866,
      "learning_rate": 0.0001887863247863248,
      "loss": 1.034,
      "step": 980
    },
    {
      "epoch": 0.1523018345448252,
      "grad_norm": 0.5034070014953613,
      "learning_rate": 0.00018844444444444445,
      "loss": 1.1059,
      "step": 990
    },
    {
      "epoch": 0.15384023691396484,
      "grad_norm": 0.4489002227783203,
      "learning_rate": 0.0001881025641025641,
      "loss": 1.1294,
      "step": 1000
    }
  ],
  "logging_steps": 10,
  "max_steps": 6501,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 6131529583460352.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
